{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####intro au texte mining - Les expressions régulières ----------------------------------------------------------------\n",
    "\n",
    "#definition de REGEX : Une expression régulière,  un ensemble de chaines de caractères possibles\n",
    "\n",
    "import re     #librairie re. regex\n",
    "\n",
    "r = re.compile(r\"nous\")    #chercher les regex \"nous\" dans le texte   re[] . on appele ca le pattern\n",
    "txt = 'A la maison, nous avons une télévision. Nous sommes donc heureux'\n",
    "pointeur = r.findall(txt)   #Chercher à l'aide d'une des trois méthodes : findall() le regex \"nous \"  (r)\n",
    "print(pointeur)\n",
    "\n",
    "#la fonction findall ne detecte pas les majuscules\n",
    "\n",
    "r = re.compile(r\"[A-Z]\")    #détecte toutes les majuscules\n",
    "pointeur = r.findall(txt)   #cherche dans le txt\n",
    "print(pointeur)\n",
    "\n",
    "re.findall(r\"[a-zA-Z0-9]\",txt) #cette expression fonctionne (sans le compiler)\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "#exemple \n",
    "\n",
    "print(\"Pour txt, [a-zA-Z0-9] détecte \", re.findall(r\"[a-zA-Z0-9]\",txt))  #lettre maj + min + chiffre\n",
    "\n",
    "print(\"Pour txt, [abc145] détecte \", re.findall(r\"[abc145]\",txt))  #abc145\n",
    "\n",
    "print(\"Pour txt, [a-zA-Z0-9\\.\\?]  détecte \", re.findall(r\"[a-zA-Z0-9.-]\",txt))    #lettre maj + min + chiffre + ./\n",
    "\n",
    "print(\"Pour txt, [^A-Z] détecte \", re.findall(r\"[^A-Z]\",txt)) #tout sauf les maj\n",
    "\n",
    "print(\"Pour txt, tion$ détecte \", re.findall(r\"tion$\",txt))  #tion\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "#numerique\n",
    "\n",
    "r = re.compile(r\"[0-9]\") # out = ['1', '3', '7', '1', '2']\n",
    "r = re.compile(r\"[0-9]+\")   #out = ['137', '12']\n",
    "r = re.compile(\"[0-9]*\") #out = ['', '', '', '', '', '', '', '', '', '', '', '', '137', '', '', '', '', '', '', '',\n",
    "#'', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '12', '', '', '', '', '', '', '', '', '', '']\n",
    "r = re.compile(\"[0-9]{3,}\")  #nombres d'au moins 3 chiffres détectés : ['137']\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "r = re.compile(r\"\\w+ment\")  #out = ['Apparemment', 'étonnement']  #mots avec ment\n",
    "r = re.compile(r\"[A-Za-z0-9\\.\\?éà]+@[a-zA-Zéà]+\\.[a-z]{2,4}\") #les [] separent le terme en 3 pour les 3 parties d un mail \n",
    "#out =['Georges98@yahoo.com', 'grégoire.richon@apple.com', 'sarkozy@élysée.fr']\n",
    "r = re.compile(r\"https?://[A-Za-z0-9\\.\\-/]+\")  \n",
    "#out = ['https://www.google.com/', 'http://www.safari.fr/data-science']\n",
    "\n",
    "#kles fonctions utilisées\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "#match() détermine si la RE correspond dès le début de la chaîne.\n",
    "#search() analyse la chaîne à la recherche d'une position où la RE correspond.\n",
    "#finditer() trouve toutes les sous-chaînes qui correspondent à la RE et les renvoie sous la forme d'un itérateur.\n",
    "#findall() trouve toutes les sous-chaînes qui correspondent à la RE et les renvoie sous la forme d'une liste.\n",
    "\n",
    "#fonction match\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "r = re.compile('(a(b)c)d')\n",
    "g = r.match('abcd')\n",
    "print(g.group(0), g.groups())\n",
    "#out = abcd ('abc', 'b')\n",
    "\n",
    "#fonction search\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "txt = 'https://www.google.com/ \\n http://www.safari.fr/data-science'\n",
    "r = re.compile(r\"http(s)?://[a-zA-z0-9\\.\\-/]+\")\n",
    "liens = r.search(txt)\n",
    "print(liens.group(), liens.groups())\n",
    "\n",
    "    \n",
    "    \n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#recuperer les mails sur un sites\n",
    "\n",
    "import requests    #librairie nécessaire\n",
    "data = requests.get(\"http://www.mg-cc.org/club-information/club-contacts\").text   #importer les données\n",
    "print(data)\n",
    "\n",
    "#trouver les emails\n",
    "r = re.compile(r\">([a-zA-Z0-9.-]+@[a-zA-Z.-]+)<\") \n",
    "email = r.findall(data)\n",
    "print(email) \n",
    "#out = ['morme@mg-cc.org', 'bfritz@mg-cc.org', 'alazovitz@mg-cc.org', 'glenhart@mg-cc.org', 'atate@mg-cc.org', '\n",
    "# kbeck@mg-cc.org', # 'bdoheny@mg-cc.org', 'DiningManager@mg-cc.org',  'cgiampa@mg-cc.org', 'sfalatek@mg-cc.org', \n",
    "#'selmairobinson@gmail.com', 'freycc@npenn.org', 'kmurphy@mg-cc.org']\n",
    "\n",
    "*#Trouver les numéros\n",
    "r = re.compile(r\"\\([0-9]{3}\\)\\s[0-9]{3}-[0-9]{4}\")\n",
    "phones = r.findall(data)\n",
    "print(phones) \n",
    "#kout : ['(215) 886-3033', '(267) 241-3239', '(215) 361-2926', '(215) 886-3200']\n",
    "\n",
    "\n",
    "#SPLIT   separe le texte \n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "txt = \"L'exemple... parfait pour comprendre\"\n",
    "\n",
    "#Compilateur\n",
    "r = re.compile(r\"\\W+\")              #out :  ['L', 'exemple', 'parfait', 'pour', 'comprendre']\n",
    "r = re.compile(r\"[^A-Za-z0-9_']+\")   #out : [\"L'exemple\", 'parfait', 'pour', 'comprendre']\n",
    "#Exemple\n",
    "\n",
    "#Split\n",
    "print(r.split(txt))\n",
    "\n",
    "\n",
    "# SUB   remplace le texte\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "txt = \"c'est super cool comme superstition\"\n",
    "#compilateur\n",
    "r = re.compile(r\"super\")\n",
    "\n",
    "print(r.sub('cool', txt))   # out : c'est cool cool comme coolstition\n",
    "\n",
    "\n",
    "r = re.compile(r\"\\bsuper\\b\")\n",
    "print(r.sub('cool', txt))\n",
    "# out : c'est cool cool comme superstition\n",
    "\n",
    "\n",
    "# \n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# lire le fichier\n",
    "df = pd.read_csv(\"tweets_macron.txt\", sep = '\\t')\n",
    "\n",
    "#afficher les 3 premieres lignes\n",
    "df.head(3)\n",
    "\n",
    "textes = df[\"text\"]\n",
    "\n",
    "#compilateur\n",
    "r = re.compile(r\" #\\w+ \")\n",
    "textes = r.sub(\"<hashtag>\", str(textes))\n",
    "\n",
    "#compilateur liens internet\n",
    "r = re.compile(r\"https?://[A-Za-z0-9./]+\")\n",
    "textes = r.sub(\"<lien>\", textes)\n",
    "\n",
    "\n",
    "# Greedy vs Lazy\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "#Les greedy vont essayer de capter le plus grand paterne possible \n",
    "#tandis que les lazy vont essayer de capturer le plus petit.\n",
    "\n",
    "    #      Greedy\tLazy\tDescription\n",
    "    #      *\t*?\t0, 1 ou plus\n",
    "    #      +\t+?\tau moins 1\n",
    "    #      ?\t??\t0, 1\n",
    "    #      {n}\t{n}?\tn fois exactement\n",
    "    #      {n,}\t{n,}?\tn fois au moins\n",
    "    #      {,m}\t{,m}?\tm fois au plus\n",
    "    #      {n,m}\t{n,m}?\tentre n et m fois\n",
    "\n",
    "\n",
    "r1 = re.findall(r\"s.*o\", 'stackoverflow')     # out : ['stackoverflo']\n",
    "r2 = re.findall(r\"s.*?o\", 'stackoverflow')   # out : ['stacko']\n",
    "    \n",
    "\n",
    "    \n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "#lazy\n",
    "r = re.compile(r\"<.*?>\")    #compilation\n",
    "balise = r.findall(txt)    #balises\n",
    "#out : ['<html>', '<head>', '<title>', '<\\title>']\n",
    "\n",
    "#greedy \n",
    "re.findall(r\"<.*>\", txt))\n",
    "# out = ['<html><head><title>Title<\\title>']\n",
    "\n",
    "\n",
    "\n",
    "#Verbose\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "re.verbose est un outil intéressant qui peut vous aider à commenter les paternes à l'aide de #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Souffrez qu'Amour cette nuit vous réveille. Par mes soupirs laissez-vous enflammer. Vous dormez trop, adorable merveille. Car c'est dormir que de ne point aimer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Souffrez qu'Amour cette nuit vous réveille.\",\n",
       " 'Par mes soupirs laissez-vous enflammer.',\n",
       " 'Vous dormez trop, adorable merveille.',\n",
       " \"Car c'est dormir que de ne point aimer.\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####intro au texte mining - Prétraitement ----------------------------------------------------------------\n",
    "\n",
    "# La tokenisation\n",
    "\n",
    "#Le package Scikit-learn et la boîte-à-outil : Natural Language Toolkit (NLTK) sont deux bibliothèques \n",
    "#qui vont permettre de créer des programmes pour l'analyse de texte.\n",
    "\n",
    "\n",
    "\n",
    "#exemple \n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Initialiser les données textes\n",
    "txt = \"Souffrez qu'Amour cette nuit vous réveille. Par mes soupirs laissez-vous enflammer. \\\n",
    "Vous dormez trop, adorable merveille. Car c'est dormir que de ne point aimer.\"\n",
    "\n",
    "# Appliquer la tokenisation\n",
    "\n",
    "\n",
    "# Découpe les phrases\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "from nltk.tokenize import PunktSentenceTokenizer \n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "tokenizer.tokenize(txt)\n",
    "\n",
    "# Découper la phrase en mots\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "from nltk.tokenize import word_tokenize\n",
    "mots = word_tokenize(txt, language='french')\n",
    "print(mots)\n",
    "\n",
    "#out : #['Souffrez', \"qu'Amour\", 'cette', 'nuit', 'vous', 'réveille', '.', 'Par', 'mes', 'soupirs', 'laissez-vous', \n",
    "#'enflammer', '.', 'Vous', 'dormez', 'trop', ',', 'adorable', 'merveille', '.', #'Car', \"c'est\", 'dormir', 'que', 'de',\n",
    "#'ne', 'point', 'aimer', '.']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Le filtrage Stop Words\n",
    "\n",
    " # Importer stopwords de la classe nltk.corpus\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "from nltk.corpus import stopwords  \n",
    "\n",
    "\n",
    "\n",
    "#afficher les mots blancs   (blanc = inutile)\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "stop_words = set(stopwords.words('french'))    # Initialiser la variable des mots vides\n",
    "print(stop_words)\n",
    "stop_words.update([\",\", \".\"])\n",
    "\n",
    "\n",
    "\n",
    "# définir la fonction stop_words_filtering\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "def stop_words_filetring(mots) : \n",
    "    tokens = []\n",
    "    for mot in mots:\n",
    "        if mot not in stop_words:\n",
    "            tokens.append(mot)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Appliquer la focnction stop_words_filtering à la variable mots\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "print(mots)\n",
    "print(stop_words_filetring(mots))\n",
    "\n",
    "\n",
    "\n",
    "#tokenisation  avec des mots de + de 3 lettres\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "#Importer la pckage nécessaire\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "\n",
    "# Initialiser un tokenisteur \n",
    "tokenizer = RegexpTokenizer(\"[a-zA-Zé]{4,}\")\n",
    "\n",
    "# Calculer les tokens\n",
    "tokens = tokenizer.tokenize(txt.lower())\n",
    "\n",
    "# Afficher les tokens\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Algorithme Bag of Words\n",
    "\n",
    "#L'algorithme Bag of Words consiste à \"vectoriser\" un document. \n",
    "#La représentation en sac de mot consiste à représenter un document par le nombre d'occurrences des mots qu'il contient.\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "#Importer le package nécessaire\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Créer un vectorisateur\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Appliquer Bag of words à la variable tokens\n",
    "vectorizer.fit_transform(tokens)\n",
    "\n",
    "# Récupération des tokens\n",
    "tokenized = vectorizer.vocabulary_\n",
    "print(tokenized)\n",
    "\n",
    "\n",
    "#la représentation vectorielle\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "print(vectorizer.transform([\"laissez-vous enflammer\",\"Dormez vous cette nuit ?\"]).toarray())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#La racinisation\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "#La racinisation ou la désuffixation (stemming en anglais) est une technique de transformation des mots \n",
    "#en leur radical ou racine (stem en anglais)\n",
    "\n",
    "# Importer le package nécessaire\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "# Créer un lemmatiseur\n",
    "porter_stemmer = FrenchStemmer()\n",
    "\n",
    "# Calculer le radical\n",
    "radical = porter_stemmer.stem ('sérieusement')\n",
    "\n",
    "# Afficher le radical\n",
    "radical\n",
    "\n",
    "#example\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "#  Définir la fonction stemming\n",
    "def stemming(mots) :\n",
    "    sortie = []\n",
    "    for string in mots :\n",
    "        radical = porter_stemmer.stem(string)\n",
    "        if (radical not in sortie) : sortie.append( radical )\n",
    "    return sortie\n",
    "\n",
    "# Appliquer la fonction stemming à la variable tokens\n",
    "print(stemming(tokens))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#La lemmatisation\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "#La lemmatisation (lemmatisation en anglais) est une technique similaire à la racinisation.\n",
    "\n",
    "# Importer le package nécessaire\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialiser un lemmatiseur\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Calculer le lemme du mot meeting\n",
    "wordnet_lemmatizer.lemmatize('meeting', pos='v'), wordnet_lemmatizer.lemmatize('meeting', pos='n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####intro au texte mining - Dataviz' : Création d'un nuage de mots -------------------------------------------------------\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Importer les packages nécessaires\n",
    "import pandas as pd\n",
    "\n",
    "# Lire le fichier movies_comments\n",
    "df = pd.read_csv('movies_comments.csv')\n",
    "\n",
    "# Afficher les cinq premières lignes de df\n",
    "print(df.head(5))\n",
    "\n",
    "# Affiche la taille du dataset\n",
    "print(\"Taille du dataset :\", df.shape)\n",
    "\n",
    "\n",
    "\n",
    "#stop word\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "text = \"\"\n",
    "for comment in df.Text :     # Définir la variable text\n",
    "    text += comment\n",
    "\n",
    "from nltk.corpus import stopwords    # Importer stopwords de la classe nltk.corpus\n",
    "\n",
    "stop_words = set(stopwords.words('english'))    # Initialiser la variable des mots vides\n",
    "print(stop_words)\n",
    "\n",
    "\n",
    "\n",
    "#wordcloud + Définition du calque du nuage des mots\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "#Importer les packages nécessaires\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Définir le calque du nuage des mots\n",
    "wc = WordCloud(background_color=\"black\", max_words=100, stopwords=stop_words, max_font_size=50, random_state=42)\n",
    "\n",
    "\n",
    "#Afficher le wordcloud\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Générer et afficher le nuage de mots\n",
    "\n",
    "plt.figure(figsize= (10,6)) # Initialisation d'une figure\n",
    "wc.generate(text)           # \"Calcul\" du wordcloud\n",
    "plt.imshow(wc) # Affichage\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# tracer le nuage de mots en respectant la forme du masque\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "#Importer les packages nécessaires\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def plot_word_cloud(text, masque, background_color = \"black\") :\n",
    "    # Définir un masque\n",
    "    mask_coloring = np.array(Image.open(str(masque)))\n",
    "\n",
    "    # Définir le calque du nuage des mots\n",
    "    wc = WordCloud(background_color=background_color, max_words=200, stopwords=stop_words, mask = mask_coloring, max_font_size=50, random_state=42)\n",
    "\n",
    "    # Générer et afficher le nuage de mots\n",
    "    plt.figure(figsize= (20,10))\n",
    "    wc.generate(text)\n",
    "    plt.imshow(wc)\n",
    "    plt.show()\n",
    "\n",
    "plot_word_cloud(text, \"iron.jpg\")\n",
    "\n",
    "# Code d'affichage du masque\n",
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread(\"iron.jpg\")\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#stop_words.update()       #ajoute des mots (ici titre des films)\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Mettre à jour la valeur de stop_words\n",
    "mots_vides = [\"mission\", \"impossible\", \"harry\", \"potter\", \"Da\", \"Vinci\", \"Mountain\", \"Brockeback\",  \"Brokeback\", \"Code\"]\n",
    "stop_words.update(mots_vides)\n",
    "\n",
    "\n",
    "\n",
    "# Séparer df en données positives et négatives\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Séparer df en données positives et négatives\n",
    "df_pos = df[df.Sentiment == 1]\n",
    "df_neg = df[df.Sentiment == 0]\n",
    "\n",
    "# Afficher les quatre premières lignes de df_pos\n",
    "df_pos.head(4)\n",
    "\n",
    "\n",
    "\n",
    "#tracer 2 images avec les filtres\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Définir les données positives et négatives de types string\n",
    "\n",
    "pos_text = \"\"\n",
    "for e in df_pos.Text : pos_text += e\n",
    "neg_text = \"\"\n",
    "for e in df_neg.Text :neg_text += e\n",
    "\n",
    "# Tracer le nuage de mots\n",
    "plot_word_cloud(pos_text, \"coeur.png\", \"white\")\n",
    "plot_word_cloud(neg_text, \"mal.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "# Création d'un histogramme\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "from collections import Counter\n",
    "import seaborn as sns \n",
    "\n",
    "chaine = ' '.join(i.lower() for i in df_neg.Text)\n",
    "dico = Counter(chaine.split())\n",
    "mots = [m[0] for m in dico.most_common(15)]\n",
    "freq = [m[1] for m in dico.most_common(15)]\n",
    "\n",
    "plt.figure(figsize= (10,6))\n",
    "sns.barplot(x=mots, y=freq)\n",
    "plt.title('15 mots les plus fréquemment employés par les internautes laissant des mauvais commentaires')\n",
    "\n",
    "\n",
    "\n",
    "#Code à emporter\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "def plot_word_cloud(text, masque, background_color = \"black\"):\n",
    "   # Définir un masque\n",
    "   mask_coloring = np.array(Image.open(str(masque)))\n",
    "\n",
    "   # Définir le calque du nuage des mots\n",
    "   wc = WordCloud(background_color=background_color, max_words=200, \n",
    "                     stopwords=stop_words, mask = mask_coloring, \n",
    "                     max_font_size=50, random_state=42)\n",
    "\n",
    "   # Générer et afficher le nuage de mots\n",
    "   plt.figure(figsize= (20,10))\n",
    "   wc.generate(text)\n",
    "   plt.imshow(wc)\n",
    "   plt.show()\n",
    "\n",
    "plot_word_cloud(text, \"iron.jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####intro au texte mining - Dataviz' : Analyse de sentiments ------------------------------------------------------------\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# importer les packages nécessaires\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# lire le fichier \"movies_coms.csv'\n",
    "df = pd.read_csv('movies_comments.csv')\n",
    "\n",
    "# Afficher les deux premières lignes de df\n",
    "df.head(2)\n",
    "\n",
    "\n",
    "#separer les features et la target\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split   # Importer la classe train_test \n",
    "\n",
    "X, y = df['Text'], df['Sentiment']   # Séparer la variable explicative de la variable à prédire\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # Séparer en données d'entraînement et données test \n",
    "\n",
    "\n",
    "\n",
    "#Marche a suivre\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Le Gradient Boosting Tree est performant en Analyse de sentiments. \n",
    "# 1 - L'approche suivante explique le raisonnement utilisé dans la conception d'un GBT :\n",
    "\n",
    "# 2 - Prendre une pondération aléatoire (poids  wiwi ) pour les classificateurs faibles (paramètres  aiai ) \n",
    "#et former un classificateur final.\n",
    "\n",
    "# 3 - Calculer l'erreur induite par ce classificateur final, et chercher le classificateur faible qui s'approche le plus \n",
    "#de cette erreur.\n",
    "\n",
    "# 4 - Retrancher ce classificateur faible du classificateur final tout en optimisant son poids par rapport à une fonction de perte.\n",
    "\n",
    "# 5 - Répéter le procédé itérativement.\n",
    "\n",
    "\n",
    "#Back of word \n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Initialiser un objet vectorisateur\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Mettre à jour la valeur de X_train et X_test\n",
    "X_train = vectorizer.fit_transform(X_train).todense()\n",
    "X_test = vectorizer.transform(X_test).todense()\n",
    "\n",
    "\n",
    "#la méthode GradientBoostingClassifier\n",
    "#La fonction GradientBoostingClassifier permet d'initialiser un classificateur permettant l'application de cet algorithme\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#clf = GradientBoostingClassifier() : permet d'initialiser un classificateur clf.\n",
    "# Créer un classificateur clf et entraîner le modèle sur l'ensemble d'entraînement\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "\n",
    "\n",
    "#clf.fit(data, target) : exécute l'algorithme gradient boosting sur le jeu de données \n",
    "#en considérant data comme varaible(s) explicative(s) et target comme variable(s) cible(s).\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#clf.predict(vars) : effectue la prédiction des variables explicatives.\n",
    "# Calculer les prédictions \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#Evaluation du modele\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "#  Importer la classe classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calcul et affichage de classification_report\n",
    "print( classification_report(y_test, y_pred) )\n",
    "\n",
    "# Calcul et affichage de la matrice de confusion\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Classe réelle'], colnames=['Classe prédite'])\n",
    "confusion_matrix\n",
    "\n",
    " #predire el resultat\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "comments = [\"Harry Potter 7 part I is an amazing movie even it is not adapted for children.\", \n",
    "            \"I left after 45 min because this movie was boring !\",\n",
    "           \"it s was horrible\",\n",
    "           \"love and horrible\"]\n",
    "\n",
    "tokenized_comments = vectorizer.transform(comments)\n",
    "\n",
    "clf.predict(tokenized_comments.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word cloud\n",
    "À l'aide du package pandas, importer le fichier trump.csv, le séparateur utilisé est \",\".\n",
    "Afficher les 10 premières lignes du data set.\n",
    "Pour cette section, nous n'utiliserons que la première colonne de ce fichier, nous utiliserons la colonne sentiment à la section suivante.\n",
    "\n",
    "Transformer la première colonne de la data frame en une variable chaîne de caractères nommée paroles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv ('trump.csv')\n",
    "df.head()\n",
    "\n",
    "\n",
    "Paroles =\"\"\n",
    "\n",
    "for mot in df['Paroles'] :\n",
    "    Paroles = Paroles + mot\n",
    "    \n",
    "Paroles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nettoyage et mise en forme\n",
    "Certains détails (rires du public, applaudissement) apparaissent dans le texte et sont inscrits dans des crochets. À l'aide du module re et en vous servant des expressions régulières, remplacer tous les mots entre crochet présents dans la variable paroles par des espaces.\n",
    "À l'aide de la classe stopwords du package , initialiser la variable stop_words contenant les mots vides anglais.\n",
    "Mettre à jour cette variable en rajoutant les chaînes de caractères suivantes : \"?\", \"!\", \".\", \",\", \":\", \";\", \"-\", \"--\", \"...\", \"\"\", \"'\", \"they've\", \"they're\", \"they'll\", \"i've\", \"i'm\", \"i'll\", \"could\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords  \n",
    "\n",
    "r = re.compile(r\">[\\w+]<\")\n",
    "Paroles = r.sub(\" \", Paroles)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))    \n",
    "\n",
    "stop_words.update([\"?\", \"!\", \".\", \",\", \":\", \";\", \"-\", \"--\", \"...\", '\"', \"'\", \n",
    "\"they've\", \"they're\", \"they'll\", \"i've\", \"i'm\", \"i'll\", \"could\"])\n",
    "\n",
    "\n",
    "Paroles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction word_tokenize du package nltk.tokenize considère les contractions (we'll, didn't, etc.) comme deux différents mots, pour éviter cela nous utiliserons la classe TweetTokenizer qui elle ne fait pas cette distinction.\n",
    "\n",
    "Importer la classe TweetTokenizer du package nltk.tokenize.\n",
    "Initialiser tokenizer, une instance de la classe TweetTokenizer.\n",
    "À l'aide de la méthode tokenize de l'objet tokenizer, découper la variable paroles en token. Stocker ces tokens dans une nouvelle liste nommée mots.\n",
    "Retirer tous les stop words de la liste mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer \n",
    "\n",
    "Paroles = Paroles.lower()\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "mots = tokenizer.tokenize (Paroles)\n",
    "\n",
    "\n",
    "tokens = []\n",
    "for mot in mots:\n",
    "    if mot not in stop_words:\n",
    "        tokens.append(mot)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction du word cloud\n",
    "Importer la classe WordCloud de la bibliothèque wordcloud.\n",
    "\n",
    "Instancier le calque du nuage de mot wc à partir de la classe WordCloud, en prenant pour paramètres :\n",
    "\n",
    "Une couleur de fond blanche\n",
    "Un maximum de mots à afficher égal à 1000\n",
    "Une police de taille maximale égale à 90\n",
    "Régler également les options suivantes :\n",
    "\n",
    "collocations sur False\n",
    "random_state fixé à 42\n",
    "mask fixé avec la variable mask\n",
    "\n",
    "À l'aide du sous package matploblib.pyplot, afficher le wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot = ' '.join (mot for mot in tokens) \n",
    "mot\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "mask = np.array(Image.open(\"trump.jpg\"))\n",
    " \n",
    "    # Définir le calque du nuage des mots\n",
    "wc = WordCloud(background_color='white', \n",
    "                   max_words=1000, \n",
    "                   stopwords=stop_words, \n",
    "                   mask = mask, \n",
    "                   collocations = False,\n",
    "                   max_font_size=90, \n",
    "                   random_state=42)\n",
    "\n",
    "    # Générer et afficher le nuage de mots\n",
    "plt.figure(figsize= (20,10))\n",
    "wc.generate(mot)\n",
    "plt.imshow(wc)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaite personnaliser le wordcloud, notamment modifier les couleurs du texte, pour que celui prenne automatiquement les couleurs de l'image d'origine.\n",
    "\n",
    "Importer la fonction ImageColorGenerator du package wordcloud.\n",
    "Créer une variable img_color à l'aide de la fonction précédente en précisant en argument votre variable mask.\n",
    "Utiliser la méthode recolor de la classe WordCloud et préciser comme paramètre \"color_func = img_color\".\n",
    "Pour plus de lissage sur les lettres, utiliser l'option \"interpolation=\"bilinear\"\" dans la méthode imshow de pyplot.\n",
    "Retirer les axes de votre graphique.\n",
    "Afficher de nouveau le wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import ImageColorGenerator\n",
    "\n",
    "img_color = ImageColorGenerator (mask)\n",
    "wc = wc.recolor (color_func = img_color)\n",
    "plt.imshow(wc,interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis\n",
    "Séparation des données\n",
    "La première étape consiste à scinder notre data set en un échantillon d'entrainement et un échantillon test pour pouvoir évaluer les performances de notre modèle.\n",
    "\n",
    "Importer la fonction train_test_split du package sklearn.model_selection.\n",
    "Créer une variable X qui ne contient que la colonne Paroles du data set trump.\n",
    "Stocker la colonne Sentiment dans la variable y.\n",
    "À l'aide de la fonction train_test_split, diviser les matrices en un ensemble d'entraînement et un ensemble de test (en précisant 20% pour la taille de l'échantillon test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insérez votre code ici\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "X, y = df['Paroles'], df['Sentiment']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorisation\n",
    "La seconde étape est la vectorisation, elle consiste à convertir chaque paragraphe des différents discours en une représentation numérique. Cela passe par la création d'un corpus et d'une matrice termes-documents.\n",
    "\n",
    "Importer la classe CountVectorizer du package sklearn.feature_extraction.text.\n",
    "Initialiser un objet vectorisateur, vectorizer, à l'aide de la méthode CountVectorizer.\n",
    "Entraîner le vectorisateur sur le jeu de données X_train et mettre à jour sa valeur en lui affectant les tokens numériques correspondants.\n",
    "Mettre à jour la valeur de X_test en lui affectant la transformation de X_test en tokens numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train).todense()\n",
    "X_test = vectorizer.transform(X_test).todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainement du modèle\n",
    "Créer un classifieur clf, ayant pour paramètre C = 1.0, en utilisant la méthode LogisticRegression du package linear_model.\n",
    "Entraîner l'algorithme sur l'ensemble d'entraînement.\n",
    "Affecter à la variable y_pred les prédictions des commentaires stockées dans la variable X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import  LogisticRegression\n",
    "\n",
    "clf = LogisticRegression ( C = 1.0)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance du modèle\n",
    "Importer la classe classification_report du package sklearn.metrics.\n",
    "Afficher le rapport de la classification du modèle LogisticRegression.\n",
    "Calculer et afficher la matrice de confusion en prenant en compte les classes estimées y_pred et réelles y_test.\n",
    "Comparer les performances de ce modèle avec un modèle Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print( classification_report(y_test, y_pred) )\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Classe réelle'], colnames=['Classe prédite'])\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf2 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "y_pred2 = clf2.predict(X_test)\n",
    "\n",
    "print( classification_report(y_test, y_pred2) )\n",
    "\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred2, rownames=['Classe réelle'], colnames=['Classe prédite'])\n",
    "confusion_matrix\n",
    "\n",
    "\n",
    "#le gradient boost est moins performant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
