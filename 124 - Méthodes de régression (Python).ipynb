{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "##  Modèles de régression avec scikit-learn : Régression linéaire simple\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Rappel : \n",
    "# Etant donnée deux variables X et Y, dites respectivement variable explicative et variable expliquée,\n",
    "# la régression linéaire simple consiste en la modélisation d'un lien entre la variable explicative et la variable expliquée\n",
    "# permettant de prédire la valeur de Y lorsque la valeur de X est mesurée\n",
    "\n",
    "# Le modèle usuel de régression linéaire simple est le suivant:  y = β0 + β1 x + ε \n",
    "        # avec : \n",
    "        # y  est une variable, quantitative continue, à expliquer (ou variable dépendante),\n",
    "        # x  est une variable, quantitative continue, explicative,\n",
    "        # ε  est un terme d’erreur aléatoire de loi normale d’espérance nulle et d’écart-type σ.\n",
    "\n",
    "\n",
    "# - A ------------------------------------------------------------------------------ Preparation des df\n",
    "\n",
    "# - 1 --------------------------------------- import\n",
    "\n",
    "import pandas as pd\n",
    "import  numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import  train_test_split , cross_validate\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "df = pd.read_csv('automobiles.csv')  \n",
    "\n",
    "df.head(1)\n",
    "\n",
    "df.describe() #les statistiques descriptives élémentaires des colonnes de df.\n",
    "\n",
    "df.plot.scatter ( 'curb-weight' , 'price' )   #tracer une courbe \n",
    "\n",
    "\n",
    "# - 2 --------------------------------------- Créer les vecteurs prix et mv\n",
    "prix  = df['price']\n",
    "mv  = df[ 'curb-weight' ]\n",
    "\n",
    "# - 2 bis --------------------------------------- Créer les vecteurs prix et mv : shape\n",
    "mv.shape  #(159,)\n",
    "mv = np.expand_dims(mv, axis=1) #new dim -> new shape (159, 1)\n",
    "mv =  np.hstack( (mv, np.ones(mv.shape)))   #array([[mv(1), 1.000e+00],[mv(2), 1.000e+00]]\n",
    "mv.shape  #(159,2)\n",
    "\n",
    "# - B ------------------------------------------------------------------------------ Modelisation\n",
    "# - 3 --------------------------------------- Instanciez un régresseur slr -> classe LinearRegression\n",
    "slr = LinearRegression ()\n",
    "slr.fit(mv,prix) #entrainement\n",
    "\n",
    "\n",
    "# - 4 --------------------------------------- intercept et l'unique coefficient\n",
    "#intercept_ : à l'ordonnée à l'origine  \n",
    "#coef_ : la pente de la droite dans le cas d'une régression linéaire simple\n",
    "slr.intercept_ , slr.coef_ \n",
    "#prix = intercept_ + slr.coef_  x curb-weight.\n",
    "#prix = -15378.2 + 10.9 x curb-weight.\n",
    "#y    = b        + a    x curb-weight.\n",
    "\n",
    "\n",
    "# - C ------------------------------------------------------------------------------coefficient de détermination (R²) \n",
    "\n",
    "# - 5 --------------------------------------- Evaluez le modèle slr par validation croisée à 4 échantillons\n",
    "cross_validate(slr, mv, prix, return_train_score=True, cv=4)\n",
    "\n",
    "# moyenne des scores obtenus sur les échantillons de test\n",
    "cross_validate(slr, mv, prix, return_train_score=True, cv=4)['test_score'].mean() #\n",
    "#\n",
    "\n",
    "\n",
    "# - 7 --------------------------------------- prediction \n",
    "#Calculez les valeurs ajustées dans un tableau appelé pred_prix, puis calculez les résidus dans residus\n",
    "pred_prix = slr.predict(mv)\n",
    "residus = pred_prix - prix\n",
    "residus.describe()\n",
    "\n",
    "\n",
    "# - 8 ---------------------------------------  Afficher 1 droite de régression\n",
    "#Afficher un nuage de point entre les valeurs de 'curb-weigtht' et les prix.\n",
    "# & la droite de régression grâce aux prédictions pred_prix.\n",
    "plt.figure(figsize= (10,8))\n",
    "plt.scatter(mv['curb-weight'], prix, color = 'darkblue')     #x / y reel \n",
    "plt.plot(mv, pred_prix ,color='k')                           #x / y predict\n",
    "\n",
    "\n",
    "# - 9 ---------------------------------------  Importer la fonction f_regression\n",
    "\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "print('F-score:', f_regression(mv, prix)[0], 'p-value :', f_regression(mv, prix)[1])\n",
    "\n",
    "## p-value ~ 0 donc la variable mv très significative.\n",
    "\n",
    "\n",
    "# - 10 --------------------------------------  l'erreur quadratique (RMSE en anglais)\n",
    "#Le RMSE est la racine carrée de la moyenne des résidus au carré.\n",
    "\n",
    "def rmse(predictions, targets):  #fct RMSE\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "rmse(pred_prix, prix)\n",
    "\n",
    "#plus l'erreur quadratique moyenne est proche de 0, plus précises sont les prédictions.\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "pred_prix2 = cross_val_predict(slr, mv, prix, cv=4) \n",
    "rmse(pred_prix2, prix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "##  Modèles de régression avec scikit-learn : Régression linéaire multiple\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#rappelle que le modèle usuel de régression linéaire multiple est le suivant:\n",
    "                        #   yi=β0+β1xi,1+β2xi,2+⋯+βpxi,p+εi\n",
    "                        #     =β0+∑j=1pβjxi,j+εi\n",
    "#où :\n",
    "#yi est une variable, quantitative continue, à expliquer (ou variable dépendante),\n",
    "#x1,…,xp sont des variables, quantitatives continues, explicatives,\n",
    "#les εi sont des termes d'erreur aléatoire de loi normale d'espérance nulle et d'écart-type σ.\n",
    "\n",
    "\n",
    "# - A ------------------------------------------------------------------------------ Preparation \n",
    "\n",
    "# - 1 --------------------------------------- import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"automobiles.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# - B ------------------------------------------------------------------------------ Modelisation (la totalité des features)\n",
    "\n",
    "\n",
    "# - 1 --------------------------------------- target, data\n",
    "target = df['price']\n",
    "data = df.drop('price',axis =1)\n",
    "\n",
    "\n",
    "# - 2 --------------------------------------- ensemble d'apprentissage et un ensemble de test\n",
    "X_train,X_test, y_train,  y_test = train_test_split (data,target,test_size = 0.2 )\n",
    "\n",
    "\n",
    "# - 3 --------------------------------------- modelisation + fit \n",
    "lr = LinearRegression()    #Instanciez un régresseur lr de la classe LinearRegression\n",
    "lr.fit(X_train , y_train)  #fit\n",
    "\n",
    "\n",
    "# - 4 --------------------------------------- Intercept et coefficients \n",
    "coeffs = list(lr.coef_)\n",
    "coeffs.insert(0, lr.intercept_)\n",
    "\n",
    "feats = list(data.columns)\n",
    "feats.insert(0, 'intercept')\n",
    "\n",
    "pd.DataFrame({'valeur estimée': coeffs}, index = feats)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# - C ------------------------------------------------------------------------------ Score \n",
    "\n",
    "# - 5 --------------------------------------- score échantillon d'apprentissage\n",
    "lr.score(X_train, y_train)      # score du modèle sur l'échantillon d'apprentissage\n",
    "\n",
    "cross_val_score(lr,X_train,y_train).mean()   #score obtenu par validation croisée\n",
    "\n",
    "#si les scores sont trop differents pb (surentrainement ) voir score echantillion tests pour determiner le pb\n",
    "\n",
    "\n",
    "# - 7 --------------------------------------- score du modèle sur l'ensemble de test\n",
    "lr.score(X_test,y_test)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# - D ------------------------------------------------------------------------------ Graph \n",
    "\n",
    "# - 8 --------------------------------------- graphique pred_test  / \n",
    "\n",
    "pred_test = lr.predict(X_test)  \n",
    "plt.scatter(pred_test, y_test)  # y pred/y reeel\n",
    "plt.plot((y_test.min(),y_test.max()), (y_test.min(),y_test.max()))  #y reel / y reel avec un delta maximiser\n",
    "\n",
    "#analyse si point disperses, residus\n",
    "\n",
    "\n",
    "# - 9 --------------------------------------- Prediction et residus\n",
    "pred_train = lr.predict(X_train)\n",
    "residus = pred_train - y_train\n",
    "\n",
    "\n",
    "# - 10 -------------------------------------- nuage des points , les résidus en fonction des valeurs de y_train\n",
    "plt.scatter(y_train, residus, color = '#980a10', s=15)\n",
    "plt.plot((y_train.min(),y_train.max()), (0,0), lw=3, color = '#0a5798')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# - D ------------------------------------------------------------------------------ ajustement d'une distribution \n",
    "\n",
    "\n",
    "# - 11 --------------------------------------- Centrez normaliser \n",
    "#Centrez réduire les résidus dans un vecteur appelé residus_norm\n",
    "residus_norm = (residus-residus.mean())/residus.std()\n",
    "\n",
    "\n",
    "# - 12 -------------------------------------- QQ-Plot  (courbe : un diagramme Quantile-Quantile)\n",
    "stats.probplot(residus_norm, plot=plt)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# - 13 -------------------------------------- correlation des features ( data) \n",
    "# on parle de corrélation si > 80\n",
    "plt.figure(figsize=(13,13))\n",
    "sns.heatmap(df.corr(),  annot=True, cmap=\"RdBu_r\", center =0);\n",
    "\n",
    "#ou \n",
    "data.corr()\n",
    "\n",
    "# - 14 -------------------------------------- Affichez une matrice des nuages de points (les var les + correle a la target)\n",
    "#entre les variables 'curb-weight', 'horsepower', 'highway-mpg', 'height', 'bore', 'width' et 'price' de df.\n",
    "\n",
    "sns.pairplot(df[['curb-weight', 'horsepower', 'highway-mpg', 'height', 'bore', 'width', 'price']])\n",
    "\n",
    "\n",
    "\n",
    "#analyse des courbes entre les datas et la target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# - E ------------------------------------------------------------------------------ ajustement d'une distribution \n",
    "\n",
    "\n",
    "# - 15 -------------------------------------- Créez une liste \"signif_features\" avec var correlé \n",
    "#qui contient les noms des variables 'curb-weight', 'horsepower', 'bore', 'width'  var correlé\n",
    "signif_features = ['curb-weight', 'horsepower', 'bore', 'width']  #list de str colomnnes choisie\n",
    "\n",
    "\n",
    "# - 16 -------------------------------------- modèle de régression lr2 + l'entraîner + score sur ensemble de train\n",
    "lr2 = LinearRegression()\n",
    "lr2.fit( X_train[signif_features], y_train)   #fit avec xtrain des colonnes choisie\n",
    "lr2.score(X_train[signif_features], y_train)  #score avec xtrain des colonnes choisie\n",
    "\n",
    "lr2.score(X_test[signif_features], y_test)  #score sur ensemble de test\n",
    "\n",
    "#La sélection des variables a permis de réduire l'over-fitting (sur-apprentissage)\n",
    "# - > Le modèle est plus consistant et plus robuste\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# - F ------------------------------------------------------------------------------ fonction SelectKBest\n",
    "#Sélectionner, à partir d'un jeu de données et d'une variable cible, \n",
    "#les k variables les plus significatives par rapport à une fonction de score utilisée, (exemple :la fonction f_regresison)\n",
    "\n",
    "\n",
    "# - 1 --------------------------------------- SelectKBest\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "sk = SelectKBest(f_regression, k=3) #selecteur de var (3var )\n",
    "\n",
    "sk.fit(X = data, y = target)   #fit\n",
    "\n",
    "data.columns[sk.get_support()]  #Affiche noms de colonnes sélectionnées par le sélecteur\n",
    "\n",
    "\n",
    "# - 2 -------------------------------------- méthode transform\n",
    "\n",
    "sk_train = sk.transform( X_train ) #transform :transforme en table numpy    \n",
    "sk_test = sk.transform ( X_test)   # les colonnes selectionnée par le selecteur SelectKBest\n",
    "\n",
    "\n",
    "# - 3 -------------------------------------- modèle de régression linéaire\n",
    "\n",
    "lr10 = LinearRegression ()             #modele\n",
    "lr10.fit   ( sk_train , y_train )             #fit\n",
    "print (lr10.score ( sk_train , y_train ))   #score\n",
    "print (lr10.score ( sk_test , y_test  ))   #score\n",
    "\n",
    "\n",
    "# - G ------------------------------------------------------------------------------ fonction SelectFromModel \n",
    "#sélectionner les variables importantes d'un dataframe, \n",
    "#à partir d'un modèle crée possédant un attribut coef_ ou feature_importances\n",
    "\n",
    "\n",
    "# - 1 --------------------------------------- classe SelectFromModel\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel   #import de la foncction \n",
    "\n",
    "lr = LinearRegression()    # modèle de régression linéaire lr\n",
    "\n",
    "sfm = SelectFromModel(lr)  # sélecteur sfm SelectFromModel, à partir du modèle lr créé\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)  #appel de fct de normalisation\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)  # -> table numpy + norm\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)    # -> table numpy+ norm\n",
    "\n",
    "sfm_train = sfm.fit_transform(X_train_scaled, y_train)   # -> table numpy + norm + colonne choisie par selecteur\n",
    "\n",
    "sfm_test = sfm.transform(X_test_scaled)                 # -> table numpy+ norm\n",
    "\n",
    "# - 2 --------------------------------------- nom des colonnes sélectionnés par sfm\n",
    "data.columns[sfm.get_support()]   #retopurne les noms des col choisies \n",
    "\n",
    "\n",
    "# - 3 --------------------------------------- modèle + fit + score\n",
    "sfmlr = LinearRegression()        #   modele de \n",
    "sfmlr.fit(sfm_train, y_train)     # fit\n",
    "\n",
    "print(sfmlr.score(sfm_train, y_train))   #score \n",
    "print(sfmlr.score(sfm_test, y_test))     #score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "##  Modèles de régression avec scikit-learn : Régression linéaire régularisée : Ridge, Lasso \n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#rappelle que le modèle est le suivant:\n",
    "        #  régression linéaire est modélisée par l'équation:      yi=β0+β1xi,1+β2xi,2+⋯+βpxi,p+εi\n",
    "\n",
    "        #    forme matricielle :                                  Y=Xβ+ϵ\n",
    "\n",
    "#La Régression régularisée : \n",
    "# L'objectif est d'éviter le sur-apprentissage \n",
    "\n",
    "# - A ------------------------------------------------------------------------------ Régression Ridge\n",
    "#La régression Ridge consiste en l'ajout d'une contrainte sur les coefficients \n",
    "#lors de la modélisation pour maîtriser l'amplitude de leurs valeurs.\n",
    "#    minβ(||||Y−Xβ||||2+α||||β||||2)    avec α ou le coefficient de pénalité\n",
    "\n",
    "\n",
    "# - 1 --------------------------------------- import\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#impoort csv\n",
    "df = pd.read_csv('automobiles.csv')\n",
    "\n",
    "#connaite son df\n",
    "df.head(2)\n",
    "df.describe()\n",
    "df.shape # (159, 16)\n",
    "\n",
    "\n",
    "\n",
    "# - 2 --------------------------------------- traitement du df\n",
    "\n",
    "#normalisation\n",
    "scaler = preprocessing.StandardScaler().fit_transform(df)  #fct de normalisation \n",
    "df[df.columns] = pd.DataFrame(scaler)  #normalisation\n",
    "\n",
    "#cut the df  \n",
    "target = df['price']                #y = target\n",
    "data = df.drop('price', axis = 1)   #x = data \n",
    "\n",
    "X_train, X_test,  y_train, y_test = train_test_split (data, target , test_size = 0.2) # ensemble d'apprentissage, de test\n",
    "\n",
    "\n",
    "# - 3 ---------------------------------------  classe Ridge \n",
    "\n",
    "from sklearn.linear_model import RidgeCV  #import modele RidgeCV \n",
    "\n",
    "ridge_reg = RidgeCV(alphas= (0.001, 0.01, 0.1, 0.3, 0.7, 1, 10, 50, 100))  #le modele choisit le param alpha le + mieux\n",
    "\n",
    "ridge_reg.fit( X_train,   y_train)  #fit du modele \n",
    "\n",
    "# - 4 --------------------------------------- param alpha_\n",
    "ridge_reg.alpha_   #Affiche le  α  retenu par le modèle\n",
    "\n",
    "\n",
    "# - 5 --------------------------------------- SCore des ensembles\n",
    "print(\"score train :\", ridge_reg.score(X_train, y_train))       #fit de l ensemble train \n",
    "print(\"score test :\", ridge_reg.score(X_test, y_test))       #fit de l ensemble trtestain \n",
    "\n",
    "#analyse des scores et deduction si le modèle overfit\n",
    "\n",
    "# - 6 --------------------------------------- Predict + mean_squarred_error\n",
    "\n",
    "ridge_pred_train= ridge_reg.predict ( X_train )    #predict  \n",
    "ridge_pred_test = ridge_reg.predict ( X_test )    #predict\n",
    "\n",
    "print (mean_squared_error(ridge_pred_train, y_train))   #mean_squarred_error\n",
    "print (mean_squared_error(ridge_pred_test, y_test))   #mean_squarred_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# - B ------------------------------------------------------------------------------ Régression Lasso\n",
    "#La régression Lasso est similaire à la régression Ridge, \n",
    "#la seule différence entre les deux étant la contrainte sur la norme L1 avec Lasso, plutôt que sur la norme L2.\n",
    "\n",
    "\n",
    "# - 1 ---------------------------------------  classe lasso \n",
    "\n",
    "from sklearn.linear_model import Lasso   #imoport modele\n",
    "\n",
    "lasso_r = Lasso(alpha=1)     #modele lasso instancier\n",
    "\n",
    "lasso_r.fit(X_train, y_train)  #fit du modele  sur l ensemble de train\n",
    "\n",
    "\n",
    "# - 2 ---------------------------------------   coef_ \n",
    "lasso_r.coef_  #attribut coef\n",
    "\n",
    "#analyse du resultat si modele =0 alpha doit etre changé\n",
    "\n",
    "\n",
    "# - 3 ---------------------------------------  classe lasso new modele\n",
    "\n",
    "lasso_reg = Lasso(alpha=1)     #modele lasso instancier\n",
    "\n",
    "lasso_reg.fit(X_train, y_train)  #fit du modele  sur l ensemble de train\n",
    "\n",
    "# - 4 ---------------------------------------  courbe du coef \n",
    "\n",
    "plt.plot(range(len(data.columns)), lasso_coef)\n",
    "plt.xticks(range(len(data.columns)), data.columns.values, rotation=70)\n",
    "plt.show()\n",
    "\n",
    "#analyse des variables choisie ( >0)\n",
    "\n",
    "\n",
    "# - 5 ---------------------------------------  score R2 sur l'ensemble d'apprentissage et l'ensemble de test\n",
    "print(\"score train:\",lasso_reg.score(X_train,y_train))\n",
    "print(\"score test :\", lasso_reg.score(X_test,y_test))\n",
    "\n",
    "\n",
    "# - 6 --------------------------------------- Predict + mean_squarred_error\n",
    "lasso_pred_train = lasso_reg.predict(X_train)\n",
    "lasso_pred_test = lasso_reg.predict(X_test)\n",
    "\n",
    "print(\"mse train:\", mean_squared_error(lasso_pred_train, y_train))\n",
    "print(\"mse test:\", mean_squared_error(lasso_pred_test, y_test))\n",
    "\n",
    "\n",
    " \n",
    "# - C ------------------------------------------------------------------------------ lasso_path \n",
    "# produit les coefficients éstimés corrrespondant aux différents  αα  qu'elle reçoit en arguments\n",
    "\n",
    "# - 1 ---------------------------------------   lasso_path\n",
    "\n",
    "from sklearn.linear_model import lasso_path    #imoport fonction\n",
    "\n",
    "mes_alphas = (0.001,0.01,0.02,0.025,0.05,0.1,0.25,0.5,0.8,1.0)   #liste d alplha\n",
    "\n",
    "alpha_path, coefs_lasso, _ = lasso_path(X_train, y_train, alphas=mes_alphas)  #sur l ensemble de train on applique le param\n",
    "\n",
    "coefs_lasso.shape\n",
    "\n",
    "# - 2 ---------------------------------------  graphique\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "for i in range(coefs_lasso.shape[0]):\n",
    "    plt.plot(alpha_path, coefs_lasso[i,:], '--')\n",
    "\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Lasso path')\n",
    "plt.show()\n",
    "\n",
    "#analyse \n",
    "#Le graphique nous montre qu'à partir de  α=0.3, plus que 3 variables seulement sont sélectionnées par le modèle, \n",
    "#et qu'à  α=1 , effectivement, tous les coefficients sont nuls.\n",
    "\n",
    "\n",
    "# - 3 ---------------------------------------  modèle LassoCV\n",
    "\n",
    "from sklearn.linear_model import LassoCV   #import classe\n",
    "\n",
    "model_lasso = LassoCV(cv=10).fit(X_train, y_train)  #modele + fit\n",
    "\n",
    "alphas = model_lasso.alphas_  #param alpha\n",
    "\n",
    "\n",
    "#Figure Mean square error pour chaque échantillon\n",
    "plt.figure(figsize = (10,8))  \n",
    "\n",
    "plt.plot(alphas, model_lasso.mse_path_, ':')\n",
    "\n",
    "plt.plot(alphas, model_lasso.mse_path_.mean(axis=1), 'k',     #Moyenne\n",
    "         label='Moyenne', linewidth=2)\n",
    "\n",
    "plt.axvline(model_lasso.alpha_, linestyle='--', color='k',    #alpha: estimation CV\n",
    "            label='alpha: estimation CV')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Mean square error')\n",
    "plt.title('Mean square error pour chaque échantillon ')\n",
    "plt.show()\n",
    "\n",
    "# - 4 ---------------------------------------  Predict + score + MSE \n",
    "\n",
    "pred_test = model_lasso.predict(X_test)                 #predict\n",
    "\n",
    "print(\"score test:\", model_lasso.score(X_test, y_test))   #score\n",
    "print(\"mse test:\", mean_squared_error(pred_test, y_test)) #mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "##  Modèles de régression avec scikit-learn : Qui sera le MVP ? - Elastic Net\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# - A ------------------------------------------------------------------------------ Dataset\n",
    "\n",
    "# - 1 ---------------------------------------  Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import model_selection, preprocessing\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# - 2 ---------------------------------------  traitement df\n",
    "df = pd.read_csv (  'nba_2013.csv'  )   #import \n",
    "\n",
    "nba.describe()    #connaitre son df\n",
    "nba.info()\n",
    "nba.shape\n",
    "\n",
    "nba.index = nba [ ['player', 'bref_team_id'] ]  #modif du df (index)\n",
    "\n",
    "#Supprimez les lignes contenant des valeurs manquantes.\n",
    "nba.isnull().sum()  #check val na\n",
    "nba.dropna( how='any',inplace = True )  #supprime val na\n",
    "\n",
    "#Affiche les différentes modalités prises par la variables pos.\n",
    "nba['pos'].mode()[0]   #modalite la plus courante\n",
    "nba['pos'].value_counts()   #modalites de la col pos\n",
    "\n",
    "#Supprime les lignes contenant des valeurs aberrantes pour la variable pos\n",
    "nba = nba[nba ['pos'] != 'G']\n",
    "\n",
    "pl_pos = nba ['pos'] # var en string\n",
    "\n",
    "nba = nba.drop ( ['season','player', 'bref_team_id', 'pos' ] , axis = 1 )   #Supprime les colonnes \n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit (nba)        #fct de normalisation les colonnes de nba\n",
    "nba[nba.columns] = pd.DataFrame(scaler.transform(nba), index= nba.index)  #utilisation de la fct de normalisatoin\n",
    "\n",
    "nba = nba.join( pd.get_dummies (pl_pos, prefix = 'Pos') )  #var en string -> get dummies\n",
    "\n",
    "\n",
    "# - 3 ---------------------------------------  Separation target/data\n",
    "#separation du df en cible / feature \n",
    "target = nba ['pts']                                   #target  ou y\n",
    "data   = nba.drop(  ['pts','Pos_SG'] , axis = 1  )    #data ou x\n",
    "\n",
    "\n",
    "\n",
    "# - B ------------------------------------------------------------------------------ Modele LinearRegression\n",
    "\n",
    "\n",
    "# - 1 ---------------------------------------  train_test_split\n",
    "#échantillons d'apprentissage et de test \n",
    "x_train,x_test,y_train,y_test = train_test_split ( data , target , test_size = 0.2, random_state=101 )\n",
    "\n",
    "\n",
    "# - 2 ---------------------------------------  correlation \n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure ( figsize=(16,15) )\n",
    "sns.heatmap (  nba.corr() , annot=True, cmap=\"RdBu_r\", center =0) #heatmap + corr()\n",
    "\n",
    "\n",
    "# - 3 ---------------------------------------  modèle de régression linéaire simple \n",
    "from sklearn.linear_model import LinearRegression   #import fct\n",
    "model_1 = LinearRegression ()     #creation de modele\n",
    "\n",
    "model_1.fit ( X_train_mp,y_train )   #fit   #ca marchera pas il faut \n",
    "\n",
    "# - 4 ---------------------------------------  m n matrice target / Data\n",
    "#echantill train \n",
    "X_train_mp = X_train['mp']   \n",
    "X_train_mp.shape   #(368,)\n",
    "X_train_mp = np.expand_dims( X_train_mp , axis = 1 ) #(368,1)\n",
    "X_train_mp  = np.hstack( ( X_train_mp, np.ones(X_train_mp.shape)) ) #Fusion  #(368,2)\n",
    "y_train.shape   #(368,)\n",
    "#test\n",
    "X_test_mp = X_test['mp'] \n",
    "X_test_mp.shape  #(92,)\n",
    "X_test_mp = np.expand_dims( X_test_mp, axis = 1 )  #new dimention \n",
    "X_test_mp.shape  #(92, 1)\n",
    "X_test_mp = np.hstack( (X_test_mp , np.ones( X_test_mp.shape)) )  #fusion \n",
    "y_test.shape    #(92,)\n",
    "\n",
    "# - 4 ---------------------------------------  score R² et le RMSE\n",
    "print (model_1.score ( X_train_mp , y_train  ) )   #score train\n",
    "print (model_1.score ( X_test_mp , y_test  ) )   #score test\n",
    "\n",
    "pred = model_1.predict (X_train_mp)      #predcition train\n",
    "pred_test = model_1.predict(X_test_mp)  #predcition test\n",
    "\n",
    "print(\"rmse train:\", np.sqrt(mean_squared_error(y_train, pred)))      # rmse train\n",
    "print(\"rmse test: \", np.sqrt(mean_squared_error(y_test, pred_test)))  # rmse test\n",
    "\n",
    "# - B ------------------------------------------------------------------------------ ElasticNetCV\n",
    "\n",
    "# - 1 ---------------------------------------  modèle + fit \n",
    "from sklearn.linear_model import  ElasticNetCV  #imprt fct \n",
    "\n",
    "# modèle de régression Elastic Net\n",
    "model_en =  ElasticNetCV ( l1_ratio = (0.1, 0.25, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 0.99) , \n",
    "                          alphas= (0.001,0.01,0.02,0.025,0.05,0.1,0.25,0.5,0.8,1.0) )\n",
    "\n",
    "model_en.fit (X_train , y_train )  #train\n",
    "\n",
    "# - 2 ---------------------------------------  intercept et coefficients estimé\n",
    "#l'intercept \n",
    "model_en.intercept_\n",
    "\n",
    "#coefficients estimés pour chaque variable de data.\n",
    "pd.DataFrame (  {'coef' :model_en.coef_}, index = data.columns)   \n",
    "\n",
    "\n",
    "# - 2 ---------------------------------------  courbe de la moyenne des erreurs MSE par validation croisée \n",
    "#en fonction des valeurs de  α\n",
    "alphas = model_en.alphas_\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "for i in range(model_en.mse_path_.shape[0]) :\n",
    "    plt.plot(alphas, model_en.mse_path_[i,:,:].mean(axis=1),\n",
    "         label='Moyenne pour l1_ratio= %.2f' %model_en.l1_ratio[i], linewidth=2)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.title('Mean squared error pour chaque $\\lambda$')\n",
    "plt.show()\n",
    "\n",
    "# - 3 ---------------------------------------  mse + predict + score\n",
    "#prédictions du modèle\n",
    "pred_train =  model_en.predict ( X_train )\n",
    "pred_test  =  model_en.predict ( X_test)\n",
    "\n",
    "#erreurs quadratiques moyennes\n",
    "print(np.sqrt(mean_squared_error(y_train, pred_train)))\n",
    "print(np.sqrt(mean_squared_error(y_test, pred_test)))\n",
    "\n",
    "print (model_en.score ( X_train  , y_train))\n",
    "print (model_en.score ( X_test , y_test))\n",
    "\n",
    "\n",
    "# - 4 --------------------------------------- moyenne et l'écart type calculés par l'objet scaler \n",
    "#et utilisés pour transformer les données\n",
    "# la moyenne et l'écart type calculés par l'objet scaler\n",
    "moy = scaler.mean_[-1]   \n",
    "ec = scaler.scale_[-1]\n",
    "print(\"moyenne :\", moy)\n",
    "print(\"ecart-type\", ec)\n",
    "\n",
    "# - 5 ---------------------------------------  points observés pour chaque joueur, et les points prédits par modèle Elastic Net\n",
    "pd.DataFrame({'points_obsérvés': (y_test*ec)+moy, 'points_predits' : np.round((pred_test*ec)+moy)}, index = X_test.index).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce test consiste à prédire, à l'aide de modèles de régressions, le prix d'une habitation en fonction de variables pertinentes représentant différentes caractéristiques architecturales, géographiques et propres au voisinage.\n",
    "\n",
    "Le jeu de données utilisé contient de nombreux attributs permettant de décrire 1460 habitations en fonction de plusieurs caractéristiques ainsi qu'une variable cible: 'SalePrice' contenant le prix de vente du bien en question.\n",
    "\n",
    "Le jeu de données est à lire dans le fichier \"house_price.csv\" (les colonnes sont séparées par des ';')\n",
    "\n",
    "Exécuter la cellule suivante pour importer les packages nécessaire pour la suite de l'exercice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge, LassoCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lire le fichier \"house_price.csv\" dans un data frame appelé hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insérez votre code ici\n",
    "\n",
    "\n",
    "df = pd.read_csv ('house_price.csv',sep = ',')\n",
    "\n",
    "col_num = df.select_dtypes ( include = ['float64', 'int64'] ).columns\n",
    "col_qual = df.select_dtypes ( include = ['object'] ).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remplacer, pour chaque variable, les valeurs manquantes par la moyenne de la variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df[col_num] :\n",
    "     df[i].fillna(df[i].mean() ,inplace = True)\n",
    "\n",
    "\n",
    "for i in df[col_qual] :\n",
    "     df[i].fillna(df[i].mode() [0],inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centrer et réduire les variables numériques du data frame, à l'aide d'une instance de la classe preprocessing.StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df [liste_num]\n",
    "\n",
    "#scaler = preprocessing.StandardScaler().fit (df)      \n",
    "#df[df.columns] = pd.DataFrame(scaler.transform(df), index= df.index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer chaque variable catégorielle en variables indicatrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join( pd.get_dummies (df[col_qual], prefix = 'gd') ) \n",
    "\n",
    "df = df.drop(df[col_qual], axis = 1)\n",
    "\n",
    "\n",
    "df.shape  # (1460, 290)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Séparer la variable cible 'SalePrice' dans target et le reste des variables dans feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['SalePrice']\n",
    "feats  = df.drop (['SalePrice'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Séparer les données en un ensemble d'apprentissage (X_train, y_train) et un ensemble de test(X_test, y_test), avec 20% des données originales pour le test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split ( feats , target , test_size = 0.2 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un une fonction rmse_cv, qui calcule pour un modèle donné, la racine de l'erreur quadratique moyenne obtenue par validation croisée à 5 échantillons, grâce à la fonction cross_val_score de sklearn.model_selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def rmse_cv(mod,X,y):  \n",
    "    return np.sqrt( cross_val_score(mod,X,y, cv=5, scoring=make_scorer(mean_squared_error)).mean() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un vecteur alphas contenant les valeurs [0.01, 0.05, 0.1, 0.3, 0.8, 1, 5, 10, 15, 30, 50].\n",
    "Afficher dans un graphique, le RMSE retourné par la fonction rmse_cv appliquée à un modèle de régression Ridge(alpha = αα ) pour chaque  αα  appartenant à alphas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insérez votre code ici\n",
    "\n",
    "from sklearn.linear_model import RidgeCV  #import modele RidgeCV \n",
    "\n",
    "alpha = [0.01, 0.05, 0.1, 0.3, 0.8, 1, 5, 10, 15, 30, 50 ]\n",
    "\n",
    "x= []\n",
    "y = []\n",
    "\n",
    "for i in range (len ( alpha)) : \n",
    "    ridge = Ridge(alpha[i])\n",
    "    x.append (alpha [i])\n",
    "    y.append (rmse_cv ( ridge, X_train,y_train  ))\n",
    "\n",
    "    \n",
    "plt.plot (x,y)\n",
    "plt.xlabel ('alpha')\n",
    "plt.ylabel ('rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À partir du graphique affiché, créer un modèle de régression Ridge performant\n",
    "Tester les performances de ce modèle sur X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insérez votre code ici\n",
    "# le alpha avec le rmse le plus petit 10\n",
    "ridge = Ridge(alpha = 10)\n",
    "ridge.fit ( X_train,y_train )\n",
    "ridge.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un modèle de régression Lasso qui choisira le paramètre  αα  parmi [10, 1, 0.1, 0.001, 0.0005] par validation croisée, grâce à la fonction LassoCV\n",
    "Afficher les performances du modèle sur l'échantillon de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso = LassoCV(cv=10, alphas = [10, 1, 0.1, 0.001, 0.0005] )\n",
    "model_lasso.fit(X_train, y_train)\n",
    "model_lasso.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À partir des coefficients retournés par le modèle de régression Lasso, afficher le nombre de variables gardés et le nombre de variables éliminés par le modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coef = model_lasso.coef_\n",
    "\n",
    "plt.plot(range(len(feats.columns)), lasso_coef)\n",
    "plt.xticks(range(len(feats.columns)), feats.columns.values, rotation=70)\n",
    "plt.ysize ( )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher les variables les plus importantes du modèle, relativement aux coefficients calculés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame ({'coef': model_lasso.coef_}, index = feats.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
