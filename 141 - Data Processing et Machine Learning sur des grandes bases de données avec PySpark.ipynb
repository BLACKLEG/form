{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------- Module 1\n",
    "#---------------------------------------------------------------------- Introduction à PySpark\n",
    "\n",
    "\n",
    "#------------------------------------------------------ Import de SparkContext du module pyspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Définiton d'un SparkContext en local\n",
    "sc = SparkContext('local')\n",
    "sc\n",
    "\n",
    "#------------------------------------------------------ time \n",
    "# Importer la bibliothèque time et calcul du temps au début de l'exécution (t0)\n",
    "from time import time\n",
    "t0 = time()\n",
    "\n",
    "### Insérez votre code d'ici \n",
    "raw_rdd = sc.textFile('2008_raw.csv')\n",
    "\n",
    "### Ne modifier pas le code ci-dessous\n",
    "# Calcul du temps de la lecture du fichier\n",
    "t1 = time() - t0\n",
    "print(\"Réalisé en {} secondes\".format(round(t1,3)))\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------ take (prend ) \n",
    "# Calcul du temps au début de l'exécution (t0)\n",
    "t0 = time()\n",
    "\n",
    "list_a = []\n",
    "\n",
    "### Insérez votre code d'ici \n",
    "for i in raw_rdd.take(5):\n",
    "    list_a.append(i)\n",
    "    print (i)\n",
    "    \n",
    "### Ne modifier pas le code ci-dessous\n",
    "# Calcul du temps de l'affichage des 5 éléments\n",
    "t1 = time() - t0\n",
    "print(\"Réalisé en {} secondes\".format(round(t1,3)))\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------ count ( compte ) \n",
    "# Calcul du temps au début de l'exécution (t0)\n",
    "t0 = time()\n",
    "\n",
    "## Insérez le code ici\n",
    "count = raw_rdd.count()\n",
    "\n",
    "### Ne modifier pas le code ci-dessous\n",
    "# Calcul du temps de l'affichage du nombre de lignes du RDD\n",
    "t1 = time() - t0\n",
    "print(\"Nombre de lignes :\", count)\n",
    "print(\"Réalisé en {} secondes\".format(round(t1,3)))\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------ Map \n",
    "airplane_rdd = raw_rdd.map(lambda line : line.split(','))\n",
    "\n",
    "print (airplane_rdd.take (1))\n",
    "airplane_rdd.take (1)\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------ reduceByKey \n",
    "# Création d'un nouveau rdd en résumant les lignes par l'aéroport de départ\n",
    "hist_rdd = airplane_rdd.map(lambda x: (x[7], 1)).reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "# Affichage d'un 5 premières lignes \n",
    "hist_rdd.take(20)\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------ collect pour forcer l'évaluation \n",
    "hist  = hist_rdd.collect()\n",
    "hist \n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------ Trier et filtrer un RDD\n",
    "sorted(hist, key= lambda x: x[1], reverse= 1)\n",
    "\n",
    "\n",
    "\n",
    "# Calcul et affichage du nombre de vols annulés par ville d'origine\n",
    "airplane_rdd \\\n",
    "    .filter(lambda x: x[10] == \"1\") \\\n",
    "    .map(lambda x: (x[8], 1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .collect()\n",
    "\n",
    "#------------------------------------------------------ Fermeture du SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0ce37ebef6e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#--------------------------------------------- Init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Import de SparkContext du module pyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Défintion d'un SparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "---------------------------------------------------------------------- Module 2\n",
    "# ---------------------------------------------------------------------- Data Processing \n",
    "\n",
    "\n",
    "#--------------------------------------------- Init\n",
    "# Import de SparkContext du module pyspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Défintion d'un SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "\n",
    "#--------------------------------------------- Importation de la base de données\n",
    "# Chargement du fichier \"miserables_full.txt\" et affichage des 10 premières lignes\n",
    "miserables = sc.textFile(\"miserables_full.txt\")\n",
    "miserables.take(10)\n",
    "\n",
    "\n",
    "#--------------------------------------------- Mise en forme de la base\n",
    "miserables_clean = rdd.map(lambda x : x.lower().replace(',',' ').replace('.',' '))\n",
    "\n",
    "\n",
    "#--------------------------------------------- Separe les mots\n",
    "# Création d'un rdd séparant les mots\n",
    "miserables_flat = miserables_clean.flatMap(lambda line: line.split(\" \"))\n",
    "miserables_flat.take(10)\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- Map/Reduce\n",
    "# Insérez votre code ici \n",
    "mots = miserables_flat.map(lambda x : (x,1) )\\\n",
    "                      .reduceByKey (lambda x,y : x +y )\n",
    "\n",
    "\n",
    "#--------------------------------------------- Compter occurence\n",
    "\n",
    "#Pour compter le nombre d'occurrences d'un élément, une très bonne technique consiste à :\n",
    "\n",
    "#- utiliser la méthode map pour créer un couple clé/valeur où chaque mot est une clé, \n",
    "            #chaque valeur vaut 1\n",
    "#- utiliser reduceByKey pour additionner les valeurs pour chaque mot\n",
    "\n",
    "### Première méthode de tri\n",
    "# tri en utilisant la fonction 'sorted' des rdd\n",
    "mots_sorted  = sorted(mots.collect(),\n",
    "                     key= lambda x: x[1],\n",
    "                     reverse= 0)\n",
    "\n",
    "### Deuxième méthode de tri\n",
    "# tri en utilisant la fonction 'sortBy' des rdd puis convertir en liste en utilisant collect\n",
    "mots_sorted_2 = mots.sortBy(lambda couple: couple[1], ascending = True) \\\n",
    "                    .collect()\n",
    "\n",
    "#--------------------------------------------- Succession de méthodes\n",
    "#Création directe d'une liste contenant les mots\n",
    "mots_sorted_3 = sc.textFile(\"miserables_full.txt\") \\\n",
    "                  .map(lambda x : x.lower().replace(',', ' ').replace('.', ' ').replace('-', ' ').replace('’', ' ')) \\\n",
    "                  .flatMap(lambda line: line.split(\" \")) \\\n",
    "                  .map(lambda x : (x,1)) \\\n",
    "                  .reduceByKey(lambda x,y : x + y) \\\n",
    "                  .sortBy(lambda couple: couple[1], ascending = True) \\\n",
    "                  .collect()\n",
    "                \n",
    "mots_sorted_3\n",
    "\n",
    "#---------------------------------------------# Fermeture du SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b362cef6b30c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Import de Spark Session et SparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------- Module 3\n",
    "# ---------------------------------------------------------------------- Les DataFrames\n",
    "\n",
    "\n",
    "#--------------------------------------------- Spark SQL\n",
    "\n",
    "# Import de Spark Session et SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Définition d'un SparkContext\n",
    "SparkContext.getOrCreate() \n",
    "\n",
    "# Définition d'une SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Introduction au DataFrame\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark\n",
    "\n",
    "#--------------------------------------------- \n",
    "# Création d'un raccourci vers le SparkContext déjà créé\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc\n",
    "\n",
    "\n",
    "#--------------------------------------------- Créer un Spark DataFrame\n",
    "# Import de Row du package pyspark.sql\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Chargement du fichier '2008_raw.csv'\n",
    "rdd = sc.textFile('2008_raw.csv').map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Création d'un nouveau rdd en sélectionnant les variables explicatives\n",
    "rdd_row = rdd.map(lambda line: Row(annee = line[0],\n",
    "                                   mois = line[1],\n",
    "                                   jours = line[2],\n",
    "                                   flightNum = line[5]))\n",
    "\n",
    "# Créer d'un data frame à partir d'un rdd\n",
    "df = spark.createDataFrame(rdd_row)\n",
    "\n",
    "\n",
    "#--------------------------------------------- afficher les 5 premiere lignes\n",
    "# Affichage des 5 premières lignes\n",
    "df.show(5)\n",
    "\n",
    "\n",
    "#--------------------------------------------- Créer Un DataFrame à partir d'un CSV\n",
    "# Lecture du fichier '2008.csv'\n",
    "raw_df = spark.read.csv('2008.csv', header=True)\n",
    "\n",
    "#--------------------------------------------- \n",
    "# Affichage du schéma des variables\n",
    "raw_df.printSchema()\n",
    "\n",
    "\n",
    "#--------------------------------------------- Explorer et manipuler un DataFrame\n",
    "# Création d'un data frame ne contenant que les variables explicatives\n",
    "flights1 = raw_df.select('annee', 'mois', 'jours', 'flightNum', 'origin', 'dest', 'distance', 'canceled', 'cancellationCode', 'carrierDelay')\n",
    "\n",
    "# Affichage de 20 premières lignes\n",
    "flights1.show() # 'show' affiche 20 lignes par défaut\n",
    "\n",
    "\n",
    "#--------------------------------------------- changer le type des var\n",
    "# Création d'un data frame en spécifiant le type des colonnes\n",
    "flights = raw_df.select(raw_df.annee.cast(\"int\"),   #la col annee passe en int\n",
    "                        raw_df.mois.cast(\"int\"),\n",
    "                        raw_df.jours.cast(\"int\"),\n",
    "                        raw_df.flightNum.cast(\"int\"),\n",
    "                        raw_df.origin.cast(\"string\"),\n",
    "                        raw_df.dest.cast(\"string\"),\n",
    "                        raw_df.distance.cast(\"int\"),\n",
    "                        raw_df.canceled.cast(\"boolean\"),\n",
    "                        raw_df.cancellationCode.cast(\"string\"),\n",
    "                        raw_df.carrierDelay.cast(\"int\"))\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- Compter (prend an compte les doubllons)\n",
    "flights.select('flightNum').distinct().count() #distinct prend en compte les doublons\n",
    "\n",
    "\n",
    "#--------------------------------------------- describe \n",
    "# Affichage d'un résumé en utilisant l'option truncate de la méthode show\n",
    "flights.describe().show(truncate = 8)\n",
    "\n",
    "### Deuxième méthode\n",
    "# Affichage d'un résumé en utilisant la méthode toPandas\n",
    "flights.describe().toPandas()\n",
    "\n",
    "#--------------------------------------------- Groupby\n",
    "# Affichage du résumé de la variable catégorielle 'cancellationCode'\n",
    "flights.groupBy('cancellationCode').count().show()\n",
    "\n",
    "# Affichage du résumé de la variable catégorielle 'cancellationCode' et 'canceled'\n",
    "flights.groupBy('cancellationCode', 'canceled').count().show()\n",
    "\n",
    "\n",
    "#--------------------------------------------- filtrer sur conditions\n",
    "\n",
    "# Affichage des 20 premièrs vols annulés pour la raison \"C\"\n",
    "flights.filter(flights.cancellationCode == 'C').show()\n",
    "\n",
    "\n",
    "#--------------------------------------------- Filtrer et grouper les données\n",
    "# Calcul du nombre vols annulés par mois\n",
    "flights.filter(flights.canceled == True).groupBy('mois').count().show()\n",
    "\n",
    "\n",
    "# On remarque que le mois de Décembre décompte beaucoup plus d'annulations que les autres mois,\n",
    "# cela peut être lié à une planification anticipée plus grande des vacances de Noël.\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- Création et aggrégation de variables\n",
    "# Création d'une nouvelle variable 'isLongFlight' et affichage des 10 premières lignes\n",
    "flights.withColumn('isLongFlight', flights.distance > 1000 ).show(10)\n",
    "\n",
    "\n",
    "#--------------------------------------------- Gestion des valeurs manquantes\n",
    "# Remplacement des valeurs manquantes par des 0 et affichage des 6 premières lignes\n",
    "flights = flights.fillna(0, 'carrierDelay').show(6)\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- Remplacer des val\n",
    "\n",
    "flights.replace ( ['A','B','C' ],['1', '2' , '3'], 'cancellationCode' ).show()\n",
    "\n",
    "\n",
    "#---------------------------------------------  trier\n",
    "# Ordonner le data frame par numéro de vol décroissant\n",
    "flights = flights.orderBy(flights.flightNum.desc()).show()\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- Requêtes SQL \n",
    "# Création d'une vue SQL\n",
    "flights.createOrReplaceTempView(\"flightsView\")\n",
    "\n",
    "# Création d'un data frame ne contenant que la variable \"flightsView\"\n",
    "sqlDF = spark.sql(\"SELECT carrierDelay FROM flightsView\")\n",
    "\n",
    "# Affichage des 10 premières lignes\n",
    "sqlDF.show(10)\n",
    "\n",
    "\n",
    "#--------------------------------------------- Sample & Astuce d'Affichage\n",
    "# Affichage d'un dizaine de lignes de la base de données\n",
    "flights.sample(False, .0001, seed = 222).toPandas()\n",
    "\n",
    "#withRemplacement : un booléen à spécifier False si l'on ne veut pas écraser le DataFrame\n",
    "#fraction : la fraction des données à conserver\n",
    "#seed : un entier quelconque qui permet de reproduire les résultats\n",
    "\n",
    "#toPandas = show\n",
    "#--------------------------------------------- Fermer la session spark \n",
    "\n",
    "# Fermeture de la session Spark\n",
    "spark.stop() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e106ea7ec783>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#--------------------------------------------- Introduction à Spark ML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Import de SparkSession et SparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------- Module 4\n",
    "# ---------------------------------------------------------------------- Régression avec PySpark\n",
    "\n",
    "\n",
    "#--------------------------------------------- Introduction à Spark ML\n",
    "# Import de SparkSession et SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Définition d'un SparkContext en local\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Construction d'une Session Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Introduction à Spark ML\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark\n",
    "\n",
    "#--------------------------------------------- Importation de la base de données\n",
    "# Chargement du fichier \" YearPredictionMSD.txt\" dans un data frame\n",
    "df_raw = spark.read.csv('YearPredictionMSD.txt')\n",
    "\n",
    "# Première méthode d'affichage \n",
    "df_raw.show(2, truncate = 4)\n",
    "# En modifiant les valeurs de 'truncate', cette méthode ne permet pas de bien visualiser les données\n",
    "# en vertu du nombre de variables\n",
    "\n",
    "# Deuxième méthode d'affichage\n",
    "df_raw.sample(False, .00001, seed = 222).toPandas()\n",
    "# En utilisant toPandas permet de mieux visualiser les données\n",
    "\n",
    "\n",
    "#--------------------------------------------- \n",
    "# Import de col du package pyspark.sql.functions\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convertir des colonnes relatives au timbre en double et l'année en int\n",
    "exprs = [col(c).cast(\"double\") for c in df_raw.columns[1:91]]\n",
    "df = df_raw.select(df_raw._c0.cast('int'), *exprs)\n",
    "\n",
    "# affichage du schéma des variables \"df\"\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "#---------------------------------------------\n",
    "# Affichage d'un résumé descriptif des données\n",
    "df.describe().toPandas()\n",
    "\n",
    "\n",
    "#--------------------------------------------- Mise en forme de la base en format svmlib\n",
    "# Import de DenseVector du package pyspark.ml.linalg\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Création d'un rdd en séparant la variable à expliquer des features\n",
    "rdd_ml = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Création d'un data frame composé de deux variables : label et features\n",
    "df_ml = spark.createDataFrame(rdd_ml, ['label', 'features'])\n",
    "\n",
    "# Affichage des 10 premières lignes du data frame\n",
    "df_ml.show(10)\n",
    "\n",
    "\n",
    "#--------------------------------------------- \n",
    "# Décomposition des données en deux ensembles d'entraînement et de test\n",
    "# par défaut l'échantillon est aléatoirement réparti\n",
    "train, test = df_ml.randomSplit([.8, .2], seed= 1234)\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- Régession Linéaire\n",
    "# Import de LinearRegression du package pyspark.ml.regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Création d'une fonction de régression linéaire\n",
    "lr = LinearRegression(labelCol='label', featuresCol= 'features')\n",
    "\n",
    "# Apprentissage des données d'entraînement : \"train\"\n",
    "linearModel = lr.fit(train)\n",
    "\n",
    "\n",
    "#--------------------------------------------- \n",
    "# Calcul des prédictions des données de test\n",
    "predicted = linearModel.transform(test)\n",
    "\n",
    "# Affichage des prédictions\n",
    "predicted.show()\n",
    "\n",
    "\n",
    "#--------------------------------------------- Evaluation du modèle\n",
    "# Calcul et affichage du RMSE\n",
    "print(\"RMSE:\", linearModel.summary.rootMeanSquaredError)\n",
    "\n",
    "# Calcul et affichage du R2\n",
    "print(\"R2:  \", linearModel.summary.r2)\n",
    "\n",
    "\n",
    "#--------------------------------------------- \n",
    "from pprint import pprint\n",
    "\n",
    "# Affichage des Coefficients du modèle linéaire\n",
    "pprint(linearModel.coefficients)\n",
    "\n",
    "# Fermeture de la session Spark \n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allez plus loin — Autres algorthimes de régression\n",
    "\n",
    "Maintenant que vous avez appris à programmer une régression linéaire en utilisant Spark ML, vous n'êtes qu'à quelques pas de maîtriser tout algorithme de régression distribué sous Spark. Pour vous aider à retenir l'essentiel, en voici un aperçu :\n",
    "1. Transformer la base de données en format svmlib:   • Sélectionner les variables numériques à utiliser pour la régression\n",
    "  • Placer la variable à expliquer en première position\n",
    "  • Mapper un couple (label, vecteur de features) dans un RDD\n",
    "  • Convertisser ce RDD en DataFrame et nommer les variables 'label' et 'features'\n",
    "2. Séparez la base de données en deux échantillon train et test\n",
    "3. Appliquez un modèle de classification\n",
    "\n",
    "4. Evaluez le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark est en constante amélioration et possède aujourd'hui quelques régresseurs notables. Ils sont utilisables de la même façon en important ces fonction depuis pyspark.ml.regression. Vous êtes invité à consulter la documentation pour observer les différents paramètres à prendre en compte pour optimiser ces algorithmes :\n",
    "\n",
    "  • LinearRegression() pour effectuer une régression linéaire lorsque le label est présupposé suivre une loi normale\n",
    "  \n",
    "  • GeneralizedLinearRegression() pour effectuer une régression linéaire généralisée lorsque le label est présupposé suivre une autre loi que l'on spécifie dans le paramètre family (gaussian, binomial, poisson, gamma)\n",
    "  \n",
    "  • AFTSurvivalRegression() pour effectuer une analyse de survie\n",
    "\n",
    "Il est également possible d'utiliser les algorithmes, qui gèrent également les variables catégorielles, détaillés dans l'exercice suivant :\n",
    "\n",
    "  • DecisionTreeRegressor() pour un arbre de décision\n",
    "  \n",
    "  • RandomForestRegressor() pour une forêt aléatoire d'arbres de décision\n",
    "  \n",
    "  • GBTRegressor() pour une forêt d'arbres gradient-boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------- Module 5\n",
    "# ------------------------------------------------- Utilisation des ML Pipelines \n",
    "\n",
    "# Les ML Pipelines permettent de faire enchaîner une succession d'estimateurs \n",
    "# ou de transformateurs # de façon à définir un processus de Machine Learning.\n",
    "\n",
    "#--------------------------------------------- \n",
    "# Import de SparkSession et SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Définition d'un SparkContext en local\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Construction d'une Session Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pipelines Spark ML\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark\n",
    "\n",
    "\n",
    "#--------------------------------------------- Les variables catégorielles\n",
    "# Chargement du fichier 'HR_comma_sep.csv'\n",
    "hr = spark.read.csv('HR_comma_sep.csv', header = True)\n",
    "\n",
    "# Affichage d'un extrait du data frame\n",
    "hr.sample(False, 0.001 , seed = 222).toPandas()\n",
    "#--------------------------------------------- \n",
    "# Ordonner les variables de telle sorte d'avoir le label en première colonne\n",
    "hr = hr.select( 'left',\n",
    "               'satisfaction_level',\n",
    "               'last_evaluation',\n",
    "               'number_project',\n",
    "               'average_montly_hours',\n",
    "               'time_spend_company',\n",
    "               'Work_accident',\n",
    "               'promotion_last_5years',\n",
    "               'sales',\n",
    "               'salary')\n",
    "\n",
    "# Affichage d'un description des variables\n",
    "hr.describe().toPandas()\n",
    "\n",
    "#Dans l exo precedent, l application de l algo ne c est pas faite sur les var \n",
    "#categorielles (car cela cree une value error car non convertible)\n",
    "\n",
    "#--------------------------------------------- \n",
    "# Import de StringIndexer du package pyspark.ml.feature\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Création d'un indexeur transformant une variable sales en indexedSales\n",
    "salesIndexer = StringIndexer(inputCol='sales', outputCol='indexedSales').fit(hr)\n",
    "\n",
    "# Création d'un DataFrame hrSalesIndexed indexant la variable sales\n",
    "hrSalesIndexed = salesIndexer.transform(hr)\n",
    "\n",
    "# Affichage d'un extrait du data frame hrSalesIndexed \n",
    "hrSalesIndexed.sample(False, 0.001 , seed = 222).toPandas()\n",
    "\n",
    "\n",
    "#--------------------------------------------- \n",
    "# Import de IndexToString du package pyspark.ml.feature\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "# Création d'une nouvelle colonne salesReconstructed\n",
    "SalesReconstructor = IndexToString(inputCol='indexedSales',\n",
    "                                   outputCol='salesReconstructed',\n",
    "                                   labels = salesIndexer.labels)\n",
    "\n",
    "# Appliquer le transformateur SalesReconstructor\n",
    "hrSalesReconstructed = SalesReconstructor.transform(hrSalesIndexed)\n",
    "\n",
    "# Affichage d'un extrait de la base de données\n",
    "hrSalesReconstructed.sample(False, 0.001 , seed = 222).toPandas()\n",
    "# On voit apparaître une nouvelle colonne 'salesReconstructed' égale à la colonne 'sales'\n",
    "\n",
    "\n",
    "#--------------------------------------------- Les Pipelines\n",
    "# Import de Pipeline du package pyspark.ml\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Création des indexeurs\n",
    "SalesIndexer = StringIndexer(inputCol='sales', outputCol='indexedSales')\n",
    "SalaryIndexer = StringIndexer(inputCol='salary', outputCol='indexedSalary')\n",
    "\n",
    "# Création d'un pipeline\n",
    "indexer = Pipeline(stages =  [SalaryIndexer, SalesIndexer])\n",
    "\n",
    "# Indexer les variables de \"hr\"\n",
    "hrIndexed = indexer.fit(hr).transform(hr)\n",
    "\n",
    "# Affichage d'un extrait\n",
    "hrIndexed.sample(False, 0.001 , seed = 222).toPandas()\n",
    "\n",
    "\n",
    "#--------------------------------------------- Mise en forme de la base en format svmlib\n",
    "# Import de DenseVector du package pyspark.ml.linalg\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Création d'une base de données excluant les variables non indexées\n",
    "hrNumeric = hrIndexed.select('left',\n",
    "                             'satisfaction_level',\n",
    "                             'last_evaluation',\n",
    "                             'number_project',\n",
    "                             'average_montly_hours',\n",
    "                             'time_spend_company',\n",
    "                             'Work_accident',\n",
    "                             'promotion_last_5years',\n",
    "                             'indexedSales',\n",
    "                             'indexedSalary')\n",
    "\n",
    "# Création d'une variable DenseVector contenant les features en passant par la structure RDD\n",
    "hrRdd = hrNumeric.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# transformation en DataFrame et nommer les variables pour obtenir une base de la forme libsvm\n",
    "hrLibsvm = spark.createDataFrame(hrRdd, ['label', 'features'])\n",
    "\n",
    "# Affichage d'un extrait\n",
    "hrLibsvm.sample(False, .001, seed = 222).toPandas()\n",
    "\n",
    "\n",
    "#--------------------------------------------- Application d'un classifieur Spark ML\n",
    "hrNumeric.describe().toPandas()\n",
    "\n",
    "# Nous avons 2 variables catégorielles: 'sales' et 'salary'\n",
    "# On peut obtenir le nombre de modalités en regardant 'min' et 'max' des variables indexées:\n",
    "# 'sales' possède le plus de modalités et en possède 10 (10 entiers entre 0.0 et 9.0)\n",
    "\n",
    "\n",
    "#--------------------------------------------- \n",
    "# Import de VectorIndexer du package pyspark.ml.feature\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "# Création d'un transformateur indexant les features\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\",\n",
    "                               outputCol=\"indexedFeatures\",\n",
    "                               maxCategories = 10).fit(hrLibsvm)\n",
    "\n",
    "\n",
    "#--------------------------------------------- \n",
    "# Import du classificateur RandomForestClassifier du package pyspark.ml.classification\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Création des transformateurs\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(hrLibsvm)\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories = 10).fit(hrLibsvm)\n",
    "\n",
    "# Création d'un classificateur \n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", predictionCol='prediction', seed = 222)\n",
    "\n",
    "# Création d'un transformateur permettant de rétablir les labels des prédictions\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Création d'un Pipeline \n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Décomposition des données en deux ensemble : données d'entraînement  et de test\n",
    "(train, test) = hrLibsvm.randomSplit([0.7, 0.3], seed = 222)\n",
    "\n",
    "# Apprentissage du modèle en utilisant les données d'entraînement\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "\n",
    "#--------------------------------------------- \n",
    "#Calcul des prédictions \n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Affichage d'un extrait des prédictions \n",
    "predictions.sample(False, 0.001 , seed = 222).toPandas()\n",
    "\n",
    "\n",
    "#--------------------------------------------- Evaluation du modèle\n",
    "# Import d'un évaluateur MulticlassClassificationEvaluator du package pyspark.ml.evaluation\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Création d'un évaluateur \n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy',\n",
    "                                              labelCol= 'indexedLabel',\n",
    "                                              predictionCol= 'prediction')\n",
    "# Calcul et affichage de la précision du modèle \n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La métrique accuracy correspond au nombre de prédictions correctes divisées par le nombre de prédictions effectuées. Elle est donc entre 0 et 1 ; une accuracy de 0 correspond à des prédictions toutes fausses et une accuracy de 1 correspond à l'absence d'erreurs dans la prédiction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allez plus loin — Autres algorithmes de classification\n",
    "\n",
    "Vous avez maintenant en main tous les outils pour effectuer tout type de classification en Spark ML. Pour résumer de façon concise, une classification s'effectue de la façon suivante :\n",
    "\n",
    "1. Transformer toutes les variables en numérique\n",
    "2. Transformer la base en format svmlib\n",
    "3. Créer un pipeline contenant :\n",
    "  • La transformation de la variable label en catégorie\n",
    "  \n",
    "  • La transformation des features catégoriels\n",
    "  \n",
    "  • Un modèle de classification\n",
    "  \n",
    "  • Un transformateur inverse de l'indexation pour les prédictions créées\n",
    "4. Evaluer le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark est en constante amélioration est possède aujourd'hui quelques classifieurs notables que vous pouvez utiliser de la même façon en important ces fonction depuis pyspark.ml.classification. Vous êtes invités à consulter la documentation pour observer les différents paramètres à prendre en compte pour optimiser ces algorithmes :\n",
    "\n",
    "  • LogisticRegression() pour effectuer une régression linéaire https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression\n",
    "  \n",
    "  • DecisionTreeClassifier() pour un arbre de régression simple\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.DecisionTreeClassifier\n",
    "\n",
    "  • RandomForestClassifier() pour une forêt aléatoire\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.RandomForestClassifier\n",
    "\n",
    "  • GBTClassifier() pour une forêt d'arbres gradient-boosted\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.GBTClassifier\n",
    "\n",
    "  • LinearSVC() pour un SVM de régression à noyau linéaire\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LinearSVC\n",
    "\n",
    "  • NaiveBayes() pour une classification naïve bayesienne https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.NaiveBayes\n",
    "\n",
    "Pour une liste complète des algorithmes de Spark ML, vous pouvez également consutler la documentation générale de pyspark.ml ; la liste des algorithmes disponibles ne cesse de grandir au fur et à mesure du développement de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------- Module 6\n",
    "# ---------------------------------------------------------------- Model Tuning  \n",
    "\n",
    "# Il existe plusieurs méthodes de tuning qui permettent de régler les \n",
    "# paramètres d'un modèle, de façon à éviter le sur-apprentissage \n",
    "# (overfitting en anglais). \n",
    "#La plus utilisée est la validation croisée (cross validation en anglais).\n",
    "\n",
    "#--------------------------------------------- \n",
    "# Import de SparkSession et de SParkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "#Création d'un SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Création d'une session Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ML Tuning\") \\\n",
    "    .getOrCreate()\n",
    "        \n",
    "spark\n",
    "\n",
    "\n",
    "#--------------------------------------------- Importation de la base de données\n",
    "# Chargement de la base de données brute\n",
    "df_full = spark.read.csv('YearPredictionMSD.txt', header=False)\n",
    "\n",
    "# On infère les bons types des colonnes\n",
    "from pyspark.sql.functions import col\n",
    "exprs = [col(c).cast(\"double\") for c in df_full.columns[1:13]]\n",
    "\n",
    "df_casted = df_full.select(df_full._c0.cast('int'),\n",
    "                           *exprs)\n",
    "\n",
    "# Enfin, pour un soucis de rapidité des calculs,\n",
    "# on ne traitera qu'un extrait de la base de données dans cet exercice\n",
    "df = df_casted.sample(False, .1, seed = 222)\n",
    "\n",
    "df.sample(False, .001, seed = 222).toPandas()\n",
    "\n",
    "#--------------------------------------------- \n",
    "\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Conversion de la base de données au format svmlib\n",
    "rdd_ml = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "df_ml = spark.createDataFrame(rdd_ml, ['label', 'features'])\n",
    "\n",
    "df_ml.show()\n",
    "\n",
    "#--------------------------------------------- \n",
    "\n",
    "# Décomposition des données en deux ensembles d'entraînement et de test\n",
    "# par défaut l'échantillon est aléatoirement réparti\n",
    "train, test = df_ml.randomSplit([0.8, 0.2], seed=222)\n",
    "\n",
    "#--------------------------------------------- \n",
    "\n",
    "# Import de LinearRegression du package pyspark.ml.regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Création d'un estimateur : Régression Linéaire\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol = 'label')\n",
    "\n",
    "#--------------------------------------------- Création d'une grille de paramètres\n",
    "\n",
    "# Import de ParamGridBuilder du package pyspark.ml.tuning\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "# Création d'une grille de paramètres\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(lr.regParam, [0, 0.5, 1]).\\\n",
    "    addGrid(lr.elasticNetParam, [0, 0.5, 1]).\\\n",
    "    build()\n",
    "\n",
    "#--------------------------------------------- Choix d'une métrique d'évaluation\n",
    "\n",
    "# Import de RegressionEvaluator du package pyspark.ml.evaluation\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Création d'un évaluateur ayant pour métrique d'évaluation r2\n",
    "ev = RegressionEvaluator(predictionCol='prediction',\n",
    "                                labelCol='label',\n",
    "                                metricName='r2')\n",
    "\n",
    "#--------------------------------------------- Réglage des paramètres par validation croisée\n",
    "\n",
    "#Import de CrossValidator du package pyspark.ml.tuning\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "# Création d'un cross validator 3-fold\n",
    "cv = CrossValidator(estimator = lr,\n",
    "                    estimatorParamMaps = param_grid,\n",
    "                    evaluator=ev,\n",
    "                    numFolds=3)\n",
    "\n",
    "#--------------------------------------------- Application du modèle\n",
    "\n",
    "# Import de la bibliothèque time et calcul du temps au début de l'exécution (t0)\n",
    "from time import time\n",
    "t0 = time()\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol = 'label')\n",
    "ev = RegressionEvaluator(predictionCol='prediction', labelCol='label', metricName='r2')\n",
    "cv = CrossValidator(estimator = lr, estimatorParamMaps = param_grid, evaluator = ev, numFolds = 3)\n",
    "\n",
    "cv_model = cv.fit(train)\n",
    "\n",
    "tt = time() - t0\n",
    "print(\"Réalisé en {} secondes\".format(round(tt,3)))\n",
    "\n",
    "#--------------------------------------------- \n",
    "\n",
    "# Calcul des prédication des données d'entraînement\n",
    "pred_train = cv_model.transform(train)\n",
    "\n",
    "# Calcul des prédication des données de test\n",
    "pred_test  = cv_model.transform(test)\n",
    "\n",
    "#--------------------------------------------- RMSE\n",
    "\n",
    "ev.setMetricName('rmse').evaluate(pred_test)\n",
    "\n",
    "#--------------------------------------------- Exploitation des résultats\n",
    "\n",
    "# Affichage des coefficients du modèle\n",
    "cv_model.bestModel.coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examen ¶\n",
    "\n",
    "Traitement de données massives avec PySpark \n",
    "\n",
    "L'exercice est composé de plusieurs questions, faites-les dans l'ordre et faites attention à respecter le nom des variables. N'hésitez pas à contacter l'équipe DataScientest si vous rencontrez des problèmes sur help@datascientest.com\n",
    "\n",
    "Jeu de données\n",
    "Le jeu de données de cet exercice contient des transactions effectuées en septembre 2013 par des titulaires de cartes bancaires européennes et compte 492 transactions frauduleuses parmi 284 807 transactions. L'ensemble de données est très déséquilibré, les classes positives (fraudes) représentent 0,172% de toutes les transactions. Les variables explicatives sont anonymisées et représentées par les trente premières colonnes. La variable cible est stockée dans la dernière colonne.\n",
    "\n",
    "L'objet de cet exercice consiste à implémenter et entraîner un modèle de random forest avec PySpark pour faire de la détection de transaction frauduleuses.\n",
    "\n",
    "Principe des forêts aléatoires\n",
    "Les forêts aléatoires sont composées d'un ensemble d'arbres décisionnels. Ces arbres se distinguent les uns des autres par le sous-échantillon de données sur lequel ils sont entraînés. Ces sous-échantillons sont tirés au hasard dans le jeu de données initial.\n",
    "\n",
    "Le principe de fonctionnement des forêts aléatoire est simple : de nombreux petits arbres de classification sont produits sur une fraction aléatoire de données. Random Forest fait voter les arbres de classification afin de déduire l'ordre et l'importance des variables explicatives.\n",
    "\n",
    "Construire une session Spark nommée spark, sans utiliser findspark\n",
    "   Rappelez-vous qu'avant de regarder la solution, vous avez toujours accès à l'aide officielle de Python en tapant help(nom_fonction) dans la console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construire une session Spark nommée spark, sans utiliser findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Construction d'une Session Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Introduction à Spark ML\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importer le fichier creditcard.csv en tant que DataFrame appelé df_raw\n",
    "\n",
    "Afficher un extrait du DataFrame df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insérez votre code ici\n",
    "\n",
    "df_raw = spark.read.csv('creditcard.csv')\n",
    "df_raw.sample(False, 0.00001, seed = 1234 ).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un DataFrame df à partir de df_raw en changeant les colonnes des variables cibles en double et la variable cible, Class, en int\n",
    "\n",
    "Afficher le schéma des variables de df\n",
    "\n",
    "   Une bonne pratique est de mettre la variable à prédire ou la variable cible dans la première colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "exprs = [col(c).cast(\"double\") for c in df_raw.columns[:30]]\n",
    "df = df_raw.select(df_raw._c30.cast('int'), *exprs)\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supprimer les lignes contenant des valeurs manquantes du DataFrame df\n",
    "\n",
    "Créer un rdd rdd_ml séparant la variable à expliquer des features (à mettre sous forme DenseVector)\n",
    "\n",
    "Créer un DataFrame df_ml contenant notre base de données sous deux variables : 'labels' et 'features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insérez votre code ici\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "rdd_ml = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "df_ml = spark.createDataFrame(rdd_ml, ['label', 'features'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer deux DataFrames appelés train et test contenant chacun respectivement 80% et 20% des données\n",
    "\n",
    "Créer un classificateur Random Forest appelé clf\n",
    "\n",
    "Apprendre le modèle des forêts aléatoires sur l'ensemble d'entraînement\n",
    "\n",
    "   L'apprentissage des données massives prend généralement un temps d'exécution long. La cellule ci-dessous doit s'exécuter en trois minutes dans le pire des cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "train, test = df_ml.randomSplit([.8, .2], seed= 1234)\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", predictionCol='prediction', seed = 222)\n",
    "\n",
    "model = rf.fit(train)\n",
    "\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculer la précision, accuracy, du modèle entraîné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insérez votre code ici\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Création d'un évaluateur \n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy',\n",
    "                                              labelCol= 'label',\n",
    "                                              predictionCol= 'prediction')\n",
    "# Calcul et affichage de la précision du modèle \n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
