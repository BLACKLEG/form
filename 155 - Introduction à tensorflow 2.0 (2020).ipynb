{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------- Cours TensorFlow \n",
    "# ------------------------------------------- Module 1: Introduction à TensorFlow \n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- cree des constantes \n",
    "import tensorflow as tf\n",
    "\n",
    "# Définition de la première constante valant 5\n",
    "x1 = tf.constant(5)\n",
    "\n",
    "# Définition de la deuxième constante valant 10\n",
    "x2 = tf.constant(10)\n",
    "\n",
    "print(x1) # Affiche la constante numéro 1\n",
    "\n",
    "print(x2) # Affiche la constante numéro 2\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Opérations (voir cell si dessous )\n",
    "\n",
    "# Appel de la fonction add pour additionner x1 et x2\n",
    "resultat = tf.add(x1, x2)\n",
    "# Equivalent :\n",
    "# resultat = x1 + x2\n",
    "\n",
    "# Affichage du résultat\n",
    "print(resultat)\n",
    "\n",
    "resultat.numpy()\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Problème 2 : Fonction  f(x) simple \n",
    "\n",
    "# Instanciation d'une Variable de valeur initiale 3.0 (donc de type flottant)\n",
    "x = tf.Variable(initial_value=3.0)\n",
    "\n",
    "def f(x):\n",
    "    # Création de \"two\" via un objet de type constant\n",
    "    two = tf.constant(2.0, dtype=tf.float32)\n",
    "    # Equivalent :\n",
    "    # two = 2.0\n",
    "\n",
    "    # Application d'un bloc with afin de définir une sous-boîte via la fonction name_scope appliquée à 'Function'\n",
    "    with tf.name_scope('Function'):\n",
    "        f_x = x*x - 2*x + two\n",
    "        \n",
    "    return f_x\n",
    "\n",
    "# Evaluation de f(3)\n",
    "f(x).numpy()\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- methode assign()\n",
    "#La méthode assign permet d'assigner une nouvelle valeur à la variable.\n",
    "\n",
    "x.assign(4.0)\n",
    "f(x).numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Calcul de gradient\n",
    "x.assign(4.0)\n",
    "# Compute f(4)\n",
    "with tf.GradientTape() as tape:\n",
    "    function = f(x)\n",
    "\n",
    "# Compute gradient of f(4)\n",
    "grad = tape.gradient(function, x)\n",
    "print(grad.numpy())\n",
    "\n",
    "\n",
    "#en calcul analytique  ∂f(x)/dx pour x = 4.\n",
    "\n",
    "# gradient_f = 2*x-2\n",
    "print('Gradient of f for x=4 :', 2*4-2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Problème de régression : Régression Linéaire\n",
    "\n",
    "#ci dessous un exemple d un modele lineaire\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import make_regression \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_regression(n_samples=100, \n",
    "                       n_features=1,\n",
    "                       n_informative=1,\n",
    "                       noise=10,\n",
    "                       random_state=0)\n",
    "\n",
    "X = np.squeeze(X)\n",
    "# plot data \n",
    "plt.scatter(X, y, color='blue', alpha=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Our dataset')\n",
    "# display plot\n",
    "print('Shape of X :', X.shape)\n",
    "print('Shape of y :', y.shape)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#----------------------------------------------affiche la droite de régression\n",
    "from interaction_intro import show_linear\n",
    "show_linear(X, y)\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------- MSE\n",
    "from interaction_intro import show_MSE\n",
    "show_MSE(X,y)\n",
    "\n",
    "\n",
    "# ---------------------------------------------- creer une classe lineaire \n",
    "class Linear():\n",
    "    def __init__(self):\n",
    "        self.w = tf.Variable(tf.random.normal([1]), name='weight')\n",
    "        self.b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        return inputs * self.w + self.b\n",
    "\n",
    "    \n",
    "\n",
    "# ----------------------------------------------Entraînement\n",
    "\n",
    "\n",
    "#graphe descente de gradient \n",
    "\n",
    "from interaction_intro import show_optimization\n",
    "# Loss function\n",
    "def f(w):\n",
    "    w = w/5\n",
    "    return w**4 + w**3 - 6*w**2 + 1 \n",
    "\n",
    "# Gardient of the loss function\n",
    "def f_gradient(w):\n",
    "    w = w/5\n",
    "    return 4*(w**3) + 3*(w)**2 - 12*w\n",
    "\n",
    "# Show the interaction\n",
    "show_optimization(f, f_gradient)\n",
    "\n",
    "\n",
    "# ----------------------------------------------  objet model de la classe Linear\n",
    "\n",
    "# Définition du modèle\n",
    "model = Linear()\n",
    "\n",
    "# Définition d'un optimisateur Adam\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "#definition d une fonction d entrainement \n",
    "def train_op(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Prédiction de notre modèle\n",
    "        y_pred = model(inputs)\n",
    "        # Calcule de l'erreur de notre modèle\n",
    "        loss_value = tf.keras.losses.mean_squared_error(targets, y_pred)\n",
    "    # Calculer le gradient de la fonction de perte\n",
    "    grads = tape.gradient(loss_value, [model.w, model.b])\n",
    "    # Descente de gradient\n",
    "    optimizer.apply_gradients(zip(grads, [model.w, model.b]))\n",
    "    # Retourner la valeur de la fonction de perte\n",
    "    return loss_value.numpy()\n",
    "\n",
    "steps = 1000\n",
    "# Entraînenement du modèle\n",
    "grads = [train_op(model, X, y) for i in range(steps)]\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------Évalutation\n",
    "# Prédiction de notre modèle\n",
    "y_pred = model(X)\n",
    "\n",
    "# Afficher l'évolution de la fonction de perte\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(grads)\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Valeur de la fonction de perte')\n",
    "\n",
    "plt.subplot(122)\n",
    "# Afficher les points (x, y).\n",
    "plt.scatter(X, y, alpha=0.5, label = 'True value')\n",
    "\n",
    "# Afficher la prédiction de X.\n",
    "plt.plot(X, y_pred, 'r', label = 'Prediction')\n",
    "plt.xlabel('x')\n",
    "plt.xlabel('y')\n",
    "plt.title('Prédiction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------Modèle Polynomial : Lien entre Tensorflow et Keras\n",
    "\n",
    "from interaction_intro import show_polynomial\n",
    "show_polynomial(X, y)\n",
    "\n",
    "\n",
    "# ----------------------------------------------Créer une classe sous le nom Polynomial\n",
    "\n",
    "class Polynomial(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        # Initialisation de tf.keras.Model\n",
    "        super(Polynomial, self).__init__()\n",
    "        # Instantier la variable w0.\n",
    "        self.w0 = tf.Variable(tf.random.normal([1]), name='w0')\n",
    "        \n",
    "        # Instantier la variable w1.\n",
    "        self.w1 = tf.Variable(tf.random.normal([1]), name='w1')\n",
    "        \n",
    "        # Instantier la variable w2.\n",
    "        self.w2 = tf.Variable(tf.random.normal([1]), name='w2')\n",
    "        \n",
    "        # Instantier la variable b.\n",
    "        self.b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "        \n",
    "    def __call__(self, inputs, training=True):\n",
    "        # Prédiction de notre modèle.\n",
    "        return self.b + self.w0*inputs + self.w1*inputs**2 + self.w2*inputs**3\n",
    "    \n",
    "    \n",
    "    \n",
    "# ---------------------------------------------- utiliser un modele de la plasse polynomial\n",
    "\n",
    "# Model definiton\n",
    "model = Polynomial()\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(1e-1)\n",
    "# Compilation\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "# Training\n",
    "training_history = model.fit(X, y, batch_size=16, epochs=100)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- graph évolution de loss en fonction de l'itéraction\n",
    "# Prédiction de notre modèle\n",
    "y_pred = model(np.linspace(-2.5, 2.5, 100))\n",
    "\n",
    "# Afficher l'évolution de la fonction de perte\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(training_history.history['loss'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Valeur de la fonction de perte')\n",
    "\n",
    "plt.subplot(122)\n",
    "# Afficher les points (x, y).\n",
    "plt.scatter(X, y, alpha=0.5, label = 'True value')\n",
    "# Afficher la prédiction de X.\n",
    "plt.plot(np.linspace(-2.5, 2.5, 100), y_pred, 'r', label = 'Prediction')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Prediction')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qu'il faut retenir : \n",
    "\n",
    "Tensor\n",
    "Le tensor est l'unité centrale de données dans tensorflow. C'est comme un tableau numpy, nous pouvons le concevoir comme l'abstraction à n dimensions des matrices.\n",
    "\n",
    "Opération\n",
    "Les tenseurs sont transmis aux opérations qui effectuent des calculs sur eux. Les opérations peuvent prendre plusieurs tenseurs en entrée, effectuent des opérations (addition, ...) et peuvent retourner plusieurs tenseurs. Quelques fonctions faisant le parallèle entre les opérateurs numpy et tensorflow :\n",
    "\n",
    "Fonction Numpy\tFonction Tensorflow\n",
    "np.matmul\ttf.matmul\n",
    "np.transpose\ttf.transpose\n",
    "np.linalg.inv\ttf.linalg.inv\n",
    "np.cos\ttf.cos\n",
    "np.sin\ttf.sin\n",
    "np.tan\ttf.tan\n",
    "np.log\ttf.log\n",
    "np.exp\ttf.exp\n",
    "np.reshape\ttf.reshape\n",
    "np.concatenate\ttf.concat\n",
    "np.expand_dims\ttf.expand_dims\n",
    "np.arange\ttf.range\n",
    "Graphe\n",
    "Les tensors et les opérations sont reliés entre eux dans un graphe de calcul. Un graphe de calcul est défini en considérant les opérations comme des nœuds et les tenseurs comme des arêtes.\n",
    "\n",
    "Eager Execution :\n",
    "Une des grandes particularités de la version 2.0+ de tensorflow est la mise en place du \"eager execution\". Le \"Eager execution\" ou l'exécution rapide consiste à évaluer les opérations immédiatement. Les opérations renvoient des valeurs concrètes au lieu de créer un graphique de calcul à exécuter ultérieurement.\n",
    "\n",
    "Gradient / Optimisation\n",
    "Tensorflow permet de calculer les propriétés d'une fonction comme sa primitive ou sa dérivée. Il rend également possible la propagation d'une erreur de la sortie du graphique vers l'entrée (backpropagation). Cette propriété est la base des méthodes d'optimisations en Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fonction Numpy\tFonction Tensorflow\n",
    "np.add or +\ttf.add or +\n",
    "\n",
    "np.subtract or -\ttf.subtract or -\n",
    "\n",
    "np.matmul\ttf.matmul\n",
    "\n",
    "np.transpose\ttf.transpose\n",
    "\n",
    "np.linalg.inv\ttf.linalg.inv\n",
    "\n",
    "np.cos\ttf.cos\n",
    "\n",
    "np.sin\ttf.sin\n",
    "\n",
    "np.tan\ttf.tan\n",
    "\n",
    "np.log\ttf.log\n",
    "\n",
    "np.exp\ttf.exp\n",
    "\n",
    "np.reshape\ttf.reshape\n",
    "\n",
    "np.concatenate\ttf.concat\n",
    "\n",
    "np.expand_dims\ttf.expand_dims\n",
    "\n",
    "np.arange\ttf.range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------- Cours TensorFlow \n",
    "# ------------------------------------------- Module 2: Relation entre Tensorflow et Keras\n",
    "\n",
    "\n",
    "# ----------------------------------------------charger et visualiser le jeu de données MNIST\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "\n",
    "# Show 10 randoms numbers.\n",
    "j = 1\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i in np.random.randint(low=0, high=len(X_train), size=[10]):\n",
    "    plt.subplot(2, 5, j)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    plt.title('Number ' + str(y_train[i]))\n",
    "    j+=1\n",
    "\n",
    "# Reshape\n",
    "X_train = X_train.reshape([-1, 28*28])\n",
    "X_test = X_test.reshape([-1, 28*28])\n",
    "\n",
    "# Shape of X_train and y_train\n",
    "print('Shape of X:', X_train.shape)\n",
    "print('Shape of y:',y_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Tensorflow et Keras\n",
    "#tf construit autour le framework keras \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Modele + couches Denses\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Sequential\n",
    "model = tf.keras.Sequential()\n",
    "# Hidden layer 1\n",
    "model.add(Dense(128, activation='sigmoid', input_shape=[28*28]))\n",
    "# Outplut Layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# ---------------------------------------------- Compile\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# ----------------------------------------------fit \n",
    "\n",
    "history = model.fit(X_train, y_train,                        # Training dataset\n",
    "                        epochs = 20,                         # Number of epochs\n",
    "                        batch_size = 64,                     # Len of batch\n",
    "                        validation_data=(X_test, y_test))    # Validation dataset\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Courbe de la fonction de coût et de précision en fonction de l'epoch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy by epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Dataset avec Tensorflow\n",
    "\n",
    "# Change the shape and type\n",
    "X_train = tf.cast(tf.reshape(X_train, [-1,28,28,1]), tf.float32)\n",
    "X_test = tf.cast(tf.reshape(X_test, [-1,28,28,1]), tf.float32)\n",
    "\n",
    "# Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset = dataset.shuffle(20000).batch(64)\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "for (X_b, y_b) in dataset.take(5):\n",
    "    print('Target elements of the batch :', y_b.numpy(), '\\n')\n",
    "    \n",
    "    \n",
    "\n",
    "# ---------------------------------------------- les convolutions\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "\n",
    "lenet = tf.keras.Sequential()\n",
    "\n",
    "lenet.add(Conv2D(filters = 30,                   # Number of output filters\n",
    "                kernel_size = (5, 5),            # Kernel shape\n",
    "                input_shape = (28, 28, 1),       # Input shapeD\n",
    "                activation = 'relu'))            # Activation function\n",
    "\n",
    "lenet.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "lenet.add(Conv2D(filters = 16,                    \n",
    "                kernel_size = (3, 3),\n",
    "                activation = 'relu'))\n",
    "\n",
    "lenet.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "lenet.add(Flatten())\n",
    "\n",
    "lenet.add(Dropout(rate = 0.2))\n",
    "\n",
    "lenet.add(Dense(units = 128, activation = 'relu'))\n",
    "\n",
    "lenet.add(Dense(units = 10, activation = 'softmax'))\n",
    "\n",
    "lenet.summary()\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------Fonction d'entraînement\n",
    "\n",
    "# Define Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_op(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # model prediction\n",
    "        y_pred = model(inputs, training=True)\n",
    "        # compute the loss function\n",
    "        loss_value = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, y_pred))\n",
    "        \n",
    "    # Compute the gradient of loss function\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    # Gradient descent\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # Return the loss function value\n",
    "    return loss_value.numpy()\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- le temps d'entraînement.\n",
    "import time\n",
    "grads = []\n",
    "epochs = 10\n",
    "t0 = time.time()\n",
    "\n",
    "# Entraînenement du modèle\n",
    "for i in range(epochs): \n",
    "    # Pour chaque epoch\n",
    "    for X_t, y_t in dataset:\n",
    "        #Entraîner le modèle pour chaque batch\n",
    "        train_op(lenet, X_t, y_t)\n",
    "        \n",
    "    # Fonction de coût pour l'ensemble de validation\n",
    "    loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_test, lenet(X_test))).numpy()\n",
    "    print('Iteration', i, ' Loss', loss)\n",
    "    grads.append(loss)\n",
    "    \n",
    "print('Time :', time.time() - t0)\n",
    "\n",
    "# Or\n",
    "# lenet.compile('adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# lenet.fit(dataset, batch_size=64, epochs=10)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Afficher l'évolution de la fonction de perte\n",
    "plt.plot(grads)\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Valeur de la fonction de perte')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Évalutation\n",
    "\n",
    "# CPU device\n",
    "with tf.device('/CPU:0'):\n",
    "    # Probability prediction\n",
    "    y_prob = lenet(X_test)\n",
    "\n",
    "# Label prediction\n",
    "y_pred = tf.argmax(y_prob, axis=1).numpy()\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Accuracy :', accuracy_score(y_test, y_pred))\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ce qu'il faut retenir :\n",
    "\n",
    "\n",
    "Les réseaux de neurones organisés en couches sont des outils de machine learning désormais accessibles offrant des résultats pouvant surpasser de loin les algorithmes classiques sur des tâches complexes.\n",
    "\n",
    "Tensorflow une bibliothèque de référence pour effectuer du deep learning. La version de tensorflow 2.0+ a été construite autour du framework keras.\n",
    "\n",
    "\n",
    "Globalement, le schéma est le même que pour beaucoup d'algorithmes classiques:\n",
    "\n",
    "1 - Définir un Dataset pour mettre en forme les données et le partitionner en batchs.\n",
    "\n",
    "2 - Construire le modèle.\n",
    "\n",
    "3 - Entraîner le modèle, nous avons vue deux manières équivalentes de le faire:\n",
    "Méthode fit: problème simple.\n",
    "Calculer le gradient de la fonction de coût puis rétropropager l'erreur: problème complexe.\n",
    "\n",
    "4 - Prédiction du modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------- Cours TensorFlow \n",
    "# ------------------------------------------- Module 3: Word Embedding : Word2Vec \n",
    "\n",
    "\n",
    "# ----------------------------------------------One hot Encoding\n",
    "\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "# Dictionary\n",
    "dictionary = {'i': 0, 'think': 1, 'therefore': 2, 'am': 3}\n",
    "# One hot representation of \"think\"\n",
    "tf.one_hot(dictionary['think'], 4).numpy()\n",
    "\n",
    "# Distance : √2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------Word2Vec\n",
    "\n",
    "# ----------------------------------------------import file\n",
    "\n",
    "file = open(\"book.txt\", \"r\",encoding='utf-8') \n",
    "text = file.read()\n",
    "file.close()\n",
    "# ---------------------------------------------- Définition du dictionnaire \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Choice only useful words to create the dictionary :\n",
    "tokenizer = RegexpTokenizer(\"[a-zA-Zé]{2,}\")\n",
    "word_list = tokenizer.tokenize(text.lower())\n",
    "\n",
    "stop_word = stopwords.words('english')\n",
    "filtered_words = [word for word in word_list if word not in stop_word]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(filtered_words);\n",
    "\n",
    "# ---------------------------------------------- word2idx \n",
    "#Define the dictionary\n",
    "word2idx = vectorizer.vocabulary_\n",
    "idx2word = dict(zip(word2idx.values(),word2idx.keys()))\n",
    "vocab_size = len(idx2word)\n",
    "\n",
    "\n",
    "# ----------------------------------------------Model Skip-Gram\n",
    "\n",
    "#Run the program\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def sentenceToData(tokens,WINDOW_SIZE):\n",
    "    window = np.concatenate((np.arange(-WINDOW_SIZE,0),np.arange(1,WINDOW_SIZE+1)))\n",
    "    X,Y=([],[])\n",
    "    for word_index, word in enumerate(tokens) :\n",
    "        if ((word_index - WINDOW_SIZE >= 0) and (word_index + WINDOW_SIZE <= len(tokens) - 1)) :\n",
    "            X.append(word2idx[word])\n",
    "            Y.append([word2idx[tokens[word_index-i]] for i in window])\n",
    "    return X, Y\n",
    "\n",
    "stop_word = stopwords.words('english')\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "X, Y = ([], [])\n",
    "for sentence in text.lower().split(\".\"):\n",
    "    word_list = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [word for word in word_list if word not in stop_word]\n",
    "    X1, Y1 = sentenceToData(filtered_words, WINDOW_SIZE//2)\n",
    "    X.extend(X1)\n",
    "    Y.extend(Y1)\n",
    "    \n",
    "X = np.array(X).astype(int).reshape([-1,1])\n",
    "y = np.array(Y).astype(int)\n",
    "print('Shape of X :', X.shape)\n",
    "print('Shape of Y :', y.shape)\n",
    "\n",
    "# ----------------------------------------------\n",
    "class Word2vec(tf.keras.Model):\n",
    "    def __init__(self, N_DIM):\n",
    "        super(Word2vec, self).__init__()\n",
    "        self.W1 = tf.Variable(tf.random.uniform([vocab_size, N_DIM],-1.0, 1.0))\n",
    "        self.W2 = tf.Variable(tf.random.uniform([N_DIM, vocab_size],-1.0, 1.0))\n",
    "                   \n",
    "    def __call__(self, X, training=True):\n",
    "        X = tf.one_hot(X, depth=vocab_size, axis=-1)\n",
    "        X = tf.squeeze(X, axis=1)\n",
    "        h = tf.linalg.matmul(X, self.W1)\n",
    "        u = tf.linalg.matmul(h, self.W2)\n",
    "        return u\n",
    "# ----------------------------------------------\n",
    "\n",
    "batch_size = 64\n",
    "def loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.int32)\n",
    "    y_true = tf.one_hot(y_true, depth=vocab_size)\n",
    "    return -tf.tensordot(y_pred, tf.reduce_sum(y_true, axis=[1]),2)/batch_size + tf.reduce_sum(4*tf.math.log(tf.reduce_sum(tf.exp(y_pred), axis=[1])))/batch_size\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Entraînement\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "word2vec = Word2vec(100)\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "# ---------------------------------------------- compile + fit\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Compile le modèle\n",
    "word2vec.compile(optimizer=optimizer, loss=loss)\n",
    "# Entraîner le modèle sur 20 epoch\n",
    "word2vec.fit(X, y, batch_size=batch_size, epochs=20)\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Définition d'un optimisateur Adam\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile le modèle\n",
    "word2vec.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# Entraîner le modèle sur 20 epoch\n",
    "word2vec.fit(X, y, epochs=20, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Extraire la représentation matricielle \n",
    "from sklearn.preprocessing import Normalizer\n",
    "vectors = word2vec.W1.numpy()\n",
    "normalizer = Normalizer()\n",
    "vectors = normalizer.fit_transform(vectors, 'l2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Métrique dans l'espace word2vec\n",
    "\n",
    "## Run the program\n",
    "\n",
    "def dot_product(vec1, vec2):\n",
    "    return np.sum((vec1*vec2))\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return dot_product(vec1, vec2)/np.sqrt(dot_product(vec1, vec1)*dot_product(vec2, vec2))\n",
    "\n",
    "def find_closest(word_index, vectors, number_closest):\n",
    "    list1=[]\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if not np.array_equal(vector, query_vector):\n",
    "            dist = cosine_similarity(vector, query_vector)\n",
    "            list1.append([dist,index])\n",
    "    return np.asarray(sorted(list1,reverse=True)[:number_closest])\n",
    "\n",
    "def compare(index_word1,index_word2,index_word3,vectors,number_closest):\n",
    "    list1=[]\n",
    "    query_vector = vectors[index_word1]-vectors[index_word2]+vectors[index_word3]\n",
    "    normalizer = Normalizer()\n",
    "    query_vector =  normalizer.fit_transform([query_vector], 'l2')\n",
    "    query_vector= query_vector[0]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if not np.array_equal(vector, query_vector):\n",
    "            dist = cosine_similarity(vector, query_vector)\n",
    "            list1.append([dist,index])\n",
    "    return np.asarray(sorted(list1,reverse=True)[:number_closest])\n",
    "\n",
    "def print_closest(word, number=10):\n",
    "    index_closest_words = find_closest(word2idx[word], vectors, number)\n",
    "    for index_word in index_closest_words :\n",
    "        print(idx2word[index_word[1]],\" -- \",index_word[0])\n",
    "        \n",
    "\n",
    "        \n",
    "# ----------------------------------------------Propriétés arithmétiques\n",
    "index_compare_words = compare(word2idx['king'], word2idx['man'], word2idx['woman'], vectors,10)\n",
    "for index_word in index_compare_words :\n",
    "    print(idx2word[index_word[1]], \" -- \", index_word[0])\n",
    "    \n",
    "    \n",
    "# ---------------------------------------------- trouver le vecteur définissant le sexe d'un mot\n",
    "## Run the program.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pairs = [('woman', 'man'),\n",
    "('sister', 'brother'),\n",
    "('daughter', 'son'),\n",
    "('mother', 'father'),\n",
    "('girl', 'boy'),\n",
    "('queen', 'king')]\n",
    "\n",
    "n=np.shape(pairs)[0]\n",
    "difference_matrix = np.array([vectors[word2idx[a[0]]]-vectors[word2idx[a[1]]] for a in pairs])\n",
    "\n",
    "pca = PCA(n_components=n)\n",
    "pca.fit(difference_matrix)\n",
    "\n",
    "words=[vectors[word2idx[word]] for word in np.concatenate(pairs)]\n",
    "\n",
    "coord = pca.fit_transform(words)\n",
    "normalizer = Normalizer()\n",
    "coord =  normalizer.fit_transform(coord, 'l2')\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(12,12))\n",
    "\n",
    "for i in range(n*2):\n",
    "    plt.annotate(np.concatenate(pairs)[i],(coord[i,0],coord[i,1]))\n",
    "\n",
    "for i in range (n):\n",
    "    plt.plot([coord[2*i,0],coord[2*i+1,0]],[coord[2*i,1],coord[2*i+1,1]])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Modèle pré-entrainé\n",
    "import gensim.downloader as api\n",
    "print(api.info())\n",
    "# ----------------------------------------------model\n",
    "model = api.load(\"glove-wiki-gigaword-200\")\n",
    "\n",
    "# ----------------------------------------------ficher les mots plus similaire de \"king\"\n",
    "model.most_similar(\"eye\")\n",
    "\n",
    "# ---------------------------------------------- mots les plus proches de  King−Man+WomanKing−Man+Woman  avec la méthode most_similar\n",
    "model.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "\n",
    "\n",
    "# ----------------------------------------------trouver la dimension définissant le genre\n",
    "## Run the program.\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pairs = [('she', 'he'),\n",
    "('her', 'his'),\n",
    "('woman', 'man'),\n",
    "('sister', 'brother'),\n",
    "('herself', 'himself'),\n",
    "('daughter', 'son'),\n",
    "('mother', 'father'),\n",
    "('girl', 'boy'),\n",
    "('queen', 'king')]\n",
    "\n",
    "n = np.shape(pairs)[0]\n",
    "\n",
    "difference_matrix = np.array([model.word_vec(a[0], use_norm=True)-model.word_vec(a[1], use_norm=True) for a in pairs])\n",
    "\n",
    "pca = PCA(n_components=n)\n",
    "pca.fit(difference_matrix)\n",
    "\n",
    "words=[model.word_vec(word, use_norm=True) for word in np.concatenate(pairs)]\n",
    "\n",
    "coord = pca.fit_transform(words)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(12,12))\n",
    "for i in range(n*2):\n",
    "    plt.annotate(np.concatenate(pairs)[i],(coord[i,0],coord[i,1]))\n",
    "for i in range (n):\n",
    "    plt.plot([coord[2*i,0],coord[2*i+1,0]],[coord[2*i,1],coord[2*i+1,1]])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion ¶\n",
    "Le \"word2vec embedding\" capture efficacement les propriétés sémantiques et arithmetiques d'un mot. Il permet également de réduire la dimension du problème de tâche d'apprentissage.\n",
    "\n",
    "La fonction embedding_lookup de tensorflow.nn permet à l'index d'un mot d'appliquer un embedding défini en argument. Les mots inconnus sont randomisés dans la matrice word2vec.\n",
    "\n",
    "Dans l'ensemble, nous espérons que cela vous a montré comment TensorFlow vous offre une grande flexibilitée dans l'implémentation de vos modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------- Cours TensorFlow \n",
    "# ------------------------------------------- Module 4: RNN : Générer du texte \n",
    "\n",
    "# ----------------------------------------------  fonctionnement d'un RNN\n",
    "\n",
    "from interaction_rnn import show_rnn\n",
    "show_rnn()\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "file = open(\"shakespeare.txt\", \"r\", encoding='utf-8') \n",
    "text = file.read()\n",
    "file.close()\n",
    "print(text[:250])\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "print('{} uniques characters'.format(len(vocab)))\n",
    "\n",
    "# ---------------------------------------------- Convertir en index\n",
    "import numpy as np\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "print('{')\n",
    "for char, _ in zip(char2idx, range(10)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Génération du Dataset\n",
    "import tensorflow as tf\n",
    "seq_length = 100\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])\n",
    "\n",
    "    \n",
    "# ----------------------------------------------découper le jeu de données en batch de longueur seq_length + 1.\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "    \n",
    "    \n",
    "    \n",
    "# ---------------------------------------------- séparer les données d'entrées et les données cibles\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
    "    \n",
    "    \n",
    "\n",
    "# ----------------------------------------------shuffle \n",
    "\n",
    "batch_size = 64\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------RNN \n",
    "from tensorflow.keras.layers import Lambda, RNN, GRU, GRUCell, Dense\n",
    "\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "def build_model(batch_size):\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(Lambda(lambda x: tf.one_hot(tf.cast(x, tf.int32), depth=vocab_size), batch_input_shape=[batch_size, None])) # Add one_hot layer\n",
    "\n",
    "    model.add(RNN(GRUCell(512), # Cell of RNN\n",
    "                return_sequences=True, # return a sequence\n",
    "                stateful=True))\n",
    "\n",
    "    model.add(Dense(vocab_size))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- fonction loss \n",
    "def loss(y_true, y_pred):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "\n",
    "# ---------------------------------------------- cree 1 modele + compile\n",
    "\n",
    "# Create model\n",
    "model = build_model(64)\n",
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-2), loss=loss)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- fit\n",
    "model.fit(dataset, epochs=10)\n",
    "\n",
    "\n",
    "# ----------------------------------------------Diminuer le learning rate à 1e-3\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=loss)\n",
    "model.fit(dataset, epochs=15)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- gestion des poids \n",
    "\n",
    "# Save weights\n",
    "model.save_weights('model_rnn')\n",
    "# Create a new model with a batch_size of 1.\n",
    "model = build_model(1)\n",
    "# Load weights\n",
    "model.load_weights('model_rnn')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------générer du texte\n",
    "def generate_text(model, start_string, num_generate = 500):\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "    # Reset initial state\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        # Probability prediction\n",
    "        prediction = tf.nn.softmax(model(input_eval), axis=-1)\n",
    "        # Index prediction\n",
    "        index = tf.argmax(prediction, axis=-1).numpy()[0]\n",
    "        input_eval = tf.expand_dims([index[-1]], 0)\n",
    "        # Save letter in text_generated list\n",
    "        text_generated.append(idx2char[index[-1]])\n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------generer un mot\n",
    "print(generate_text(model, start_string=\"ROMEO: \"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------Charger les poids entraînés\n",
    "\n",
    "model = build_model(1)\n",
    "model.load_weights('./weights/gru_model')\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------Afficher le texte généré\n",
    "print(generate_text(model, start_string=\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Dans l'ensemble de l'exercice, vous avez formé un réseau de neurones pour générer du texte. Même si la longueur du texte que nous échantillonnons est assez petite, nous pouvons néanmoins remarquer certaines caractéristiques intéressantes du texte généré. Par exemple, le réseau apprend quelques mots de base comme «and», «of» «you» et «could» assez rapidement dans la formation. En outre, il apprend également quelques règles grammaticales de base: mettre des majuscules au premier mot de la phrase, finir une phrase par de la ponctuation, poser des questions fonctionne si les phrases commencent par des mots comme «what», «where». Le système n'est pas parfait mais étant donné que nous avons alimenté le réseau en caractères simples, il est remarquable que le réseau soit capable d'apprendre ces dépendances à long terme.\n",
    "\n",
    "Cet exercice est assez différent des exercices précédents car nous n'avons pas classé ou appris certaines fonctionnalités à partir de données. Les réseaux de neurones sont utiles non seulement pour classer les données mais aussi pour générer des données.\n",
    "\n",
    "Le taux d'apprentissage est très important pendant la formation. Si nous avons un taux d'apprentissage très élevé au début, nous pourrions rester coincés dans certains minima locaux pas vraiment représentatifs. Trop bas et l'entraînement devient lent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------- Cours TensorFlow \n",
    "# ------------------------------------------- Module 5 : Generative Adversarial Networks (GAN)\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------Créer des personnages animés\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "list_imgs = os.listdir('images')\n",
    "X = []\n",
    "k=0\n",
    "for i, l in enumerate(list_imgs[:1000]):\n",
    "    # Load image\n",
    "    img = plt.imread('./images/'+l)\n",
    "    # Remove low resolution image\n",
    "    if img.shape[0]<50 or img.shape[1]<50:\n",
    "        k=k+1\n",
    "        continue\n",
    "    # Remove gray images\n",
    "    if len(img.shape)!=3:\n",
    "        continue\n",
    "    # Resize image    \n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    X.append(img)\n",
    "\n",
    "X = np.array(X)\n",
    "print('Shape of X :', X.shape)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- affichez des images de notre datasets\n",
    "j = 1\n",
    "plt.figure(figsize=(15,7))\n",
    "for i in np.random.randint(low=0, high=len(X), size=[8]):\n",
    "    plt.subplot(2,4,j)\n",
    "    plt.imshow(X[i])\n",
    "    j += 1\n",
    "    \n",
    "\n",
    "#Créer un dataset de X à l'aide de la fonction from_tensor_slices de tf.data.Dataset.\n",
    "# ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "# Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X))\n",
    "# Map function\n",
    "def fn_map(x):\n",
    "    return x/255\n",
    "# Transformations\n",
    "dataset = dataset.shuffle(10000).map(fn_map).batch(32, drop_remainder=True)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Modèle Génératif\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, LeakyReLU, Dropout, Flatten, Reshape, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(noise_len):\n",
    "    model = tf.keras.Sequential()\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128 * 8 * 8\n",
    "    model.add(Dense(n_nodes, input_dim=noise_len))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((8, 8, 128)))\n",
    "    # upsample to 16x16\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 32x32\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 64x64\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(3, (7,7), activation='sigmoid', padding='same'))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- fonction generate input noise et generate fake sample \n",
    "\n",
    "\n",
    "from numpy import zeros\n",
    "from numpy.random import randn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_inputs_noise(noise_len, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(noise_len * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, noise_len)\n",
    "    return x_input\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, noise_len, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_inputs_noise(noise_len, n_samples)\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------  modèle générateur sous le nom model  \n",
    "# size of the latent space\n",
    "noise_len = 100\n",
    "# define the discriminator model\n",
    "model = define_generator(noise_len)\n",
    "# generate samples\n",
    "n_samples = 8\n",
    "X_sample, _ = generate_fake_samples(model, noise_len, n_samples)\n",
    "\n",
    "# plot the generated samples\n",
    "plt.figure(figsize=(15, 7))\n",
    "for i in range(n_samples):\n",
    "    # define subplot\n",
    "    plt.subplot(2, 4, 1 + i)\n",
    "    # turn off axis labels\n",
    "    plt.axis('off')\n",
    "    # plot single image\n",
    "    plt.imshow(X_sample[i])\n",
    "    \n",
    "# show the figure\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------define_discriminator \n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, LeakyReLU, Dropout, Flatten, Reshape, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(input_shape=(64,64,3)):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv2D(256, (3,3), strides=(2, 2), padding='same', input_shape=input_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(128, (3,3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Modèle GAN\n",
    "\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect them\n",
    "    model = tf.keras.Sequential()\n",
    "    # add generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    " \n",
    "# size of the latent space\n",
    "noise_len = 100\n",
    "# create the discriminator\n",
    "d_model = define_discriminator()\n",
    "d_model.trainable = False\n",
    "# create the generator\n",
    "g_model = define_generator(noise_len)\n",
    "# create the gan\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------Retourner d_loss et g_loss.\n",
    "\n",
    "from numpy import vstack, ones\n",
    "batch_size = 64\n",
    "half_batch = int(batch_size/2)\n",
    "\n",
    "def train_op(X_real):\n",
    "    # get randomly selected 'real' samples\n",
    "    y_real = np.ones([half_batch, 1])\n",
    "    # generate 'fake' examples\n",
    "    X_fake, y_fake = generate_fake_samples(g_model, noise_len, half_batch) \n",
    "    # create training set for the discriminator\n",
    "    X_discriminator, y_discriminator = vstack((X_real, X_fake)), vstack((y_real, y_fake))\n",
    "    # update discriminator model weights\n",
    "    d_loss = d_model.train_on_batch(X_discriminator, y_discriminator)\n",
    "    \n",
    "    # prepare points in latent space as input for the generator\n",
    "    X_gan = generate_inputs_noise(noise_len, batch_size)\n",
    "    # create inverted labels for the fake samples\n",
    "    y_gan = ones((batch_size, 1))\n",
    "    # update the generator via the discriminator's error\n",
    "    g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "    # summarize loss on this batch\n",
    "    return d_loss, g_loss\n",
    "    print('>%d, %d/, d=%.3f, g=%.3f' % (i+1, j+1, d_loss, g_loss))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# ---------------------------------------------- Entraîner le modèle sur 10 epochs et afficher à chaque itération\n",
    "\n",
    "n_epochs = 10\n",
    "half_batch = int(batch_size/2)\n",
    "for i in range(n_epochs):\n",
    "    # enumerate batches over the training set\n",
    "    for j, X_real in enumerate(dataset):\n",
    "        # Training model\n",
    "        d_loss, g_loss = train_op(X_real)\n",
    "        # Print loss\n",
    "        print('> Epoch %d, Batch %d, d_loss=%.3f, g_loss=%.3f' % (i+1, j+1, d_loss, g_loss))\n",
    "        \n",
    "        \n",
    "        \n",
    "# ---------------------------------------------- afficher des images générées par notre modèle\n",
    "\n",
    "noise_len\n",
    "n_samples = 8\n",
    "X, _ = generate_fake_samples(g_model, noise_len, n_samples)\n",
    "\n",
    "# plot the generated samples\n",
    "plt.figure(figsize=(15, 7))\n",
    "for i in range(n_samples):\n",
    "    # define subplot\n",
    "    plt.subplot(2, 4, 1 + i)\n",
    "    # turn off axis labels\n",
    "    plt.axis('off')\n",
    "    # plot single image\n",
    "    plt.imshow(X[i], cmap='gray_r')\n",
    "# show the figure\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Charger les poids entraînés sur plus de 200000 itérations du modèle gan_model\n",
    "\n",
    "\n",
    "gan_model.load_weights('./weights/gan_model')\n",
    "\n",
    "\n",
    "#afficher des images générées par notre modèle avec les nouveaux poid\n",
    "# ----------------------------------------------\n",
    "\n",
    "n_samples = 8\n",
    "noise_len = 100\n",
    "X, _ = generate_fake_samples(g_model, noise_len, n_samples)\n",
    "\n",
    "# plot the generated samples\n",
    "plt.figure(figsize=(15, 7))\n",
    "for i in range(n_samples):\n",
    "    # define subplot\n",
    "    plt.subplot(2, 4, 1 + i)\n",
    "    # turn off axis labels\n",
    "    plt.axis('off')\n",
    "    # plot single image\n",
    "    plt.imshow(X[i], cmap='gray_r')\n",
    "# show the figure\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------- generer des new images\n",
    "x_input = np.zeros([8, 100])\n",
    "x_input[:,0]=np.linspace(0, 5, 8)\n",
    "x_input[:,1]=np.linspace(0, 3, 8)\n",
    "X = g_model.predict(x_input)\n",
    "\n",
    "# plot the generated samples\n",
    "plt.figure(figsize=(15, 7))\n",
    "for i in range(n_samples):\n",
    "    # define subplot\n",
    "    plt.subplot(2, 4, 1 + i)\n",
    "    # turn off axis labels\n",
    "    plt.axis('off')\n",
    "    # plot single image\n",
    "    plt.imshow(X[i], cmap='gray_r')\n",
    "# show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous avez pu générer de nouvelles images en opposant un réseau de neurones à un autre. Cependant, il est difficile de comprendre l'influence de chaque dimension du vecteur d'entrée sur les caractéristiques du personnage.\n",
    "\n",
    "Dans l'ensemble, nous espérons que cela vous a montré comment TensorFlow vous offre une grande flexibilité dans l'implémentation de vos modèles.\n",
    "\n",
    "Ce qu'il faut retenir :\n",
    "Les réseaux de neurones organisés en couches sont des outils de machine learning désormais accessibles offrant des résultats pouvant surpasser de loin les algorithmes classiques sur des tâches complexes.\n",
    "\n",
    "Tensorflow une bibliothèque de référence pour effectuer du deep learning. La version de tensorflow 2.0+ a été construite autour du framework keras.\n",
    "\n",
    "Globalement, le schéma est le même que pour beaucoup d'algorithmes classiques:\n",
    "\n",
    "Mise en forme des données.\n",
    "Définir un Dataset et partitionner le en batch.\n",
    "Construction du modèle.\n",
    "Entraîner le modèle, nous avons vue trois manières équivalentes de le faire:\n",
    "Méthode fit: problème simple.\n",
    "Méthode train_on_batch: pour entraîner itération par itération.\n",
    "Calculer le gradient de la fonction puis rétropropager l'erreur: problème complexe.\n",
    "Comme nous l'avons vu dans l'ensemble du module, un paramètre crucial est le taux d'apprentissage. Dans cet exercice, nous avons défini le même taux d'apprentissage pour les deux réseaux. Nous aurions pu jouer sur les taux d'apprentissages pour les deux réseaux pour gagner en efficacité.\n",
    "\n",
    "Dans l'ensemble, nous espérons que cela vous a montré comment TensorFlow vous offre une grande flexibilité dans l'implémentation de vos modèles.\n",
    "\n",
    "\n",
    "\n",
    "Bravo ! Vous êtes arrivé à la fin de la formation de Tensorflow. N'hésitez pas à nous faire part de vos retours pour l'amélioration de nos formations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet examen, nous allons tester vos connaissances sur les idées et concepts de base de Tensorflow. Suite à un exercice court sur un problème d'optimisation, nous testerons ensuite votre capacité à implémenter un réseau de neurones et une métrique pour un problème de régression.\n",
    "\n",
    "L'exercice est composé de plusieurs questions, faites-les dans l'ordre et faites attention à respecter le nom des variables. N'hésitez pas à contacter l'équipe DataScientest si vous rencontrez des problèmes sur help@datascientest.com.\n",
    "\n",
    "Exercice: Warm Up!\n",
    "Soit  f(x)=exp(x−2)+15x²+50x+1\n",
    "\n",
    "L'objectif de cet exercice est de trouver la valeur x qui minimise la fonction f.\n",
    "\n",
    "Afficher la courbe de la fonction f entre [-5, 5]\n",
    "Trouver graphiquement la valeur approximative x qui minimise la fonction f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288.71826"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = tf.Variable (3.0)\n",
    "\n",
    "#fonction f(x)=exp(x−2)+15x²+50x+1\n",
    "def f(x):\n",
    "    # Création de \"two\" via un objet de type constant\n",
    "    one = tf.constant(1.0, dtype=tf.float32)\n",
    "    \n",
    "    # Application d'un bloc with afin de définir une sous-boîte via la fonction name_scope appliquée à 'Function'\n",
    "    with tf.name_scope('Function'):\n",
    "        f_x = tf.exp ( x - 2 ) + (15* x**2 )+ (50 * x) + one\n",
    "\n",
    "    return f_x\n",
    "\n",
    "\n",
    "f(x).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24d528189a0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "liste_x = [-5.0,-4.0,-3.0,-2.0,-1.0,0.0,1.0,2.0,3.0,4.0,5.0  ]\n",
    "liste_y = []\n",
    "\n",
    "for i in liste_x : \n",
    "\n",
    "    liste_y.append (f(i).numpy())\n",
    "\n",
    "liste_y\n",
    "plt.plot (liste_x,liste_y)\n",
    "\n",
    "#entre -2 et -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme ∂f(x)/∂x=0 n'admet pas de solution explicite, trouver la valeur numérique x qui minimise la fonction f en utilisant des descentes de gradient. Est-ce que la solution est cohéremnte avec la solution trouvée graphiquement ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_p(x):\n",
    "    e0_p = tf.exp ( tf.subtract ( tf.Variable( x ) , tf.constant ( 2.0,dtype=tf.float32 ) ))       \n",
    "    e1_p = tf.Variable ( tf.constant (30.0,dtype=tf.float32) * tf.Variable(x))\n",
    "    e2_p = tf.constant (50.0,dtype=tf.float32) \n",
    "  \n",
    "    return  e0_p.numpy() + e1_p.numpy() + e2_p.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pour x =  -5.0\n",
      "-99.999084\n",
      "pour x =  -4.0\n",
      "-69.99752\n",
      "pour x =  -3.0\n",
      "-39.993263\n",
      "pour x =  -2.0\n",
      "-9.981686\n",
      "pour x =  -1.0\n",
      "20.049788\n",
      "pour x =  0.0\n",
      "50.135334\n",
      "pour x =  1.0\n",
      "80.36788\n",
      "pour x =  2.0\n",
      "111.0\n",
      "pour x =  3.0\n",
      "142.71829\n",
      "pour x =  4.0\n",
      "177.38905\n",
      "pour x =  5.0\n",
      "220.08554\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_p = []\n",
    "\n",
    "for i in liste_x : \n",
    "    print ('pour x = ',i)\n",
    "    print (f_p(i))\n",
    "    y_p.append (f_p(i))\n",
    "    \n",
    "plt.plot (liste_x,y_p)\n",
    "plt.axhline(0,c ='red')\n",
    "plt.axvline(-2,c ='red')\n",
    "plt.show();\n",
    "\n",
    "#oui c est coherent car = 0 entre -2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_x = [-5.0 , -4.0 , -3.0 , -2.0 , -1.0 , 0.0 , 1.0 , 2.0 , 3.0 , 4.0 ,5.0  ]\n",
    "\n",
    "\n",
    "#for i in range (-5,6):\n",
    "#    x.assign( float (i))\n",
    "#    with tf.GradientTape() as tape:\n",
    "#        function = f(x)\n",
    "        \n",
    "#    grad = tape.gradient(function, x)\n",
    "#    print ('x=',i)\n",
    "#    print(grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher la valeur du gradient en cette valeur.\n",
    "\n",
    "Est-ce que la valeur du gradient est cohérente ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.014472961"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_p (-1.668)\n",
    "\n",
    "#ok "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction a modeliser f(x)=exp(x−2)+15x²+50x+1\n",
    "\n",
    "class Polynomial(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        # Initialisation de tf.keras.Model\n",
    "        super(Polynomial, self).__init__()\n",
    "        # Instantier la variable w0.\n",
    "        self.w0 = tf.Variable(tf.random.normal([1]), name='w0')\n",
    "        \n",
    "        # Instantier la variable w1.\n",
    "        self.w1 = tf.Variable(tf.random.normal([1]), name='w1')\n",
    "        \n",
    "        # Instantier la variable w2.\n",
    "        self.w2 = tf.Variable(tf.random.normal([1]), name='w2')\n",
    "               \n",
    "        # Instantier la variable b.\n",
    "        self.b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "        \n",
    "    def __call__(self, inputs, training=True):\n",
    "        # Prédiction de notre modèle.\n",
    "        return self.b + self.w0*inputs + self.w1*inputs**2 + self.w2* tf.exp (inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    e0 = tf.exp ( tf.subtract ( tf.Variable(x) , tf.constant (2.0,dtype=tf.float32) , name='e0' ))       \n",
    "    e1 = tf.Variable ( tf.constant (15.0,dtype=tf.float32) * tf.Variable(x)**2, name='e1')\n",
    "    e2 = tf.Variable( tf.constant (50.0,dtype=tf.float32) * tf.Variable(x), name='e2')\n",
    "    b =  tf.constant (1.0,dtype=tf.float32, name='b')\n",
    "    return b + e0 + e1 + e2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "f([5.0,1.0] ).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable (3.0)\n",
    "\n",
    "#fonction a modeliser f(x)=exp(x−2)+15x²+50x+1\n",
    "def f(x):\n",
    "    # Création de \"two\" via un objet de type constant\n",
    "    one = tf.constant(1.0, dtype=tf.float32)\n",
    "    \n",
    "    # Application d'un bloc with afin de définir une sous-boîte via la fonction name_scope appliquée à 'Function'\n",
    "    with tf.name_scope('Function'):\n",
    "        f_x = tf.exp ( x - 2 ) + (15* x**2 )+ (50 * x) + one\n",
    "\n",
    "    return f_x\n",
    "\n",
    "\n",
    "f(x).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.assign(4.0)\n",
    "# Compute f(4)\n",
    "\n",
    "\n",
    "for i in range (-5,5):\n",
    "    x.assign( float (i))\n",
    "    with tf.GradientTape() as tape:\n",
    "        function = f(x)\n",
    "        \n",
    "    grad = tape.gradient(function, x)\n",
    "    print(grad.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction a modeliser f(x)=exp(x−2)+15x²+50x+1\n",
    "\n",
    "class Polynomial(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        # Initialisation de tf.keras.Model\n",
    "        super(Polynomial, self).__init__()\n",
    "        # Instantier la variable w0.\n",
    "        self.w0 = tf.Variable( 50.0, name='w0')\n",
    "        \n",
    "        # Instantier la variable w1.\n",
    "        self.w1 = tf.Variable(15.0, name='w1')\n",
    "        \n",
    "        # Instantier la variable w2.\n",
    "        self.w2 = tf.Variable(1.0, name='w2')\n",
    "               \n",
    "        # Instantier la variable b.\n",
    "        self.b = tf.constant (1.0, name='bias')\n",
    "        \n",
    "    def __call__(self, inputs, training=True):\n",
    "        # Prédiction de notre modèle.\n",
    "        return self.b + self.w0*inputs + self.w1*inputs**2 + self.w2* tf.exp (inputs - 2.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Polynomial()\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(1e-1)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_history = model.fit(x, y, batch_size=16, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model([-5,2])\n",
    "\n",
    "pred = []\n",
    "for i in liste_x : \n",
    "    pred.append ( model (i) )\n",
    "    \n",
    "plt.plot (liste_x,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_x = [-5.0,-4.0,-3.0,-2.0,-1.0,0.0,1.0,2.0,3.0,4.0,5.0  ]\n",
    "liste_y = []\n",
    "\n",
    "for i in liste_x : \n",
    "\n",
    "    liste_y.append (f(i).numpy())\n",
    "\n",
    "liste_y\n",
    "plt.plot (liste_x,liste_y)\n",
    "\n",
    "plt.plot (liste_x, pred,'r--')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Afficher l'évolution de la fonction de perte\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(training_history.history['loss'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Valeur de la fonction de perte')\n",
    "\n",
    "plt.subplot(122)\n",
    "# Afficher les points (x, y).\n",
    "plt.scatter(liste_x, y, alpha=0.5, label = 'True value')\n",
    "# Afficher la prédiction de X.\n",
    "plt.plot(liste_x, pred, 'r', label = 'Prediction')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher l'évolution de la fonction de perte\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(training_history.history['loss'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Valeur de la fonction de perte')\n",
    "\n",
    "plt.subplot(122)\n",
    "# Afficher les points (x, y).\n",
    "plt.scatter(X, y, alpha=0.5, label = 'True value')\n",
    "# Afficher la prédiction de X.\n",
    "plt.plot(np.linspace(-2.5, 2.5, 100), y_pred, 'r', label = 'Prediction')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme  ∂f(x)/∂x=0  n'admet pas de solution explicite, trouver la valeur numérique x qui minimise la fonction f en utilisant des descentes de gradient.\n",
    "Est-ce que la solution est cohérente avec la solution trouvée graphiquement ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable (3.0)\n",
    "x.assign(2.0)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insérez votre code ici\n",
    "x.assign(0.0)\n",
    "\n",
    "def f_p(x):\n",
    "    e0_p = tf.exp ( tf.subtract ( tf.Variable( x ) , tf.constant ( 2.0,dtype=tf.float32 ) ))       \n",
    "    e1_p = tf.Variable ( tf.constant (30.0,dtype=tf.float32) * tf.Variable(x))\n",
    "    e2_p = tf.constant (50.0,dtype=tf.float32) \n",
    "  \n",
    "    return  e0_p.numpy() + e1_p.numpy() + e2_p.numpy()\n",
    "\n",
    "f_p(0.0)\n",
    "\n",
    "y_p = []\n",
    "\n",
    "\n",
    "for i in liste_x : \n",
    "    print ('pour x = ',i)\n",
    "    print (f_p(i))\n",
    "    y_p.append (f_p(i))\n",
    "    \n",
    "plt.plot (liste_x,y_p)\n",
    "plt.show();\n",
    "#oui variation = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher la valeur du gradient en cette valeur.\n",
    "\n",
    "Est-ce que la valeur du gradient est cohérente ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insérez votre code ici\n",
    "\n",
    "f(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problème de régression : Détection de la boîte englobant le visage\n",
    "\n",
    "\n",
    "Dans cet examen, nous allons former un modèle permettant de trouver les coordonnées du rectangle englobant le visage d'une personne. Nous travaillerons sur des images provenant de vidéo YouTube stocké sous forme d'image dans le dossier imgs\". Les cordonnées des points à l'extrémité du rectangle englobant le visage sont sous forme de CSV.\n",
    "\n",
    "\n",
    "\n",
    "Exécuter la cellule suivante pour charger les images dans la variable X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "X = []\n",
    "for p in os.listdir('imgs'):\n",
    "    X.append(plt.imread('imgs/'+p))\n",
    "\n",
    "X = np.array(X)\n",
    "print('Shape of X :', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger le fichier \"face_boxe.csv\" sous le nom df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insérez votre code ici\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv ('face_boxe.csv')\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymin</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.420237</td>\n",
       "      <td>0.840894</td>\n",
       "      <td>0.749575</td>\n",
       "      <td>0.833809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.244296</td>\n",
       "      <td>0.960838</td>\n",
       "      <td>0.919464</td>\n",
       "      <td>0.835696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.709742</td>\n",
       "      <td>0.533572</td>\n",
       "      <td>0.425805</td>\n",
       "      <td>0.373661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024793</td>\n",
       "      <td>0.224256</td>\n",
       "      <td>0.919463</td>\n",
       "      <td>0.077241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.401649</td>\n",
       "      <td>0.186513</td>\n",
       "      <td>0.749395</td>\n",
       "      <td>0.627936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       xmin      xmax      ymin      ymax\n",
       "0  0.420237  0.840894  0.749575  0.833809\n",
       "1  0.244296  0.960838  0.919464  0.835696\n",
       "2  0.709742  0.533572  0.425805  0.373661\n",
       "3  0.024793  0.224256  0.919463  0.077241\n",
       "4  0.401649  0.186513  0.749395  0.627936"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature = np.random.rand (20,4)\n",
    "feature = pd.DataFrame (feature, columns = ['xmin','xmax','ymin','ymax'])\n",
    "feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on cree les targets\n",
    "\n",
    "target = pd.DataFrame ()\n",
    "target['xmoy'] = (feature['xmin'] + feature['xmax']) / 2\n",
    "target['ymoy'] = (feature['ymin'] + feature['ymax']) / 2\n",
    "\n",
    "target['w' ] =  feature['xmax'] - feature['xmin']\n",
    "target['h'] = feature['ymax'] - feature['ymin']\n",
    "\n",
    "target.head()\n",
    "\n",
    "#on separe les ensembles \n",
    "\n",
    "int (len (feature) *0.8)  #16\n",
    "\n",
    "X_train = feature.loc [:16]\n",
    "X_test = feature.loc [:16]\n",
    "\n",
    "y_train = target.loc [:16]\n",
    "y_test = target.loc [:16]\n",
    "\n",
    "\n",
    "#on applique le tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([17, 4])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape  #(17, 4)\n",
    "X_train = tf.cast(tf.reshape(X_train, [17,4]), tf.float64)\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset = dataset.shuffle(20000).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target elements of the batch :\n",
      " [[ 0.63056538  0.79169182  0.42065635  0.08423357]\n",
      " [ 0.51722447  0.47115654  0.85966887 -0.00196432]\n",
      " [ 0.52309771  0.53199564  0.6599921  -0.93067373]\n",
      " [ 0.47566256  0.70403341  0.09980888  0.07267634]\n",
      " [ 0.43940866  0.17004619  0.7152352  -0.0678336 ]\n",
      " [ 0.45531019  0.61622444 -0.09777505  0.65028961]\n",
      " [ 0.52981123  0.48165895 -0.92550752  0.93092085]\n",
      " [ 0.29408094  0.68866531 -0.21513643 -0.12145958]\n",
      " [ 0.62165734  0.39973309 -0.17616994 -0.05214369]\n",
      " [ 0.39306112  0.5662681  -0.58408617 -0.38857316]\n",
      " [ 0.12452446  0.49835173  0.1994625  -0.8422223 ]\n",
      " [ 0.41104156  0.08034592 -0.64378195  0.03037214]\n",
      " [ 0.63470627  0.38022273 -0.46316884 -0.52982159]\n",
      " [ 0.34682361  0.61572839 -0.67079767  0.00919909]\n",
      " [ 0.602567    0.87758016  0.71654265 -0.08376854]\n",
      " [ 0.76326442  0.4712612  -0.27504023  0.6523453 ]\n",
      " [ 0.19793878  0.62672877 -0.1515963  -0.17816681]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (X_b, y_b) in dataset.take(5):\n",
    "    print('Target elements of the batch :\\n', y_b.numpy(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécuter la cellule suivante pour afficher des images du jeu de données ainsi que le rectangle englobant le visage correspondant.\n",
    "   Nous avons choisi d'utiliser la fonction plot de matplotlib.pyplot pour afficher les segments d'un rectangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,5))\n",
    "j=1\n",
    "for i in np.random.randint(0, len(df), size=[8]):\n",
    "    plt.subplot(2,4,j)\n",
    "    plt.axis('off')\n",
    "    img = X[i]\n",
    "    plt.imshow(img)\n",
    "    x_min = df.xmin[i]\n",
    "    x_max = df.xmax[i]\n",
    "    y_min = df.ymin[i]\n",
    "    y_max = df.ymax[i]\n",
    "    plt.plot([x_min, x_max, x_max, x_min, x_min], [y_min, y_min, y_max, y_max, y_min], '-b')\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour localiser une unique boîte encadrant la voiture dans l'image, nous pouvons se ramener à un problème de régression sur 4 variables cibles suivante.\n",
    "\n",
    "\n",
    "\n",
    "xmoyxmoy  : position horizontale (normalisé) du milieu de la boîte.\n",
    "ymoyymoy  : position verticale (normalisé) du milieu de la boîte.\n",
    "w : largeur (normalisé) de la boîte.\n",
    "h : hauteur (normalisé) de la boîte.\n",
    "Les valeurs de  xmoyxmoy ,  ymoyymoy , w et h doivent être comprises entre 0 et 1. Il est donc nécessaire de diviser les valeurs de  xmoyxmoy ,  ymoyymoy , w et h par 256.\n",
    "\n",
    "Mettre en forme les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df/256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Séparer le jeu de données en un ensemble d'entraînement et en un ensemble de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 4)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[:14]\n",
    "test = df[14:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle :\n",
    "\n",
    "Vous allez maintenant définir un modèle convolutionnel permettant de prédire les valeurs  xmoyxmoy ,  ymoyymoy , w et h en fonction des images en entrée.\n",
    "\n",
    "\n",
    "En vous inspirant de l'image précédente, définir un modèle convolutionnel permettant d'estimer les paramètres X, Y, w et h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 5 from 4 for '{{node conv2d_1/Conv2D/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv2d_1/Conv2D/Reshape, conv2d_1/Conv2D/Conv2D/ReadVariableOp)' with input shapes: [?,17,4,1], [5,5,1,30].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1811\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1812\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1813\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 5 from 4 for '{{node conv2d_1/Conv2D/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv2d_1/Conv2D/Reshape, conv2d_1/Conv2D/Conv2D/ReadVariableOp)' with input shapes: [?,17,4,1], [5,5,1,30].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-10adb31bba95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlenet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m lenet.add(Conv2D(filters = 30,                   # Number of output filters\n\u001b[0m\u001b[0;32m      6\u001b[0m                 \u001b[0mkernel_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m            \u001b[1;31m# Kernel shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m       \u001b[1;31m# Input shapeD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    204\u001b[0m           \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m           \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m           \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m           \u001b[0mset_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    926\u001b[0m                                                 input_list)\n\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1115\u001b[0m           \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m               \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    245\u001b[0m       \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1008\u001b[0m     \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m     name=None):\n\u001b[1;32m-> 1010\u001b[1;33m   return convolution_internal(\n\u001b[0m\u001b[0;32m   1011\u001b[0m       \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m       \u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[0;32m   1138\u001b[0m         \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv1d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1140\u001b[1;33m       return op(\n\u001b[0m\u001b[0;32m   1141\u001b[0m           \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m           \u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   2590\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2591\u001b[0m         name=name)\n\u001b[1;32m-> 2592\u001b[1;33m   return squeeze_batch_dims(\n\u001b[0m\u001b[0;32m   2593\u001b[0m       \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2594\u001b[0m       functools.partial(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36msqueeze_batch_dims\u001b[1;34m(inp, op, inner_rank, name)\u001b[0m\n\u001b[0;32m    311\u001b[0m           inp, array_ops.concat(([-1], inner_shape), axis=-1))\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m     \u001b[0mout_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp_reshaped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[0mout_inner_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_reshaped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0minner_rank\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m    973\u001b[0m         \"'conv2d' Op, not %r.\" % dilations)\n\u001b[0;32m    974\u001b[0m   \u001b[0mdilations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dilations\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdilations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[0;32m    976\u001b[0m         \u001b[1;34m\"Conv2D\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    740\u001b[0m       \u001b[1;31m# Add Op to graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0m\u001b[0;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m                                  attrs=attr_protos, op_def=op_def)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m       \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    592\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         compute_device)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3475\u001b[0m     \u001b[1;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3476\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3477\u001b[1;33m       ret = Operation(\n\u001b[0m\u001b[0;32m   3478\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3479\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1972\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mop_def\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1974\u001b[1;33m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0m\u001b[0;32m   1975\u001b[0m                                 control_input_ops, op_def)\n\u001b[0;32m   1976\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1813\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1814\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1815\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1817\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative dimension size caused by subtracting 5 from 4 for '{{node conv2d_1/Conv2D/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv2d_1/Conv2D/Reshape, conv2d_1/Conv2D/Conv2D/ReadVariableOp)' with input shapes: [?,17,4,1], [5,5,1,30]."
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "\n",
    "lenet = tf.keras.Sequential()\n",
    "\n",
    "lenet.add(Conv2D(filters = 30,                   # Number of output filters\n",
    "                kernel_size = (5, 5),            # Kernel shape\n",
    "                input_shape = (1,17, 4, 1),       # Input shapeD\n",
    "                activation = 'relu'))            # Activation function\n",
    "\n",
    "lenet.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "lenet.add(Conv2D(filters = 16,                    \n",
    "                kernel_size = (3, 3),\n",
    "                activation = 'relu'))\n",
    "\n",
    "lenet.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "lenet.add(Flatten())\n",
    "\n",
    "lenet.add(Dropout(rate = 0.2))\n",
    "\n",
    "lenet.add(Dense(units = 128, activation = 'relu'))\n",
    "\n",
    "lenet.add(Dense(units = 4 , activation = 'softmax'))\n",
    "\n",
    "lenet.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser la fonction de coût suivante :\n",
    "\n",
    "Loss=(xpred−xtrue)2+(ypred−ytrue)2+(|wpred|−wtrue )2+(|hpred|−htrue )2\n",
    " \n",
    "   Astuce : Vous pouvez découper un tensor X en prenant toutes les lignes (:) mais en gardant qu'une colonne spécifique (i) en effectuant l'opéraction X[:, i].\n",
    "Définir une fonction retournant le résultat de la fonction de coût ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_op(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # model prediction\n",
    "        y_pred = model(inputs, training=True)\n",
    "        # compute the loss function\n",
    "        loss_value = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(targets, y_pred))\n",
    "        \n",
    "    # Compute the gradient of loss function\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    # Gradient descent\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # Return the loss function value\n",
    "    return loss_value.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraîner votre modèle sur un nombre d'itérations faibles (moins de 5 minutes d'entraînement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs): \n",
    "    # Pour chaque epoch\n",
    "    for X_t, y_t in dataset:\n",
    "        #Entraîner le modèle pour chaque batch\n",
    "        train_op(lenet, X_t, y_t)\n",
    "        \n",
    "    # Fonction de coût pour l'ensemble de validation\n",
    "    loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_test, lenet(X_test))).numpy()\n",
    "    print('Iteration', i, ' Loss', loss)\n",
    "    grads.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher pour une des images de l'ensemble de validation le rectangle estimé par notre modèle à l'aide de la fonction show_img."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous vous rappelons que nous avons entraîné notre modèle que sur peu d'itération ce qui explique de mauvais résultat.\n",
    "\n",
    "Exécuter la cellule suivante pour charger un modèle entraîné sur ce jeu de données plus de 5 heures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher pour une des images de l'ensemble de validation le rectangle estimé par notre modèle à l'aide de la fonction show_img."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def show_img(img, x0, y0, w0, h0):\n",
    "    plt.imshow(img)\n",
    "    x0 = np.abs(x0)\n",
    "    y0 = np.abs(y0)\n",
    "    w0 = np.abs(w0)\n",
    "    h0 = np.abs(h0)\n",
    "    x1= (x0-w0/2)*256\n",
    "    x2= (x0+w0/2)*256\n",
    "    y1= (y0-h0/2)*256\n",
    "    y2= (y0+h0/2)*256\n",
    "    plt.plot([x1,x2,x2,x1,x1], [y1,y1,y2,y2,y1], \"r\")\n",
    "## Insérez votre code ici\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous vous rappelons que nous avons entraîné notre modèle que sur peu d'itération ce qui explique de mauvais résultat.\n",
    "\n",
    "Exécuter la cellule suivante pour charger un modèle entraîné sur ce jeu de données plus de 5 heures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "model = json_file.read()\n",
    "json_file.close()\n",
    "# Load model\n",
    "model = model_from_json(model)\n",
    "# Load weights\n",
    "model.load_weights('./weights/face_detection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher pour une des images de l'ensemble de validation le rectangle estimé par notre modèle à l'aide de la fonction show_img."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la fonction url_to_image, vous pouvez charger une image à partir d'un lien URL.\n",
    "\n",
    "Prédire le rectangle encadrant le visage avec des images sur internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import cv2\n",
    "def url_to_image(url):\n",
    "    resp = urllib.request.urlopen(url) \n",
    "    img = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    img = cv2.imdecode(img, -1)\n",
    "    img = cv2.resize(img[:,:,[2,1,0]], (256, 256))\n",
    "    return img\n",
    "\n",
    "## Example :\n",
    "img = url_to_image(\"https://starbyface.com/ImgBase/testPhoto/test1.jpg\")\n",
    "\n",
    "## Insérez votre code ici\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Évaluation : Intersection over Union (partie optionnelle)¶\n",
    "\n",
    "Intersection over Union (IoU) est la métrique d'évaluation la plus populaire utilisée dans les tests de référence de détection d'objets. Cependant, il existe un écart entre les optimisations couramment utilisées pour régresser les paramètres d'une boîte englobante et la maximisation de cette métrique.\n",
    "\n",
    "\n",
    "\n",
    "Nous allons nous intéresser dans cette partie à évaluer notre modèle à l'aide de cette métrique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec :\n",
    "\n",
    "xA=max(x11,x21)\n",
    "xA=max(x11,x21)\n",
    " \n",
    "yA=max(y11,y21)\n",
    "yA=max(y11,y21)\n",
    " \n",
    "xB=min(x12,x22)\n",
    "xB=min(x12,x22)\n",
    " \n",
    "yB=min(y12,y22)\n",
    "yB=min(y12,y22)\n",
    " \n",
    "L'air de la zone d'intersection s'écrit :\n",
    "\n",
    "intersection=max(xB−xA,0)⋅max(yB−yA,0)\n",
    "Aintersection=max(xB−xA,0)⋅max(yB−yA,0)\n",
    " \n",
    "L'air de la zone d'union s'écrit :\n",
    "\n",
    "union=1+2−intersection\n",
    "Aunion=A1+A2−Aintersection\n",
    " \n",
    "C'est-à-dire :\n",
    "\n",
    "union=(x12−x11)⋅(y12−y11)+(x22−x21)⋅(y22−y21)−intersection\n",
    "Aunion=(x12−x11)⋅(y12−y11)+(x22−x21)⋅(y22−y21)−Aintersection\n",
    " \n",
    "La métrique \"intersection over Union\" s'écrit donc :\n",
    "\n",
    "IoU=intersectionunion\n",
    "IoU=AintersectionAunion\n",
    " \n",
    "Définir dans une fonction iou avec comme arguments y_true (de taille [n_img, 4]) et y_pred (de taille [n_img, 4]):\n",
    "Définir x1, y1, w1 et h1 les coordonnées de y_true.\n",
    "Définir x2, y2, w2 et h2 les coordonnées de y_pred. Appliquer la fonction valeur absolue à ses paramètres (lors du training, le modèle ne distingue pas les valeurs négatives des valeurs positives).\n",
    "Calculer xA, yA, xB et yB à l'aide de la fonction maximum de tensorflow.\n",
    "Calculer l'air de la zone d'intersection.\n",
    "Calculer l'air de la zone d'union.\n",
    "Retourner la valeur IoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculer la métrique IoU de notre modèle sur l'échantillon de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
