{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#123 - Méthodes de Clustering (Python) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Module 1 - Introduction -------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La différence distinct entre l'apprentissage supervisé à l'apprentissage non-supervisé est le fait que l'apprentissage non-supervisé cherche à trouver des partitions de modèles par lui-même.\n",
    "\n",
    "L'apprentissage non supervisé répond à différentes tâches :\n",
    "\n",
    "* Clustering (segmentation, regroupement) \n",
    "* Règles d'association \n",
    "* Réduction de dimensions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Clustering \n",
    "\n",
    "Le partitionnement de données (data clustering en anglais) est une des méthodes d'analyse des données. Elle vise à diviser un ensemble de données en différents groupes homogènes, en ce sens que les données de chaque sous-ensemble partagent des caractéristiques communes, qui correspondent le plus souvent à des critères de proximité (similarité) que l'on définit en introduisant des mesures et classes de distance entre objets. \n",
    " \n",
    "L'objectif du clustering est de construire des classes automatiquement en fonction des instances (ou des observations) disponibles.\n",
    "\n",
    "2 - Règles d'association\n",
    "\n",
    "Dans le domaine du data mining la recherche des règles d'association est une méthode populaire étudiée d'une manière approfondie dont le but est de découvrir des relations ayant un intérêt pour le statisticien entre deux ou plusieurs variables stockées dans de très importantes bases de données. Cette approche consiste à analyser les relations entre les variables ou détecter des associations\n",
    "\n",
    "3 - Réduction de dimensions\n",
    "\n",
    "La taille des données peut être mesurée selon deux dimensions, le nombre de variables et le nombre d'exemples. \n",
    "La sélection de sous-ensemble de caractéristiques permet d'éliminer les informations non-pertinentes et redondantes selon le critère utilisé. Cette sélection/extraction permet donc de réduire la dimension de l'espace des exemples et de rendre l'ensemble des données plus représentatif du problème. \n",
    "En effet, les principaux objectifs de la réduction de dimension sont :\n",
    "\n",
    "    *Faciliter la visualisation et la compréhension des données\n",
    "    *Réduire l'espace de stockage nécessaire\n",
    "    *Réduire le temps d'apprentissage et d'utilisation\n",
    "    *Identifier les facteurs pertinents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Module 2 - Clustering : classification non supervisée - K-Means  ---------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# L'algorithme de K-means : est facile à mettre en œuvre, et applicable à tout type et toute taille de données. \n",
    "# Cependant, le nombre de classe doit être fixé a priori,\n",
    "# et le résultat final est dépendant du tirage initial (aléatoire) des centres de classes.\n",
    "# Cette méthode n'est pas efficace quand les clusters se croisent.\n",
    "\n",
    "# A - Préparation et modélisation des données ---------------------------------------------------------------------------------\n",
    "\n",
    "# 1 ------------------------------------------ import \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Lecture du fichier \"ruspini.csv\"\n",
    "df = pd.read_csv(\"ruspini.csv\", index_col = 'indices')\n",
    "\n",
    "# Affichage des 5 premières lignes du dataset\n",
    "df.head(5)\n",
    "\n",
    "# 2 ------------------------------------------ Visualisation des données \n",
    "#Tracer le nuage des points ruspini et afficher le graphique\n",
    "\n",
    "plt.figure ( figsize= (16,6))\n",
    "plt.scatter ( df['x'],df['y'] )\n",
    "plt.xlabel ('x')\n",
    "plt.ylabel ('y')\n",
    "\n",
    "plt.title('ruspini')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 3 ------------------------------------------ Détection des valeurs aberrantes\n",
    "#Tracer les deux diagrammes en boîte des deux variables explicatives : x et y\n",
    "\n",
    "liste = [df['x'], df['y']]\n",
    "plt.figure()\n",
    "plt.title('Diagramme en boîte des deux variables explicatives')\n",
    "plt.boxplot(liste, labels = ['x', 'y'])  # Diagramme en boîte (boxplot) de tous les variables explicatives\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# B - Apprentissage des données -----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Le partitionnement en k-moyennes (ou k-means) est une méthode de clustering (regroupement de données) très utilisé \n",
    "# en apprentissage non supervisé. Étant donnés des points et un entier k, \n",
    "# l'algorithme vise à diviser les points en k groupes, appelés clusters, homogènes et compacts.\n",
    "\n",
    "clf = KMeans(n_clusters = 2)  #classificateur clf qui utilise 2 centres et construit au final 2 clusters\n",
    "clf.fit(df)  #exécute l'algorithme des k-moyennes sur le jeu de données data\n",
    "\n",
    "# Centroids and labels\n",
    "centroids = clf.cluster_centers_   # liste des positions des 2 centroïdes\n",
    "labels = clf.labels_   # vecteur qui contient le numéro du groupe de chacune des données\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# C - Visualisation des clusters ----------------------------------------------------------------------------------------------\n",
    "\n",
    "#Visualisation des clusters et des centroïdes \n",
    "\n",
    "# Liste des couleurs\n",
    "colors = [\"g.\",\"r.\"]\n",
    "\n",
    "# Graphique du nuage de points attribués au cluster correspondant\n",
    "for i in range(len(df)):\n",
    "    plt.plot(df.iloc[i,0], df.iloc[i,1], colors[labels[i]], markersize = 10)#\n",
    "\n",
    "# Graphique des centroïdes\n",
    "#plt.scatter(centroids[:, 0],centroids[:, 1], marker = \"o\", color = \"blue\",s=30, linewidths = 1, zorder = 10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# D - Méthode de coude ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# permet d'obtenir le meilleur partitionnement des données\n",
    "# Le choix du nombre de clusters k optimal est un arbitrage entre la compacité des clusters et leur séparation\n",
    "\n",
    "# 1 ------------------------------------------ Initialisation d'un cluster + entraîner l'algorithme KMeans + Calcul distortion\n",
    "\n",
    "# Importation de la fonction cdist du package scipy.spatial.distance\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]   # Liste des nombre de clusters\n",
    "\n",
    "distortions = []   # Initialisation de la liste de distortions\n",
    "\n",
    "for n_clusters in range_n_clusters:    # Calcul des distortions pour les différents modèles\n",
    "    \n",
    "    cluster = KMeans(n_clusters = n_clusters)   # Initialisation d'un cluster ayant un pour nombre de clusters n_clusters\n",
    "    \n",
    "    cluster.fit(df)  # Apprentissage des données suivant le cluster construit ci-dessus\n",
    "    \n",
    "    # Ajout de la nouvelle distortion à la liste des données\n",
    "    distortions.append(sum(np.min(cdist(df, cluster.cluster_centers_, 'euclidean'), axis=1)) / np.size(df, axis = 0))\n",
    "\n",
    "    \n",
    "# 2 ------------------------------------------ graphique des distortions en fonction du nombre de clusters\n",
    "    \n",
    "# Visualisation des distortions en fonction du nombre de clusters\n",
    "plt.plot(range_n_clusters, distortions, 'gx-')\n",
    "plt.xlabel('Nombre de Clusters K')\n",
    "plt.ylabel('Distortion (WSS/TSS)')\n",
    "plt.title('Méthode du coude affichant le nombre de clusters optimal')\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "#analyse :\n",
    "\n",
    "#La courbe obtenue décroit fortement et change de trajectoire après k = 4. Ainsi, le nombre de clusters optimal est 4.\n",
    "\n",
    "# 3 ------------------------------------------ reentrainer le cluster avec nbre cluster = 4*\n",
    "\n",
    "clf = KMeans(n_clusters = 4)  #classificateur clf qui utilise 4 centres / clusters\n",
    "clf.fit(df)  #exécute l'algorithme des k-moyennes sur le jeu de données data\n",
    "\n",
    "# Centroids and labels\n",
    "centroids = clf.cluster_centers_   # liste des positions des 2 centroïdes\n",
    "labels = clf.labels_   # vecteur qui contient le numéro du groupe de chacune des données\n",
    "\n",
    "# 4 ------------------------------------------ Visualisation des clusters et des centroïdes \n",
    "\n",
    "# Liste des coleurs\n",
    "colors = [\"g.\",\"r.\",\"c.\",\"y.\"]\n",
    "\n",
    "# Grphique du nuage de points attribués au cluster correspondant\n",
    "for i in range(len(df)):\n",
    "    plt.plot(df.iloc[i,0], df.iloc[i,1], colors[labels[i]], markersize = 10)\n",
    "\n",
    "# Graphique des centroïdes\n",
    "plt.scatter(centroids[:, 0],centroids[:, 1], marker = \"o\", color = \"blue\",s=30, linewidths = 1, zorder = 10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Module 3 - Clustering : classification non supervisée - Classification ascendante hiérarchique ---------------------------\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# La classification ascendante hiérarchique (CAH) a pour principal avantage de pouvoir visualiser le regroupement progressif \n",
    "# des données ainsi que l'augmentation de la dispersion dans un groupe produit par une agrégation, grâce au dendrogramme\n",
    "# Cependant, la CAH nécessite le calcul des distances entre chaque pair d'individus, \n",
    "# et peut donc s'avérer très longue dès que le nombre d'individus est élevé (1000+).\n",
    "\n",
    "\n",
    "# A - Préparation et modélisation des données ---------------------------------------------------------------------------------\n",
    "\n",
    "# 1 ------------------------------------------ import \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Lecture du fichier \"anim.txt\"\n",
    "df = pd.read_csv(\"fromage.txt\", sep = '\\t', index_col=0)\n",
    "\n",
    "# Affichage des 5 premières lignes du dataset\n",
    "df.head()\n",
    "\n",
    "# - 2 ------------------------------------------ Détection des valeurs aberrantes \n",
    "\n",
    "# Récupération des indices des colonnes \n",
    "ls_features = list(df.keys())   #df.keys() methode = df.columns attributs\n",
    "\n",
    "# Diagramme en boîte (boxplot) de toutes les variables explicatives\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('Diagrammes en boîte correspondants aux variables explicatives')\n",
    "plt.boxplot(df.values, 0, 'rD', labels = ls_features)\n",
    "# ou df.boxplot(column=list(df.columns))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# df.values recupere les valeurs et les retourne sous forme de liste\n",
    "\n",
    "\n",
    "#analyse : \n",
    "# variance observée dispersé ou concentré ou depend de la caractéristique observées\n",
    "# variance est une mesure de la dispersion\n",
    "\n",
    "\n",
    "# B - Apprentissage des données ----------------------------------------------------------------------------------------------\n",
    "\n",
    "# - 1 ------------------------------------------ Classification ascendante hiérarchique\n",
    "# méthode de partitionnement des données d'apprentissage non supervisé\n",
    "\n",
    "# Initialisation du classificateur CAH pour 4 clusters\n",
    "cluster = AgglomerativeClustering(n_clusters = 4)\n",
    "\n",
    "# Apprentissage des données \n",
    "cluster.fit(df[ls_features])\n",
    "\n",
    "# Calcul des labels du data set\n",
    "labels = cluster.labels_\n",
    "\n",
    "\n",
    "# - 2 ------------------------------------------ Dendrogramme\n",
    "\n",
    "#Effectuer un regroupement hiérarchique\n",
    "\n",
    "# Importation des packages nécessaires pour la CAH\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Initialisaion de la figrue\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Génération de la matrice des liens\n",
    "Z = linkage(df, method = 'ward', metric = 'euclidean')\n",
    "\n",
    "# Affichage du dendrogramme\n",
    "plt.title(\"Dendrogramme CAH\")\n",
    "dendrogram(Z, labels = df.index, leaf_rotation = 90., color_threshold = 290) #regroupement hiérarchique en dendrogramme,\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# - 3 ------------------------------------------ Evaluation du clustering\n",
    "\n",
    "# coefficient de silhouette pour le partitionnement\n",
    "\n",
    "from sklearn.metrics import silhouette_score   # Importation de la fonction silhouette_score du package sklearn.metrics\n",
    "\n",
    "silhouette_score(df, labels, metric='sqeuclidean')   # Calcul du coefficient silhouette\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   # Définition de la liste de n_clusters\n",
    "\n",
    "s_scores = []  # Initialisation de la liste des coefficients de la silhouette score\n",
    "\n",
    "\n",
    "# Calcul du coefficient de silhouette pour les différentes valeurs de n_clusters\n",
    "for n_clusters in range_n_clusters :\n",
    "    \n",
    "    cluster = AgglomerativeClustering(n_clusters = n_clusters)   # Initialisation du classificateur CAH pour n_clusters\n",
    "\n",
    "    cluster.fit(df[ls_features])    # Apprentissage des données \n",
    "\n",
    "    labels = cluster.labels_       # Calcul des labels du data set\n",
    "    \n",
    "    # Caclul du coefficient de silhouette\n",
    "    s_score = silhouette_score(df, labels, metric='sqeuclidean')\n",
    "    s_scores.append(s_score)\n",
    "\n",
    "\n",
    "# Graphique du coefficient de silhouette en fonction du nombre de clusters\n",
    "plt.plot(range_n_clusters, s_scores, 'bo-')\n",
    "plt.title('Graphique du coefficient de silhouette en fonction du nombre de clusters')\n",
    "plt.xlabel('Nombre de clusters')\n",
    "plt.ylabel('Coefficient de silhouette')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Module 4 - Clustering : classification non supervisée - Mean Shift -------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# \n",
    "\n",
    "# A - Préparation et modélisation des données ---------------------------------------------------------------------------------\n",
    "\n",
    "# 1 ------------------------------------------ import \n",
    "# Importation des packages nécessaires\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "# Lecture de l'image bird_small.png\n",
    "img = plt.imread(\"bird_small.png\")\n",
    "\n",
    "\n",
    "# 2 ------------------------------------------ Calculer et afficher les dimensions de l'image img\n",
    "\n",
    "# Calcul et affichage des dimensions de l'image img\n",
    "dimensions = img.shape\n",
    "print(\"dimensions de l'image bird_small.png : \", dimensions)\n",
    "\n",
    "\n",
    "# 3 ------------------------------------------ Affichage de l'image bird_small.png\n",
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 4 ------------------------------------------ Redimensionner l'image img en une matrice de dimensions\n",
    "X = img.reshape(img.shape[0]*img.shape[1], img.shape[2])\n",
    "\n",
    "\n",
    "\n",
    "# B - Compression des images --------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 1 ------------------------------------------ Algorithme de Mean Shift \n",
    "\n",
    "# Le principe de la compression des images consiste à trouver le centroïde le plus proche à chaque pixel. \n",
    "# Pour cela, il est nécessaire d'implémenter une fonction permettant de renvoyer les indices du centroïde \n",
    "# le plus proche de chaque observation de X.\n",
    "\n",
    "# Définition de get_clusters_centroids(X, quantile, n_samples)\n",
    "def get_clusters_centroids(X, quantile, n_samples) :\n",
    "    \n",
    "    # Bande passante\n",
    "    bandwidth = estimate_bandwidth(X, quantile = quantile, n_samples = n_samples)\n",
    "    \n",
    "    # Initilisation de l'objet Cluster\n",
    "    cluster = MeanShift(bandwidth = bandwidth)\n",
    "    \n",
    "    # Apprentissage des données\n",
    "    cluster.fit(X)\n",
    "    \n",
    "    return cluster.cluster_centers_, cluster.labels_   \n",
    "\n",
    "\n",
    "# 2 ------------------------------------------ fonction findClosestCentroids \n",
    "\n",
    "# Définition de la fonction findClosestCentroids\n",
    "\n",
    "def findClosestCentroids(X, centroids) :\n",
    "    \n",
    "    idx = []                                     # Initialisation du vecteur des indices\n",
    "    \n",
    "    ## \"Décommentez\" et complétez la ligne suivante\n",
    "    \n",
    "    #K =                                          # Calcul du nombre de clusters\n",
    "    \n",
    "    for i in range(0, np.size(X, axis=0)) :      # Parcourir tous les pixels de l'image\n",
    "        \n",
    "        norm_val = []                            # Initialisation du vecteur des distances du pixel i aux centroïdes \n",
    "        \n",
    "        for j in range(0, K) :                   # Parcourir tous les centroïdes\n",
    "            \n",
    "            a = ((X[i] - centroids[j])**2).sum() \n",
    "            norm_val.append(a)                   # Stockage de la distance du pixel i au centroïde j\n",
    "\n",
    "        ## \"Décommentez\" et complétez la ligne suivante            \n",
    "        \n",
    "        #I =                                      # Récupérer les indices du centroïde le plus proche au pixel i\n",
    "        \n",
    "        idx.append(I)                            # Ajout de des indices à la liste idx \n",
    "        \n",
    "    return idx\n",
    "\n",
    "\n",
    "# 3 ------------------------------------------ compression de l'image \n",
    "\n",
    "# Calcul des labels et des positions des centroïdes \n",
    "centroids, labels = get_clusters_centroids(X, 0.1, 300)\n",
    "\n",
    "# Calcul des indices du centroïde le plus proche à chaque élément de X\n",
    "idx = findClosestCentroids(X, centroids)\n",
    "\n",
    "# compression de l'image \n",
    "X_recovered = centroids[idx]\n",
    "\n",
    "\n",
    "# 4 ------------------------------------------ Afficher l'image originale et l'image reconstruite\n",
    "# Redimensionnement de la matrice X\n",
    "X_recovered = X_recovered.reshape(img.shape[0], img.shape[1], img.shape[2])\n",
    "\n",
    "# Affichage de l'image originale et celle reconstruite\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.title('Image originale')\n",
    "plt.subplot(122)\n",
    "plt.imshow(X_recovered)\n",
    "plt.title('Image reconstruite')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 5 ------------------------------------------ calculer le taux de compression\n",
    "# Importation des packages nécessaires\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "# Enregistrement de l'image reconstruite\n",
    "imageio.imwrite('bird_small_recovered.png', X_recovered)\n",
    "\n",
    "# Taille en octets de l'image originale\n",
    "original_size = os.stat('bird_small.png').st_size\n",
    "\n",
    "# Taille en octets de l'image reconstruite\n",
    "recovered_size = os.stat('bird_small_recovered.png').st_size\n",
    "\n",
    "# Caclul et affichage du ratio de compression\n",
    "compression_rate = round(recovered_size/original_size*100,2)\n",
    "print('Ratio de compression : ', compression_rate, '%')\n",
    "\n",
    "\n",
    "\n",
    "# C - Amélioration de la qualité de compression -------------------------------------------------------------------------------\n",
    "\n",
    "# 1 ------------------------------------------ Refaire la compression de l'image 'bird_small.png'\n",
    "\n",
    "# Calcul des labels et des positions des centroïdes en utilisant quantile = 0.01\n",
    "centroids2, labels2 = get_clusters_centroids(X, 0.01, 300)\n",
    "\n",
    "# Calcul des indices du centroïde le plus proche à chaque élément de X\n",
    "idx = findClosestCentroids(X, centroids2)\n",
    "\n",
    "# compression de l'image \n",
    "X2_recovered = centroids2[idx]\n",
    "\n",
    "# Redimensionnement de la matrice X\n",
    "X2_recovered = X2_recovered.reshape(img.shape[0], img.shape[1], img.shape[2])\n",
    "\n",
    "# Affichage de l'image originale et celle reconstruite\n",
    "plt.figure(figsize = (8, 16))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.title('Image originale')\n",
    "plt.subplot(122)\n",
    "plt.imshow(X2_recovered)\n",
    "plt.title('Image Reconstruite')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 2 ------------------------------------------ Calculer le nouveau taux de compression   (execution longue)\n",
    "# Enregistrement de l'image reconstruite\n",
    "imageio.imwrite('bird_small_recovered_2.png', X2_recovered)\n",
    "\n",
    "# Taille en octects de l'image reconstruite\n",
    "recovered_2_size = os.stat('bird_small_recovered_2.png').st_size\n",
    "\n",
    "# Caclul et affichage du ratio de compression\n",
    "compression_rate_2 = round(recovered_2_size/original_size*100,2)\n",
    "print('Ratio de compression : ', compression_rate_2, '%')\n",
    "\n",
    "\n",
    "# 3 ------------------------------------------ Afficher l'image originale et l'image reconstruite\n",
    "# Redimensionnement de la matrice X\n",
    "X_recovered = X_recovered.reshape(img.shape[0], img.shape[1], img.shape[2])\n",
    "\n",
    "# Affichage de l'image originale et celle reconstruite\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.title('Image originale')\n",
    "plt.subplot(122)\n",
    "plt.imshow(X_recovered)\n",
    "plt.title('Image reconstruite')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# D - Ratio de compression, évaluation de l'algorithme ------------------------------------------------------------------------\n",
    "\n",
    "# 1 ------------------------------------------ calculer le taux de compression\n",
    "\n",
    "# Importation des packages nécessaires\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "# Enregistrement de l'image reconstruite\n",
    "imageio.imwrite('bird_small_recovered.png', X_recovered)\n",
    "\n",
    "# Taille en octets de l'image originale\n",
    "original_size = os.stat('bird_small.png').st_size\n",
    "\n",
    "# Taille en octets de l'image reconstruite\n",
    "recovered_size = os.stat('bird_small_recovered.png').st_size\n",
    "\n",
    "# Caclul et affichage du ratio de compression\n",
    "compression_rate = round(recovered_size/original_size*100,2)\n",
    "print('Ratio de compression : ', compression_rate, '%')\n",
    "\n",
    "\n",
    "\n",
    "# E - Amélioration de la qualité de compression -------------------------------------------------------------------------------\n",
    "\n",
    "# 1 ------------------------------------------ Refaire la compression de l'image 'bird_small.png' \n",
    "#en améliorant la qualité de compression\n",
    "\n",
    "# Calcul des labels et des positions des centroïdes en utilisant quantile = 0.01\n",
    "centroids2, labels2 = get_clusters_centroids(X, 0.01, 300)\n",
    "\n",
    "# Calcul des indices du centroïde le plus proche à chaque élément de X\n",
    "idx = findClosestCentroids(X, centroids2)\n",
    "\n",
    "# compression de l'image \n",
    "X2_recovered = centroids2[idx]\n",
    "\n",
    "# Redimensionnement de la matrice X\n",
    "X2_recovered = X2_recovered.reshape(img.shape[0], img.shape[1], img.shape[2])\n",
    "\n",
    "# Affichage de l'image originale et celle reconstruite\n",
    "plt.figure(figsize = (8, 16))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.title('Image originale')\n",
    "plt.subplot(122)\n",
    "plt.imshow(X2_recovered)\n",
    "plt.title('Image Reconstruite')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 2 ------------------------------------------ Calculer le nouveau taux de compression\n",
    "\n",
    "# Enregistrement de l'image reconstruite\n",
    "imageio.imwrite('bird_small_recovered_2.png', X2_recovered)\n",
    "\n",
    "# Taille en octects de l'image reconstruite\n",
    "recovered_2_size = os.stat('bird_small_recovered_2.png').st_size\n",
    "\n",
    "# Caclul et affichage du ratio de compression\n",
    "compression_rate_2 = round(recovered_2_size/original_size*100,2)\n",
    "print('Ratio de compression : ', compression_rate_2, '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examen\n",
    "\n",
    "Clustering avec scikit-learn¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les deux principales méthodes de classification non-supervisée, vues au cours de la formation, sont la classification ascendante hiérarchique (CAH) et les méthodes de centres mobiles (K-Means). Cependant elles présentent toutes deux certains avantages et inconvénients :\n",
    "\n",
    "\n",
    "L'algorithme de K-means : est facile à mettre en œuvre, et applicable à tout type et toute taille de données. Cependant, le nombre de classe doit être fixé a priori, et le résultat final est dépendant du tirage initial (aléatoire) des centres de classes.\n",
    "\n",
    "\n",
    "La classification ascendante hiérarchique (CAH) a pour principal avantage de pouvoir visualiser le regroupement progressif des données ainsi que l'augmentation de la dispersion dans un groupe produit par une agrégation, grâce au dendrogramme. Il n'est pas nécessaire de définir le nombre de classes à l'avance, et le dendrogramme permet de se faire une idée du nombre adéquat de classes dans lesquelles les données peuvent être regroupées. Cependant, la CAH nécessite le calcul des distances entre chaque pair d'individus, et peut donc s'avérer très longue dès que le nombre d'individus est élevé (1000+).\n",
    "\n",
    "\n",
    "Une solution efficace pour bénéficier des avantages de ces deux méthodes en diminuant leurs inconvénients respectifs est le recours à la méthode de Classification Mixte.\n",
    "\n",
    "\n",
    "Elle se compose de trois étapes :\n",
    "\n",
    "\n",
    "Application de la méthode des K-Means pour obtenir rapidement un nombre assez élevé de classes homogènes. Une bonne pratique est de prendre un nombre de groupes = 1/10 du nombre d'individus.\n",
    "\n",
    "\n",
    "Classification ascendante hiérarchique à partir des classes obtenues pour conserver le choix du nombre de classes grâce au dendrogramme.\n",
    "\n",
    "\n",
    "Consolidation de la partition finale avec l'algorithme des K-Means à partir des barycentres des classes obtenus à l'étape précédente.\n",
    "\n",
    "\n",
    "L'objectif du test est de mettre en application un algorithme de classification mixte à partir des packages et méthodes étudiées dans la formation.\n",
    "\n",
    "\n",
    "Les données utilisées proviennent d'images de divers véhicules et contiennent des caractéristiques propres à leur silhouette. Le but de l'exercice est de réussir à classes les véhicules en plusieurs groupes, selon ces caractéristiques.\n",
    "\n",
    "\n",
    "Importer les modules pandas, matplotlib.pyplot ainsi que les sous-modules KMeanset AgglomerativeClusteringde sklearn.cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lire dans un data frame appelé sv_data, le fichier \"vehicles_silhouette.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_data = pd.read_csv(\"vehicles_silhouette.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher la description mathématique des différentes variables présentes dans les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_data.info()\n",
    "sv_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Représenter visuellement et comparer la distribution des variables, à l'aide par exemple d'un boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('distribution des variables')\n",
    "plt.boxplot(sv_data.values, 0, 'rD', labels = list(sv_data.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser le sous-module MinMaxScaler de sklearn.preprocessing pour normaliser les données à l'aide de la transformation Min-Max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "column=list(sv_data.columns)\n",
    "\n",
    "column\n",
    "\n",
    "scaler = MinMaxScaler ()\n",
    "sv_data [column] = scaler.fit_transform(sv_data [column]   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouper les données en reproduisant un algorithme de classification mixte , à l'aide des étapes suivantes :\n",
    "\n",
    "    - Appliquer l'algorithme des K-Means pour regrouper les données en 50 clusters.\n",
    "    - A partir des centres de gravité et labels obtenus pour chaque cluster, afficher un dendrogramme afin de choisir le nombre de clusters adéquat (>2).\n",
    "    - Appliquer un algorithme de classification ascendante hiérarchique à partir des centres de gravité obtenus à l'étape 1 avec le nombre de clusters obtenu à l'étape 2.\n",
    "    - Calculer les centres de gravité de chaque nouveau groupe.\n",
    "    - Utiliser les centres de gravité calculés pour consolider ces clusters par l'algorithme des K-Means. (L'argument init de KMeans permet de préciser les centres de gravité à partir desquels l'algorithme démarre.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appliquer l'algorithme des K-Means pour regrouper les données en 50 clusters.\n",
    "\n",
    "K = 50\n",
    "\n",
    "clf = KMeans(n_clusters = K )  \n",
    "clf.fit(sv_data) \n",
    "\n",
    "centroids = clf.cluster_centers_  \n",
    "labels = clf.labels_   \n",
    "\n",
    "\n",
    "\n",
    "# A partir des centres de gravité et labels obtenus pour chaque cluster, \n",
    "# afficher un dendrogramme afin de choisir le nombre de clusters adéquat (>2).\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "z = linkage ( centroids, method = \"ward\" , metric = \"euclidean\" )\n",
    "lab = list ( set ( labels ) )\n",
    "\n",
    "\n",
    "plt.title('CAH avec matérialisation des 4 classes')\n",
    "dendrogram(z , labels = lab , leaf_rotation = 90., color_threshold = 0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Appliquer un algorithme de classification ascendante hiérarchique \n",
    "#à partir des centres de gravité obtenus à l'étape 1 avec le nombre de clusters obtenu à l'étape 2.\n",
    "\n",
    "\n",
    "clf_ach = AgglomerativeClustering(n_clusters = 3) # instanciation\n",
    "clf_ach.fit(centroids) # clustering\n",
    "cah_labels = clf_ach.labels_\n",
    "\n",
    "\n",
    "# Calculer les centres de gravité de chaque nouveau groupe.\n",
    "sv_data[\"kmeans_labels\"] = labels\n",
    "sv_data[\"cah_labels\"] = sv_data[\"kmeans_labels\"].apply(lambda x : cah_labels[x])\n",
    "\n",
    "cah_centroids = sv_data.drop(\"kmeans_labels\", axis=1).groupby(\"cah_labels\").mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Utiliser les centres de gravité calculés pour consolider ces clusters par l'algorithme des K-Means. \n",
    "# (L'argument init de KMeans permet de préciser les centres de gravité à partir desquels l'algorithme démarre.)\n",
    "\n",
    "\n",
    "kmeans_cah = KMeans(n_clusters=3, init=cah_centroids)\n",
    "kmeans_cah.fit(sv_data[cah_centroids.columns])\n",
    "sv_data[\"k_a_labels\"] = kmeans_cah.labels_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les classes des véhicules présents dans le data frame sont en réalité connus et présents dans le fichier classes_vehicles.csv. Les véhicules sont classés comme 'car', 'bus' ou 'van'.\n",
    "\n",
    "Lire le fichier contenant les classes réelles des véhicules\n",
    "\n",
    "À l'aide d'une matrice de confusion, expliquer votre choix de correspondance entre les classes obtenues par clustering et les classes de véhicules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read.csv(\"classes_vehicles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(sv_data[\"k_a_labels\"], classes.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
