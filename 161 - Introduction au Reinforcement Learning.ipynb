{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction au Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques définitions :\n",
    "\n",
    "Apprentissage supervisé : les données sont labélisées et nous essayons de construire un modèle capable de prédire ces labels.\n",
    "\n",
    "Apprentissage non-supervisé : les données ne sont pas labélisées et nous essayons de construire un modèle capable de déduire des informations sur la façon dont ces données sont structurées.\n",
    "\n",
    "Il existe également un troisième type d'apprentissage : le Reinforcement Learning ou l'apprentissage par renforcement, en français. Dans ce configuration, l'objectif est de constuire un algorithme capable d'apprendre directement de son expérience : il se trouve dans un certain environnement et il essaie d'apprendre par lui-même quel chemin prendre pour atteindre un objectif prédéfini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------------------------------------------------------\n",
    "from gridworld.agents import RandomAgent\n",
    "from gridworld.environments import VanillaGridWorld, OneVsOneGridWorld, WindyGridWorld, CliffGridWorld\n",
    "\n",
    "\n",
    "#Instantier un Agent RandomAgent\n",
    "#----------------------------------------------------------\n",
    "agent1 = RandomAgent(prefix='vanilla')\n",
    "agent2 = RandomAgent(prefix='wumpus')\n",
    "agent3 = RandomAgent(prefix='windy')\n",
    "agent4 = RandomAgent(prefix='cliff')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# instantier chaque environnement avec l'Agent correspondant \n",
    "#sous les noms vanilla_env, wumpus_env, windy_env et cliff_env.\n",
    "#----------------------------------------------------------\n",
    "vanilla_env = VanillaGridWorld(agent=agent1)\n",
    "wumpus_env = OneVsOneGridWorld(agent=agent2)\n",
    "windy_env = WindyGridWorld(agent=agent3)\n",
    "cliff_env = CliffGridWorld(agent=agent4)\n",
    "\n",
    "\n",
    "\n",
    "# visualiser ce qu'il s'est passé\n",
    "#----------------------------------------------------------\n",
    "vanilla_env.display_in_jupyter()\n",
    "\n",
    "#Afficher les épisodes \n",
    "#----------------------------------------------------------\n",
    "wumpus_env.display_in_jupyter()\n",
    "windy_env.display_in_jupyter()\n",
    "cliff_env.display_in_jupyter()\n",
    "\n",
    "\n",
    "\n",
    "#Observation, Récompense, Gain\n",
    "#----------------------------------------------------------\n",
    "# importer et utiliser la fonction getsource \n",
    "# qui affiche le contenu des méthodes de vanilla_env\n",
    "\n",
    "from inspect import getsource\n",
    "\n",
    "print(getsource(vanilla_env.run_episode))\n",
    "\n",
    "print(getsource(vanilla_env.step))\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------\n",
    "print(getsource(agent1.ve))\n",
    "\n",
    "print(getsource(agent1.act))\n",
    "\n",
    "print(agent1.set_of_actions)\n",
    "\n",
    "#La fonction d'observation est très importante \n",
    "#car c'est celle qui permet à l'Agent d'apprendre.\n",
    "\n",
    "\n",
    "#agent_position : la position actuelle de l'Agent sur la grille.\n",
    "\n",
    "#reward : une récompense reçue par l'Agent suite à sa dernière action.\n",
    "\n",
    "#breeze : une variable booléenne avec une valeur True si un trou est \n",
    "#proche de l'Agent.\n",
    "\n",
    "#smell : une variable booléenne avec une valeur True si un Wumpus est \n",
    "#proche de l'Agent.\n",
    "\n",
    "#end_episode : une variable booléenne avec la valeur True si l'Agent est mort.\n",
    "\n",
    "#info : une chaîne de caractère donnant des informations sur la façon \n",
    "#dont l'Agent est mort.\n",
    "\n",
    "\n",
    "\n",
    "#Politique\n",
    "#----------------------------------------------------------\n",
    "#La politique est la façon dont l'Agent choisit son action. \n",
    "#Il s'agit essentiellement du contenu de la méthode agir.\n",
    "\n",
    "from gridworld.agents import Agent #iport \n",
    "new_agent = Agent(prefix='new_agent')  #instance d agent \n",
    "\n",
    "# new_agent.act()\n",
    "\n",
    "\n",
    "#----------------------------------------------------------\n",
    "import numpy as np \n",
    "def act():\n",
    "    return np.random.choice(['down', 'right'])\n",
    "\n",
    "new_agent.act = act\n",
    "new_agent.act()\n",
    "\n",
    "\n",
    "#Instancier un environnement via VanillaGridWorld avec cet Agent, \n",
    "#lancer un épisode et l'afficher.\n",
    "#----------------------------------------------------------\n",
    "new_env = VanillaGridWorld(agent=new_agent)\n",
    "\n",
    "new_env.run_episode()\n",
    "\n",
    "new_env.display_in_jupyter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 2 : Les mathématiques derrière l'apprentissage par renforcement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions, états, politique - la mécanique derrière\n",
    "\n",
    "\n",
    "Au cours d'un épisode, chaque pas de temps est indexé par t :  t∈[0, T] , où  T  est le nombre maximal de pas de temps atteignable au cours d'un épisode.\n",
    "\n",
    "L'ensemble de tous les états possibles est représenté par :  S={s∈S}\n",
    "\n",
    "L'ensemble de toutes les actions possibles est représenté par :  A={a∈A}\n",
    "La politique suivie par l'Agent est représentée par  π .\n",
    "\n",
    "\n",
    "\n",
    "L'une des hypothèses les plus importantes du Reinforcement Learning est celle selon laquelle l'environnement de l'Agent suit un processus de décision de Markov (MDP), ce qui signifie que les actions et les états reposent uniquement sur une partie de l'histoire de l'Agent. En général, nous supposons que l'état et l'action actuels sont suffisants pour déterminer l'action et l'état suivants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Importer VanillaGridWorld de gridworld.environments \n",
    "#et LoudyAgent de gridworld.agents.\n",
    "\n",
    "#Instancier une environnement VanillaGridWorld de taille 3 \n",
    "#et un Agent Loudyagent.\n",
    "#----------------------------------------------------------\n",
    "from gridworld.agents import LoudyAgent\n",
    "from gridworld.environments import VanillaGridWorld\n",
    "\n",
    "agent = LoudyAgent()\n",
    "env = VanillaGridWorld(agent=agent, grid_size=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Récompense vs. Gain\n",
    "#----------------------------------------------------------\n",
    "#A chaque pas de temps, l'Agent reçoit une récompense notée  Rt\n",
    "#G = somme (Rt)\n",
    "\n",
    "\n",
    "#Lancer un épisode de l'environnement et analyser les résultats\n",
    "#--------------------- \n",
    "env.run_episode()\n",
    "\n",
    "#Visualiser une épisode via la méthode display_in_jupyter\n",
    "#---------------------\n",
    "env.display_in_jupyter()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Les fonctions de valeur\n",
    "#----------------------------------------------------------\n",
    "#La fonction de valeur  V  est quelque chose de très important : \n",
    "#    elle nous donne le rendement \n",
    "#    attendu d'un état dans le cadre d'une politique donnée. \n",
    "\n",
    "\n",
    "# variable verbose de l'Agent env.agent à 2 et lancer un épisode\n",
    "#---------------------\n",
    "env.agent.verbose = 2\n",
    "\n",
    "env.run_episode()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Prédiction vs. Contrôle\n",
    "#----------------------------------------------------------\n",
    "#Prédiction : le but des problèmes de prédiction est \n",
    "#    d'apprendre à quel point une politique donnée est performante.\n",
    "\n",
    "#Contrôle : le but des problèmes de contrôle est \n",
    "#    d'apprendre une politique qui sera performante.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Exploration vs. Exploitation\n",
    "#----------------------------------------------------------\n",
    "#L'exploration correspond au fait que nous essayons de nouvelles actions \n",
    "#        et nous tentons d'explorer de nouveaux états. \n",
    "#        Cela nous permet d'augmenter la probabilité de trouver \n",
    "#        un meilleur moyen d'atteindre notre objectif.\n",
    "        \n",
    "        \n",
    "#Sous le mode exploitation, nous supposons, au contraire, que le modèle \n",
    "#        utilisé pour prédire l'action est suffisamment bon pour l'utiliser.\n",
    "\n",
    "\n",
    "\n",
    "#Afficher l'ensemble des actions possibles de l'Agent en utilisant \n",
    "#l'attribut set_of_actions\n",
    "#---------------------\n",
    "print(agent.set_of_actions)\n",
    "\n",
    "\n",
    "\n",
    "#Créer une matrice optimal_Q de taille 3 x 3 x nombre_actions contenant 1 et 0 \n",
    "#qui donnera la fonction  Q  optimale greedy\n",
    "#---------------------\n",
    "\n",
    "import numpy as np\n",
    "optimal_Q = np.zeros(shape=(3,3,4))\n",
    "optimal_Q[0, 0, 1] = 1  # if in position (0, 0) go down\n",
    "optimal_Q[1, 0, 1] = 1  # if in position (1, 0) go down\n",
    "optimal_Q[2, 0, 2] = 1  # if in position (2, 0) go right\n",
    "optimal_Q[2, 1, 2] = 1  # if in position (2, 1) go right\n",
    "\n",
    "\n",
    "\n",
    "#Fixer l'attribut Q de env.agent à optimal_Q\n",
    "#---------------------\n",
    "env.agent.Q = optimal_Q\n",
    "\n",
    "\n",
    "#Mettez env.agent.EPSILON à 0, puis lancez un épisode et affichez-le\n",
    "#---------------------\n",
    "env.agent.EPSILON = 0\n",
    "\n",
    "env.run_episode(max_length=5)\n",
    "env.display_in_jupyter()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 3 : Monte Carlo Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Itération sur politique de Monte Carlo\n",
    "#---------------------\n",
    "Cet algorithme vise à calculer une valeur précise du rendement attendu pour  QQ  pour chaque état et action.\n",
    "\n",
    "Il donne en fait une estimation moyenne de la valeur  QQ  de la politique pour chaque couple état-action.\n",
    "\n",
    "Après n épisodes,\n",
    "\n",
    "Q(s,a)←G(s,a)¯n \n",
    "G(s,a)¯n  est le rendement moyen de l'action  a  dans l'état  s  sur les n premiers épisodes.\n",
    "\n",
    "Notez que si un couple état-action est visité plusieurs fois dans un même épisode, cela nous donne plusieurs mises à jour.\n",
    "\n",
    "Comme vous l'avez remarqué, la mise à jour de la fonction  Q  n'est effectuée qu'à la fin de l'épisode. \n",
    "\n",
    "Notre Agent n'a que 3 méthodes :\n",
    "\n",
    "reset : exécutée au début de chaque épisode.\n",
    "\n",
    "act : exécuté à chaque étape, en choisissant l'action à réaliser.\n",
    "\n",
    "observe : exécuté à chaque étape, prenant comme arguments des informations sur la position de l'Agent et sur la récompense.\n",
    "\n",
    "Par ailleurs, l'Agent garde une trace de ses actions et de ses états passés via l'appel de la méthode act. \n",
    "De même, il garde en mémoire la valeur de ses récompenses via l'appel de la méthode observe. La mise à jour de la fonction  Q et de  ϵ  est effectuée via la méthode reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### # Notre environnement\n",
    "#---------------------------------------------------------- \n",
    "\n",
    "#Importer Agent et RandomAgent de gridworld.agents \n",
    "#et instancier un Agent RandomAgent.\n",
    "#---------------------\n",
    "from gridworld.agents import Agent, RandomAgent\n",
    "random_agent = RandomAgent()\n",
    "\n",
    "#Importer VanillaGridWorld de gridworlds et l'instancier avec l'Agent.\n",
    "#---------------------\n",
    "from gridworld.environments import VanillaGridWorld\n",
    "\n",
    "env = VanillaGridWorld(agent=random_agent)\n",
    "\n",
    "#Lancer un épisode et visualiser le.\n",
    "#---------------------\n",
    "env.run_episode()\n",
    "\n",
    "env.display_in_jupyter()\n",
    "\n",
    "\n",
    "\n",
    "# Itération sur politique de Monte Carlo\n",
    "#---------------------------------------------------------- \n",
    "\n",
    "#afficher correctement le pseudo-code de l'algorithme.\n",
    "#---------------------\n",
    "from IPython.display import HTML\n",
    "HTML('''<style>\n",
    "    .pseudo-code-indent {\n",
    "    margin-left:20px;\n",
    "    border-left-width: 5px;\n",
    "    border-left-style: solid;\n",
    "    border-left-color: #75DFC1;\n",
    "    padding-left:10px;\n",
    "    margin-bottom: 10px;\n",
    "    font-size: 15px;\n",
    "    font-family: times;\n",
    "    }\n",
    "   .pseudo-code-container {\n",
    "    margin:20px;\n",
    "    padding:20px;\n",
    "    border-width:5px;\n",
    "    border-radius:5px;\n",
    "    border-color:#40c6a0;\n",
    "    border-style: solid;\n",
    "    }\n",
    "    \n",
    "</style>''')\n",
    "\n",
    "\n",
    "\n",
    "# Implémentation de l'Agent\n",
    "#---------------------------------------------------------- \n",
    "\n",
    "#Implémenter la méthode reset\n",
    "#---------------------\n",
    "import numpy as np \n",
    "\n",
    "class MonteCarloAgent(Agent):\n",
    "    def __init__(self, prefix='MC_GLIE', grid_size=(5, 5)):\n",
    "        \n",
    "        # préfixe pour nommer les vidéos\n",
    "        self.prefix = prefix \n",
    "        \n",
    "        # définition de l'ensemble d'actions\n",
    "        self.set_of_actions = ['up', 'down', 'left', 'right']\n",
    "        \n",
    "        # valeur d'epsilon pour la politique epsilon-greedy\n",
    "        self.EPSILON = 1.\n",
    "        \n",
    "        # facteur d'actulisation\n",
    "        self.GAMMA = .8\n",
    "        \n",
    "        # initialisation de la matrice Q\n",
    "        self.Q_function = np.random.uniform(size=(grid_size[0], grid_size[1], len(self.set_of_actions)))\n",
    "        \n",
    "        # création d'une matrice N\n",
    "        self.N = np.zeros(shape=(grid_size[0], grid_size[1], len(self.set_of_actions)))\n",
    "        \n",
    "        # initialisation de la position\n",
    "        self.position = (0, 0)\n",
    "        \n",
    "        # listes pour garder une trace des actions, états et récompenses \n",
    "        # dans l'ordre chronologique\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        # initialisation du compteur d'épisodes\n",
    "        self.episode = 0\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def act(self):\n",
    "        # ajout de la position actuelle à l'historique des positions\n",
    "        self.states.append(self.position)\n",
    "        \n",
    "        # choix de l'action selon la politique epsilon-greedy\n",
    "        if np.random.uniform() < self.EPSILON:\n",
    "            action = np.random.choice(self.set_of_actions)\n",
    "        else : \n",
    "            action = self.set_of_actions[np.argmax(self.Q_function[self.position[0], self.position[1]])]\n",
    "        \n",
    "        # ajout de l'action actuelle à l'historique des actions\n",
    "        self.actions.append(action)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def observe(self, agent_position, reward, breeze, smell, end_episode, info): \n",
    "        # ajouter de la récompense à l'historique des récompenses\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "        # mise à jour de la position actuelle\n",
    "        self.position = agent_position\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        # incrémentation du nombre d'épisodes\n",
    "       \n",
    "        # calcul des rendements\n",
    "\n",
    "        # calcul et mise à jour des variables Q et N\n",
    "        \n",
    "        # mise à jour d'epsilon\n",
    "        \n",
    "        # ré-initialisation des historiques\n",
    "                             \n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "# Entraînement de l'Agent\n",
    "#---------------------------------------------------------- \n",
    "\n",
    "#Instancier un Agent MonteCarloAgent, nom agent_MC, (taille grille grid_size = 10)\n",
    "#Instancier un environnement VanillaGridWorld, nom env, (taille de grille grid_size = 10)\n",
    "#---------------------\n",
    "agent_MC = MonteCarloAgent(prefix='MC', grid_size=(10, 10))\n",
    "env = VanillaGridWorld(agent=agent_MC, grid_size=10)\n",
    "\n",
    "\n",
    "#Exécuter un épisode et le visualiser en utilisant \n",
    "#la méthode display_in_jupyter avec l'argument episode_number fixé à 1.\n",
    "#---------------------\n",
    "env.run_episode()\n",
    "env.display_in_jupyter(episode_number=1)\n",
    "\n",
    "#lancer 5000 épisodes avec l'argument write fixé à False\n",
    "#---------------------\n",
    "for i in range(5000):\n",
    "    env.run_episode(write=False)\n",
    "\n",
    "    \n",
    "    \n",
    "#Résultats\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#Afficher la valeur de EPSILON \n",
    "#qui est un attribut de l'Agent dans son environnement.\n",
    "#---------------------\n",
    "print(env.agent.EPSILON)\n",
    "\n",
    "#fixer temporairement  ϵ  à 0 \n",
    "#et donc à lancer un épisode avec une politique greedy (et afficher)\n",
    "#---------------------\n",
    "env.display_greedy_episode()\n",
    "\n",
    "\n",
    "#Les inconvénients des méthodes de Monte Carlo\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#L'algorithme implémenté présente certains inconvénients. \n",
    "#Parmi eux figure le fait que pour fonctionner, \n",
    "#il doit réussir à mettre fin à un épisode en trouvant la pièce d'or.\n",
    "\n",
    "#Importer WindyGridWorld de gridworlds.py.\n",
    "#Instancier un Agent Monte Carlo avec une taille de grille grid_size fixée à (7, 10) et un préfixe prefix fixé à 'windy-MC'.\n",
    "#Instancier un object de classe WindyGridWorldavec cet Agent.\n",
    "#---------------------\n",
    "from gridworld.environments import WindyGridWorld\n",
    "agent = MonteCarloAgent(prefix='windy-MC', grid_size=(7, 10))\n",
    "env = WindyGridWorld(agent=agent)\n",
    "\n",
    "#Exécuter un épisode et visualiser le\n",
    "#---------------------\n",
    "\n",
    "env.run_episode()\n",
    "env.display_in_jupyter()\n",
    "\n",
    "#Lancer 1000 épisodes de l'environnement avec l'argument write fixé à False.\n",
    "#---------------------\n",
    "for i in range(1000):\n",
    "    env.run_episode(write=False)\n",
    "    \n",
    "#Appeler la méthode display_greedy_episode de l'environnement.\n",
    "#---------------------\n",
    "env.display_greedy_episode()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 4 : Temporal Difference Learning : SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "# Notre environnement \n",
    "\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#Importer Agent et \n",
    "#RandomAgent de gridworld.agents et \n",
    "#instancier un Agent random_agent de classe RandomAgent.\n",
    "#---------------------\n",
    "from gridworld.agents import Agent, RandomAgent\n",
    "\n",
    "random_agent = RandomAgent()\n",
    "\n",
    "\n",
    "#Importer VanillaGridWorld de gridworld.environments et \n",
    "#instancier env avec l'Agent précédent.\n",
    "#---------------------\n",
    "from gridworld.environments import VanillaGridWorld\n",
    "\n",
    "env = VanillaGridWorld(agent=random_agent)\n",
    "\n",
    "#Lancer un épisode et visualiser le.\n",
    "#---------------------\n",
    "env.run_episode()\n",
    "\n",
    "env.display_in_jupyter()\n",
    "\n",
    "\n",
    "\n",
    "#SARSA \n",
    "#----------------------------------------------------------\n",
    "from IPython.display import HTML\n",
    "HTML('''<style>\n",
    "    .pseudo-code-indent {\n",
    "    margin-left:20px;\n",
    "    border-left-width: 5px;\n",
    "    border-left-style: solid;\n",
    "    border-left-color: #75DFC1;\n",
    "    padding-left:10px;\n",
    "    margin-bottom: 10px;\n",
    "    font-size: 15px;\n",
    "    }\n",
    "   .pseudo-code-container {\n",
    "    margin:20px;\n",
    "    padding:20px;\n",
    "    border-width:5px;\n",
    "    border-radius:5px;\n",
    "    border-color:#40c6a0;\n",
    "    border-style: solid;\n",
    "    font-family: times;\n",
    "    }\n",
    "    \n",
    "</style>''')\n",
    "\n",
    "\n",
    "\n",
    "#Implémentation de l'Agent \n",
    "#----------------------------------------------------------\n",
    "\n",
    "# implémenter la méthode observe.\n",
    "#---------------------\n",
    "\n",
    "from gridworld.agents import Agent\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "class SarsaAgent(Agent):\n",
    "    def __init__(self, prefix='SARSA', grid_size=(7,10), starting_position=(3, 0), gold_position = (3,7), epsilon=0.5, gamma=.8, holes_positions=[]):\n",
    "\n",
    "        # préfixe pour nommer les vidéos\n",
    "        self.prefix = prefix\n",
    "\n",
    "        # définition de l'ensemble d'actions\n",
    "        self.set_of_actions = ['up', 'down', 'right', 'left']\n",
    "        self.number_of_actions = len(self.set_of_actions)\n",
    "\n",
    "        # position de départ\n",
    "        self.starting_position = starting_position\n",
    "\n",
    "        # initialisation de la matrice Q\n",
    "        self.Q_function = np.random.uniform(size=(grid_size[0], grid_size[1], self.number_of_actions))\n",
    "        \n",
    "        # attribution d'une valeur nulle pour la valeur Q \n",
    "        ## de l'état terminal en raison de la pièce d'or\n",
    "        self.Q_function[gold_position[0], gold_position[1]] = 0\n",
    "        ## de l'état terminal en raison des trous\n",
    "        for hole in holes_positions:\n",
    "            self.Q_function[hole[0], hole[1]] = 0\n",
    "\n",
    "        # paramètres SARSA\n",
    "        self.ALPHA = 1\n",
    "        self.GAMMA = gamma\n",
    "        self.EPSILON = epsilon\n",
    "\n",
    "        # paramètres d'apprentissage \n",
    "        ## actions\n",
    "        self.current_action = None\n",
    "        self.previous_action = None\n",
    "        ## récompenses\n",
    "        self.current_reward = None\n",
    "        self.previous_reward = None\n",
    "        ## positions\n",
    "        self.current_position = self.starting_position\n",
    "        self.previous_position = None\n",
    "        \n",
    "        # initialisation du compteur d'épisodes\n",
    "        self.step_number = 0\n",
    "\n",
    "        pass\n",
    "\n",
    "    def act(self):\n",
    "        \"\"\"\n",
    "        renvoie une action parmi l'ensemble des actions possibles \n",
    "            - suit une politique d'epsilon-greedy \n",
    "            - met à jour les paramètres de l'action en cours et de l'action précédente\n",
    "        \"\"\"\n",
    "        \n",
    "        # mise à jour de l'action précédente\n",
    "        self.previous_action = self.current_action\n",
    "\n",
    "        # choix de l'action basée sur une politique epsilon-greedy \n",
    "        if np.random.uniform() < self.EPSILON:\n",
    "            action = np.random.choice(self.set_of_actions)\n",
    "        else:\n",
    "            action_index = np.argmax(self.Q_function[self.current_position[0], self.current_position[1]])\n",
    "            action = self.set_of_actions[action_index]\n",
    "    \n",
    "        # mise à jour d'action actuelle\n",
    "        self.current_action = action\n",
    "        \n",
    "        # augmentation du compteur \n",
    "        self.step_number+=1\n",
    "        \n",
    "        # mise à jour de alpha\n",
    "        self.ALPHA = 1/self.step_number\n",
    "      \n",
    "        return action\n",
    "\n",
    "    def observe(self, agent_position, reward, breeze, smell, end_episode, info):\n",
    "        \n",
    "        # obtention de l'indice A'\n",
    "\n",
    "        if self.previous_action is not None:\n",
    "\n",
    "            # obtention de l'indice A\n",
    "            \n",
    "            # obtention de Q(S,A)\n",
    "            \n",
    "            # obtention de Q(S',A')\n",
    "            \n",
    "            # calcul de la mise à jour\n",
    "            \n",
    "            pass\n",
    "\n",
    "            \n",
    "        # mise à jour des valeurs  \n",
    "        # de la position précédente\n",
    "        \n",
    "        # de la position en cours\n",
    "        \n",
    "        # de la récompense précédente\n",
    "        \n",
    "        # de la récompense en cours\n",
    "        \n",
    "        \n",
    "        # mise à jour si l'épisode se termine\n",
    "        \n",
    "\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self):\n",
    "        self.previous_action = None\n",
    "        self.previous_reward = None\n",
    "        self.previous_position = None\n",
    "        self.current_action = None\n",
    "        self.current_reward = None\n",
    "        self.current_position = self.starting_position\n",
    "        self.step_number = 1\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "#nstancier un Agent SarsaAgent avec \n",
    "#    une taille de grille de (10, 10) ,\n",
    "#    une position initiale starting_position de (0, 0) et \n",
    "#    la position de la pièce d'or fixée à (9, 9)\n",
    "#Initialiser un environnementVanillaGridWorld avec cet Agent.\n",
    "#---------------------\n",
    "agent = SarsaAgent(prefix='sarsa', grid_size=(10, 10), starting_position=(0, 0), gold_position=(9, 9))\n",
    "env = VanillaGridWorld(agent=agent)\n",
    "\n",
    "#ancer 1000 épisode avec l'argument write fixé à False.\n",
    "#---------------------\n",
    "for i in range(1000):\n",
    "     env.run_episode(write=False)\n",
    "\n",
    "\n",
    "# Résultats \n",
    "#----------------------------------------------------------\n",
    "\n",
    "# Appeler la méthode display_greedy_episode de l'environnement\n",
    "#---------------------\n",
    "env.display_greedy_episode()\n",
    "\n",
    "\n",
    "# Avantages par rapport à la méthode de Monte Carlo \n",
    "#----------------------------------------------------------\n",
    "\n",
    "\n",
    "#Importer WindyGridWorld de gridworld.environments.\n",
    "#Créer un Agent RandomAgent, \n",
    "#un environnement WindyGridWorld et visualiser un épisode.\n",
    "#---------------------\n",
    "from gridworld.environments import WindyGridWorld\n",
    "windy_env = WindyGridWorld(agent=RandomAgent())\n",
    "\n",
    "windy_env.run_episode()\n",
    "windy_env.display_in_jupyter(width=500, height=300)\n",
    "\n",
    "\n",
    "#Créer un Agent SarsaAgent et \n",
    "#initialiser un environnement WindyGridWorld avec cet Agent.\n",
    "#---------------------\n",
    "agent = SarsaAgent(prefix='sarsa-windy', epsilon=.5)\n",
    "env = WindyGridWorld(agent=agent)\n",
    "\n",
    "\n",
    "#Entraîner l'Agent durant 1000 épisodes avec l'argument write fixé à False.\n",
    "#---------------------\n",
    "for i in range(1000):\n",
    "    env.run_episode(write=False)\n",
    "\n",
    "#Appeler la méthode display_greedy_episode avec cet environnement\n",
    "#---------------------\n",
    "env.display_greedy_episode()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 5 : Temporal Difference Learning : Q-Learning¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, nous pouvons constater qu'il y a une légère différence :\n",
    "\n",
    "l'Agent SARSA prend un chemin plus long pour arriver au but. Il agit avec beaucoup de précaution.\n",
    "l'Agent QAgent est moins prudent mais il trouve le chemin optimal.\n",
    "Cet exemple est très intéressant et montre la principale différence entre ces deux algorithmes.\n",
    "\n",
    "Comme l'Agent SARSA repose sur un algorithme on-policy and que sa politique est  ϵϵ -greedy, il améliorera sa fonction  QQ , sachant que le caractère aléatoire de son comportement pourrait le faire tomber de la falaise. C'est pourquoi il va si loin de la falaise. En revanche, l'Agent Q-Learning apprend à chaque fois en considérant la meilleure action à prendre, indépendamment du fait de son choix de prendre cette action ou pas dans les faits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notre environnement\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#Importer Agent et RandomAgent de gridworld.agents et \n",
    "#instancier un Agent RandomAgent sous le nom random_agent.\n",
    "#---------------------\n",
    "from gridworld.agents import Agent, RandomAgent\n",
    "\n",
    "\n",
    "#Importer VanillaGridWorld de gridworld.environments et \n",
    "#instancier env avec l'Agent précédent.\n",
    "#---------------------\n",
    "from gridworld.environments import VanillaGridWorld\n",
    "\n",
    "env = VanillaGridWorld(agent=random_agent)\n",
    "\n",
    "\n",
    "#Lancer un épisode et visualiser le.\n",
    "#---------------------\n",
    "env.run_episode()\n",
    "\n",
    "env.display_in_jupyter()\n",
    "\n",
    "\n",
    "#Q-learning \n",
    "#----------------------------------------------------------\n",
    "#Le premier algorithme que nous allons voir est appelé Q-Learning.\n",
    "#---------------------\n",
    "from IPython.display import HTML\n",
    "HTML('''<style>\n",
    "    .pseudo-code-indent {\n",
    "    margin-left:20px;\n",
    "    border-left-width: 5px;\n",
    "    border-left-style: solid;\n",
    "    border-left-color: #75DFC1;\n",
    "    padding-left:10px;\n",
    "    margin-bottom: 10px;\n",
    "    font-size: 15px;\n",
    "    font-family: times;\n",
    "    }\n",
    "   .pseudo-code-container {\n",
    "    margin:20px;\n",
    "    padding:20px;\n",
    "    border-width:5px;\n",
    "    border-radius:5px;\n",
    "    border-color:#40c6a0;\n",
    "    border-style: solid;\n",
    "    font-size: 15px;\n",
    "    font-family: times;\n",
    "    }\n",
    "    \n",
    "</style>''')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Implémentation de l'Agent \n",
    "#----------------------------------------------------------\n",
    "\n",
    "#Implémenter l'algorithme de Q-Learning en complétant la méthode observe.\n",
    "#---------------------\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, prefix='QAgent', grid_size=(7,10), starting_position=(3, 0), gold_position = (3,7), epsilon=0.5, gamma=.8, holes_positions=[]):\n",
    "\n",
    "        # préfixe pour nommer les vidéos\n",
    "        self.prefix = prefix\n",
    "\n",
    "        # définition de l'ensemble d'actions\n",
    "        self.set_of_actions = ['up', 'down', 'right', 'left']\n",
    "        self.number_of_actions = len(self.set_of_actions)\n",
    "\n",
    "        # position de départ\n",
    "        self.starting_position = starting_position\n",
    "\n",
    "        # initialisation de la matrice Q\n",
    "        self.Q_function = np.random.uniform(size=(grid_size[0], grid_size[1], self.number_of_actions))\n",
    "\n",
    "        # attribution d'une valeur nulle pour la valeur Q \n",
    "        ## de l'état terminal en raison de la pièce d'or\n",
    "        self.Q_function[gold_position[0], gold_position[1]] = 0\n",
    "        ## de l'état terminal en raison des trous\n",
    "        for hole in holes_positions:\n",
    "            self.Q_function[hole[0], hole[1]] = 0\n",
    "\n",
    "        # paramètres Q-learning\n",
    "        self.ALPHA = 1\n",
    "        self.GAMMA = gamma\n",
    "        self.EPSILON = epsilon\n",
    "\n",
    "        # paramètres d'apprentissage \n",
    "        ## actions\n",
    "        self.current_action = None\n",
    "        self.previous_action = None\n",
    "        ## récompenses\n",
    "        self.current_reward = None\n",
    "        self.previous_reward = None\n",
    "        ## positions\n",
    "        self.current_position = self.starting_position\n",
    "        self.previous_position = None\n",
    "        \n",
    "        # initialisation du compteur d'épisodes\n",
    "        self.step_number = 0\n",
    "\n",
    "        pass\n",
    "\n",
    "    def act(self):\n",
    "        \"\"\"\n",
    "        renvoie une action parmi l'ensemble des actions possibles \n",
    "            - suit une politique d'epsilon-greedy \n",
    "            - met à jour les paramètres de l'action en cours et de l'action précédente\n",
    "        \"\"\"\n",
    "        \n",
    "        # mise à jour de l'action précédente\n",
    "        self.previous_action = self.current_action\n",
    "\n",
    "        # choix de l'action basée sur une politique epsilon-greedy \n",
    "        if np.random.uniform() < self.EPSILON:\n",
    "            action = np.random.choice(self.set_of_actions)\n",
    "        else:\n",
    "            action_index = np.argmax(self.Q_function[self.current_position[0], self.current_position[1]])\n",
    "            action = self.set_of_actions[action_index]\n",
    "    \n",
    "        # mise à jour d'action actuelle\n",
    "        self.current_action = action\n",
    "        \n",
    "        # augmentation du compteur \n",
    "        self.step_number += 1\n",
    "        \n",
    "        # mise à jour de alpha\n",
    "        self.ALPHA = 1/self.step_number\n",
    "      \n",
    "        return action\n",
    "\n",
    "    def observe(self, agent_position, reward, breeze, smell, end_episode, info):\n",
    "      \n",
    "        # obtention de l'indice A'\n",
    "        \n",
    "        if self.previous_action is not None:\n",
    "\n",
    "            # obtention de l'indice A \n",
    "            \n",
    "            # obtention de l'indice A*\n",
    "            \n",
    "            # obtention de Q(S,A) \n",
    "            \n",
    "            # obtention de Q(S',A*)\n",
    "            \n",
    "            # calcul de la mise à jour\n",
    "            \n",
    "            pass\n",
    "            \n",
    "        # mise à jour des valeurs  \n",
    "        # de la position précédente \n",
    "        \n",
    "        # de la position en cours \n",
    "        \n",
    "        # de la récompense précédente\n",
    "        \n",
    "        # de la récompense en cours\n",
    "        \n",
    "        # mise à jour si l'épisode se termine\n",
    "            \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self):\n",
    "        self.previous_action = None\n",
    "        self.previous_reward = None\n",
    "        self.previous_position = None\n",
    "        self.current_action = None\n",
    "        self.current_reward = None\n",
    "        self.current_position = self.starting_position\n",
    "        self.step_number = 1\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n",
    "#Instancier un Agent de classe QAgent avec \n",
    "#    une taille de grille grid_size de (10,10), \n",
    "#    une position initiale starting_position de (0, 0) et \n",
    "#    la position de la pièce d'or fixée à (9, 9).\n",
    "#Initialiser un environnementVanillaGridWorld avec cet Agent.\n",
    "#---------------------\n",
    "agent = QAgent(grid_size=(10, 10), \n",
    "               starting_position=(0, 0), \n",
    "               gold_position=(9, 9))\n",
    "env = VanillaGridWorld(agent=agent, grid_size=10)\n",
    "\n",
    "\n",
    "#Lancer 1000 épisode avec l'argument write fixé à False.\n",
    "#---------------------\n",
    "for i in range(1000):\n",
    "    env.run_episode(write=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Résultats \n",
    "#----------------------------------------------------------\n",
    "\n",
    "#Appeler la méthode display_greedy_episode de l'environnement.\n",
    "#---------------------\n",
    "env.display_greedy_episode()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Différence avec SARSA\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#Importer CliffGridWorld de gridworld.environments, lancer et \n",
    "#visualiser un épisode avec un Agent ramdom de classe RandomAgent.\n",
    "#---------------------\n",
    "from gridworld.environments import CliffGridWorld\n",
    "\n",
    "env = CliffGridWorld(agent=RandomAgent())\n",
    "\n",
    "env.run_episode()\n",
    "env.display_in_jupyter(width=600, height=200)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Implémenter un Agent de classe SarsaAgent avec les paramètres suivants :\n",
    "    #grid_size = (4, 12)\n",
    "    #starting_position = (3, 0)\n",
    "    #gold_position = (3, 11)\n",
    "    #holes_positions = [(3,i) for i in range(1,11)]\n",
    "    #epsilon = .5\n",
    "#Initialiser un environnement CliffGridWorld avec cet Agent.\n",
    "#Entraîner le pendant 5000 épisodes.\n",
    "#---------------------\n",
    "from gridworld.agents import SarsaAgent\n",
    "env_sarsa = CliffGridWorld(agent=SarsaAgent(grid_size=(4, 12),\n",
    "                                            starting_position=(3, 0),\n",
    "                                            gold_position=(3, 11),\n",
    "                                            epsilon = .1, \n",
    "                                            holes_positions = [(3,i) for i in range(1,11)]\n",
    "                                        ))\n",
    "\n",
    "\n",
    "for i in range(5000):\n",
    "    env_sarsa.run_episode(write=False)\n",
    "    \n",
    "    \n",
    "#Appeler la méthode display_greedy_episode de l'environnement.\n",
    "#---------------------\n",
    "env_sarsa.display_greedy_episode()\n",
    "\n",
    "\n",
    "\n",
    "#Implémenter un Agent de classe QAgent avec les paramètres suivants :\n",
    "    #grid_size = (4, 12)\n",
    "    #starting_position = (3, 0)\n",
    "    #gold_position = (3, 11)\n",
    "    #holes_positions = [(3,i) for i in range(1,11)]\n",
    "    #epsilon = .5\n",
    "#Initialiser un environnement CliffGridWorld avec cet Agent.\n",
    "#Entraîner le pendant 5000 épisodes.\n",
    "#---------------------\n",
    "env_q = CliffGridWorld(agent=QAgent(grid_size=(4, 12),\n",
    "                                            starting_position=(3, 0),\n",
    "                                            gold_position=(3, 11),\n",
    "                                            epsilon = .5, \n",
    "                                            holes_positions = [(3,i) for i in range(1,11)]\n",
    "                                        ))\n",
    "\n",
    "\n",
    "for i in range(5000):\n",
    "    env_q.run_episode(write=False)\n",
    "\n",
    "#appeler la méthode display_greedy_episode de l'environnement\n",
    "#---------------------\n",
    "env_q.display_greedy_episode()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 6 : TD Learning ou Monte Carlo : Biais ou inefficacité ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "\n",
    "\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "#---------------------\n",
    "\n",
    "\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------\n",
    "#----------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
